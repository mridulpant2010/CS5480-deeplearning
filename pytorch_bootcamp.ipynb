{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`integer` is a 0-d Tensor: 1234\n",
      "`decimal` is a 0-d Tensor: 3.1415927410125732\n",
      "`one_d tensor` is a 1-d Tensor: tensor([1, 2, 3, 4])\n",
      "`2_d tensor` is a 2-d Tensor: tensor([[ 1,  2,  3,  4],\n",
      "        [21, 22, 23, 24]])\n",
      "`count_to_100` is a 1-d Tensor with shape: torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "integer = torch.tensor(1234)\n",
    "decimal = torch.tensor(3.14159265359)\n",
    "\n",
    "print(f\"`integer` is a {integer.ndim}-d Tensor: {integer}\")\n",
    "print(f\"`decimal` is a {decimal.ndim}-d Tensor: {decimal}\")\n",
    "\n",
    "one_d_tensor = torch.tensor([1,2,3,4])\n",
    "print(f\"`one_d tensor` is a {one_d_tensor.ndim}-d Tensor: {one_d_tensor}\")\n",
    "\n",
    "two_d_tensor = torch.tensor([[1,2,3,4],[21,22,23,24]])\n",
    "print(f\"`2_d tensor` is a {two_d_tensor.ndim}-d Tensor: {two_d_tensor}\")\n",
    "\n",
    "assert two_d_tensor.ndim == 2\n",
    "\n",
    "count_to_100 = torch.tensor(range(100))\n",
    "\n",
    "print(f\"`count_to_100` is a {count_to_100.ndim}-d Tensor with shape: {count_to_100.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  In image processing and computer vision, we will use 4-d Tensors with dimensions corresponding to batch size, number of color channels, image height, and image width.\n",
    "# what is a batch_size in a 4d tensor.\n",
    "# create a 4d tensor Use torch.zeros to initialize a 4-d Tensor of zeros with size 10 x 3 x 256 x 256. \n",
    "# 10 images with channel of 3 and height and width = 256\n",
    "\n",
    "fourd_array = torch.tensor([[ [[0 for _ in range(256)] for _ in range(256)] for _ in range(3)] for _ in range(10)])\n",
    "\n",
    "assert fourd_array.shape == (10,3,256,256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c1: 76\n",
      "c2: 76\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Create the nodes in the graph and initialize values\n",
    "a = torch.tensor(15)\n",
    "b = torch.tensor(61)\n",
    "\n",
    "# Add them!\n",
    "c1 = torch.add(a, b)\n",
    "c2 = a + b  # PyTorch overrides the \"+\" operation so that it is able to act on Tensors\n",
    "print(f\"c1: {c1}\")\n",
    "print(f\"c2: {c2}\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "# build a tensor of size 2,3\n",
    "matrix_mult = torch.tensor([[ np.random.normal() for _ in range(3)]for _ in range(2)])\n",
    "print(matrix_mult.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2]) torch.Size([2, 1])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "input = torch.tensor([[2,3]])\n",
    "output = torch.tensor([[3.],[4.]])\n",
    "print(input.shape,output.shape)\n",
    "\n",
    "result = torch.matmul(matrix_mult.T,output)\n",
    "print(result.shape)\n",
    "\n",
    "#TODO: transpose a tensor ?\n",
    "print(result.T.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeuralNetworks in Pytorch\n",
    "\n",
    "- use torch.nn.Module which serves as a base class for all NN modules in pytorch and thus provides a framework \n",
    "- Use a single Module to define our simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_input shape: torch.Size([2, 1])\n",
      "neural_net shape: torch.Size([2, 3])\n",
      "output of the z matrix is  torch.Size([3, 1])\n",
      "output shape: torch.Size([3, 1])\n",
      "output result: tensor([[0.2957],\n",
      "        [0.4909],\n",
      "        [0.1631]], grad_fn=<SigmoidBackward0>)\n",
      " Dimensions of the bias is : torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self,num_inputs,num_outputs):\n",
    "        super(NeuralNetwork,self).__init__()\n",
    "        self.Weight = nn.Parameter(torch.randn(num_inputs,num_outputs)) # what are these initialization in neural network\n",
    "        self.bias = nn.Parameter(torch.randn(num_outputs,1))    \n",
    "    def forward(self,x):\n",
    "        #z = torch.add(torch.matmul(self.Weight.T,x) ,self.bias) # i don't get how this size has gone to 3x3\n",
    "        z = torch.matmul(self.Weight.T,x) + self.bias \n",
    "        print(\"output of the z matrix is \",z.shape)\n",
    "        output = torch.sigmoid(z)\n",
    "        return output\n",
    "    \n",
    "num_inputs= 2\n",
    "num_outputs=3\n",
    "neural_net = NeuralNetwork(num_inputs,num_outputs)\n",
    "x_input =  torch.tensor([[2.],[1.]])\n",
    "print(f\"x_input shape: {x_input.shape}\")\n",
    "print(f\"neural_net shape: {neural_net.Weight.shape}\")\n",
    "result = neural_net.forward(x_input)\n",
    "#result = neural_net(torch.tensor([1,2]))\n",
    "print(f\"output shape: {result.shape}\")\n",
    "print(f\"output result: {result}\")\n",
    "print(f\" Dimensions of the bias is : {neural_net.bias.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1]) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "case1= nn.Parameter(torch.randn(num_outputs,1))  \n",
    "case2 = nn.Parameter(torch.randn(num_outputs))  \n",
    "print(case1.shape,case2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_input shape: torch.Size([1, 2])\n",
      "neural_net shape: torch.Size([2, 3])\n",
      "output of the z matrix is  torch.Size([1, 3])\n",
      "output shape: torch.Size([1, 3])\n",
      "output result: tensor([[0.4413, 0.3826, 0.8387]], grad_fn=<SigmoidBackward0>)\n",
      " Dimensions of the bias is : torch.Size([3])\n",
      "output of the z matrix is  torch.Size([1, 3])\n",
      "input shape: torch.Size([1, 2])\n",
      "output shape: torch.Size([1, 3])\n",
      "output result: tensor([[0.4413, 0.3826, 0.8387]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "class DenseLayer(nn.Module):\n",
    "    def __init__(self,num_inputs,num_outputs):\n",
    "        super().__init__()\n",
    "        self.Weight = nn.Parameter(torch.randn(num_inputs,num_outputs)) # what are these initialization in neural network\n",
    "        self.bias = nn.Parameter(torch.randn(num_outputs))    \n",
    "    def forward(self,x):\n",
    "        #z = torch.add(torch.matmul(self.Weight.T,x) ,self.bias) # i don't get how this size has gone to 3x3\n",
    "        z = torch.matmul(x,self.Weight) + self.bias \n",
    "        print(\"output of the z matrix is \",z.shape)\n",
    "        output = torch.sigmoid(z)\n",
    "        return output\n",
    "    \n",
    "num_inputs= 2\n",
    "num_outputs=3\n",
    "neural_net = DenseLayer(num_inputs,num_outputs)\n",
    "x_input =  torch.tensor([[1,2.]])\n",
    "print(f\"x_input shape: {x_input.shape}\")\n",
    "print(f\"neural_net shape: {neural_net.Weight.shape}\")\n",
    "result = neural_net.forward(x_input)\n",
    "#result = neural_net(torch.tensor([1,2]))\n",
    "print(f\"output shape: {result.shape}\")\n",
    "print(f\"output result: {result}\")\n",
    "print(f\" Dimensions of the bias is : {neural_net.bias.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "y = neural_net(x_input)\n",
    "\n",
    "print(f\"input shape: {x_input.shape}\")\n",
    "print(f\"output shape: {y.shape}\")\n",
    "print(f\"output result: {y}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential API\n",
    "Sequential API we can readily create neural networks by stacking together layers like building blocks from the pytorch and a single nn.Linear to define the network.\n",
    "\n",
    "- what is the difference between Linear layer sigmoid and the prev layer with nn.Module utilising the matrix multiplication.\n",
    "    - will this code and the prev code results in the same output?\n",
    "    - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output of the z matrix is  torch.Size([1, 3])\n",
      "input shape: torch.Size([1, 2])\n",
      "output shape: torch.Size([1, 3])\n",
      "output result: tensor([[0.4413, 0.3826, 0.8387]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "n_inputs = 2\n",
    "n_outputs = 3\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(n_inputs,n_outputs),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# Test the model with example input\n",
    "x_input = torch.tensor([[1, 2.]])\n",
    "model_output = model(x_input)\n",
    "y = neural_net(x_input)\n",
    "print(f\"input shape: {x_input.shape}\")\n",
    "print(f\"output shape: {y.shape}\")\n",
    "print(f\"output result: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x1 and 2x3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#x_input = torch.tensor([2.,3.])\u001b[39;00m\n\u001b[0;32m     18\u001b[0m x_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m2.\u001b[39m],[\u001b[38;5;241m1.\u001b[39m]])\n\u001b[1;32m---> 19\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mll_sigmoid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput result: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m, in \u001b[0;36mLinearLayerSigmoid.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,inputs):\n\u001b[1;32m----> 8\u001b[0m     linear\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(linear)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x1 and 2x3)"
     ]
    }
   ],
   "source": [
    "class LinearLayerSigmoid(nn.Module):\n",
    "    def __init__(self,num_inputs,num_outputs):\n",
    "        super().__init__()\n",
    "        self.linear=  nn.Linear(num_inputs,num_outputs) #defininig a linear layer\n",
    "        self.activation= nn.Sigmoid() #defining an activation function\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        linear=self.linear(inputs)\n",
    "        outputs=self.activation(linear)\n",
    "        return outputs\n",
    "    \n",
    "\n",
    "num_inputs= 2\n",
    "num_outputs=3\n",
    "ll_sigmoid = LinearLayerSigmoid(num_inputs,num_outputs)\n",
    "\n",
    "#x_input = torch.tensor([2.,3.])\n",
    "x_input = torch.tensor([[2.],[1.]])\n",
    "output = ll_sigmoid.forward(x_input)\n",
    "print(f\"output shape: {output.shape}\")\n",
    "print(f\"output result: {output}\")\n",
    "# how come ll_sigmoid.forward(x_input) and ll_sigmoid(x_input) results in the similar results?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9., grad_fn=<PowBackward0>)\n",
      "dy_dx of y=x^2 at x=3.0 is:  tensor(30.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = x ** 2\n",
    "y.backward(torch.tensor(5))\n",
    "print(y)\n",
    "\n",
    "dy_dx = x.grad\n",
    "print(\"dy_dx of y=x^2 at x=3.0 is: \", dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(64., grad_fn=<PowBackward0>)\n",
      " res of the gradient is  tensor(240.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(4.0, requires_grad=True)\n",
    "y = x **3\n",
    "y.backward(torch.tensor(5.0))\n",
    "print(y)\n",
    "res = x.grad\n",
    "print(\" res of the gradient is \", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(48.)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find the min of L = (x-xf)^2\n",
    "\n",
    "-  when do we keep requires_grad=True and when do we keep it false. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# how can i write the gradient descent for a func\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.plot([1,20,33,400,5,6],[1,2,3,4,5,6])\n",
    "plt.title('Basic Line Plot')  # Adds a title\n",
    "plt.xlabel('X-axis')  # Adds an X-axis label\n",
    "plt.ylabel('Y-axis')  # Adds a Y-axis label\n",
    "plt.show()  # Displays the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pytorch and music generation with the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\pantm\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: comet_ml in c:\\users\\pantm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.49.5)\n",
      "Requirement already satisfied: dulwich!=0.20.33,>=0.20.6 in c:\\users\\pantm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from comet_ml) (0.22.8)\n",
      "Requirement already satisfied: everett<3.2.0,>=1.0.1 in c:\\users\\pantm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from everett[ini]<3.2.0,>=1.0.1->comet_ml) (3.1.0)\n",
      "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in c:\\users\\pantm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from comet_ml) (4.23.0)\n",
      "Requirement already satisfied: psutil>=5.6.3 in c:\\users\\pantm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from comet_ml) (6.1.0)\n",
      "Requirement already satisfied: python-box<7.0.0 in c:\\users\\pantm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from comet_ml) (6.1.0)\n",
      "Requirement already satisfied: requests-toolbelt>=0.8.0 in c:\\users\\pantm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from comet_ml) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.18.4 in c:\\users\\pantm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from comet_ml) (2.32.3)\n",
      "Requirement already satisfied: rich>=13.3.2 in c:\\users\\pantm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from comet_ml) (13.9.4)\n",
      "Requirement already satisfied: semantic-version>=2.8.0 in c:\\users\\pantm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from comet_ml) (2.10.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.1.0 in c:\\users\\pantm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from comet_ml) (2.22.0)\n",
      "Requirement already satisfied: simplejson in c:\\users\\pantm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from comet_ml) (3.20.1)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in c:\\users\\pantm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from comet_ml) (2.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.2 in c:\\users\\pantm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from comet_ml) (1.17.2)\n",
      "Requirement already satisfied: wurlitzer>=1.0.2 in c:\\users\\pantm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from comet_ml) (3.1.1)\n",
      "Requirement already satisfied: configobj in c:\\users\\pantm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from everett[ini]<3.2.0,>=1.0.1->comet_ml) (5.0.9)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\pantm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (25.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\pantm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\pantm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\pantm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.23.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pantm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.18.4->comet_ml) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pantm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.18.4->comet_ml) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pantm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.18.4->comet_ml) (2025.1.31)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\pantm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich>=13.3.2->comet_ml) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\pantm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich>=13.3.2->comet_ml) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\pantm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from markdown-it-py>=2.2.0->rich>=13.3.2->comet_ml) (0.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in c:\\users\\pantm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from referencing>=0.28.4->jsonschema!=3.1.0,>=2.6.0->comet_ml) (4.12.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\pantm\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install comet_ml \n",
    "# \n",
    "import comet_ml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Download and import the MIT Introduction to Deep Learning package\n",
    "!pip install mitdeeplearning --quiet\n",
    "\n",
    "COMET_API_KEY  = \"HXfJSXP0syikMTTNHyr4cH7pc\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 817 songs in text\n",
      "\n",
      "Example song: \n",
      "X:32\n",
      "T:Crabs in the Skillet\n",
      "Z: id:dc-jig-26\n",
      "M:6/8\n",
      "L:1/8\n",
      "K:G Dorian\n",
      "D|G3 d2F|G2A B2c|d_ed cBA|dcB AGF|!\n",
      "G3 d2F|G2A B2d|c=Bc fed|cAF F2:|!\n",
      "d|gag gfe|fgf fed|cde fed|cAG G2d|!\n",
      "gag gfe|fgf fed|cde fed|cAF F2:|!\n",
      "D|G3 A3|B3 c3|d_ed cBA|dcB AGF|!\n",
      "G3 A3|B3 d3|c=Bc fed|cAF F2:|!\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset\n",
    "from mitdeeplearning import lab1\n",
    "songs =lab1.load_training_data()\n",
    "\n",
    "# Print one of the songs to inspect it in greater detail!\n",
    "example_song = songs[100]\n",
    "print(\"\\nExample song: \")\n",
    "print(example_song)\n",
    "\n",
    "lab1.play_song(example_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 83 unique characters in the dataset\n",
      "['\\n', ' ', '!', '\"', '#', \"'\", '(', ')', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '<', '=', '>', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '^', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|']\n"
     ]
    }
   ],
   "source": [
    "# Join our list of song strings into a single string containing all songs\n",
    "songs_joined = \"\\n\\n\".join(songs)\n",
    "\n",
    "# Find all unique characters in the joined string\n",
    "vocab = sorted(set(songs_joined))\n",
    "print(\"There are\", len(vocab), \"unique characters in the dataset\")\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "### Define numerical representation of text ###\n",
    "\n",
    "# Create a mapping from character to unique index.\n",
    "# For example, to get the index of the character \"d\",\n",
    "#   we can evaluate `char2idx[\"d\"]`.\n",
    "char2idx = {u: i for i, u in enumerate(vocab)}\n",
    "# Create a mapping from indices to characters. This is\n",
    "#   the inverse of char2idx and allows us to convert back\n",
    "#   from unique index to the character in our vocabulary.\n",
    "idx2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\\n', ' ', '!', '\"', '#', \"'\", '(', ')', ',', '-', '.', '/', '0',\n",
       "       '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '<', '=', '>',\n",
       "       'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
       "       'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
       "       '[', ']', '^', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i',\n",
       "       'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v',\n",
       "       'w', 'x', 'y', 'z', '|'], dtype='<U1')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a character or a sequence of characters, what is the most probable next character?\n",
    "how can we achieve this?\n",
    "    - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vectorize the song string\n",
    "def vectorize_string(string):\n",
    "    vectorized_output = np.array([char2idx[char] for char in string])\n",
    "    return vectorized_output\n",
    "\n",
    "vectorized_songs = vectorize_string(songs_joined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- what does an RNN require?\n",
    "  - input sequence: an input sequence that we feed into our RNN \n",
    "  - target sequence: for each input sequence, which will be used in training the RNN to predict the next character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch function works correctly!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pantm\\AppData\\Local\\Temp\\ipykernel_39656\\369779558.py:16: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  x_batch = torch.tensor(input_batch, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## what is this batch function trying to do.\n",
    "import torch\n",
    "def get_batch(vectorized_songs, seq_length, batch_size):\n",
    "    # the length of the vectorized songs string\n",
    "    n = vectorized_songs.shape[0] - 1\n",
    "    # randomly choose the starting indices for the examples in the training batch\n",
    "    idx = np.random.choice(n - seq_length, batch_size)\n",
    "\n",
    "    '''TODO: construct a list of input sequences for the training batch'''\n",
    "    input_batch = [vectorized_songs[i: i + seq_length] for i in idx]\n",
    "\n",
    "    '''TODO: construct a list of output sequences for the training batch'''\n",
    "    output_batch = [vectorized_songs[i+1: i + seq_length+1] for i in idx]\n",
    "\n",
    "    # Convert the input and output batches to tensors\n",
    "    x_batch = torch.tensor(input_batch, dtype=torch.long)\n",
    "    y_batch = torch.tensor(output_batch, dtype=torch.long)\n",
    "\n",
    "    return x_batch, y_batch\n",
    "\n",
    "# Perform some simple tests to make sure your batch function is working properly!\n",
    "test_args = (vectorized_songs, 10, 2)\n",
    "x_batch, y_batch = get_batch(*test_args)\n",
    "assert x_batch.shape == (2, 10), \"x_batch shape is incorrect\"\n",
    "assert y_batch.shape == (2, 10), \"y_batch shape is incorrect\"\n",
    "print(\"Batch function works correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'X:1\\nT:Alex' ---- characters mapped to int ----> [49 22 13  0 45 22 26 67 60 79]\n"
     ]
    }
   ],
   "source": [
    "print ('{} ---- characters mapped to int ----> {}'.format(repr(songs_joined[:10]), vectorized_songs[:10]))\n",
    "# check that vectorized_songs is a numpy array\n",
    "assert isinstance(vectorized_songs, np.ndarray), \"returned result should be a numpy array\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step   0\n",
      "  input: 26 ('A')\n",
      "  expected output: 31 ('F')\n",
      "Step   1\n",
      "  input: 31 ('F')\n",
      "  expected output: 58 ('c')\n",
      "Step   2\n",
      "  input: 58 ('c')\n",
      "  expected output: 31 ('F')\n",
      "Step   3\n",
      "  input: 31 ('F')\n",
      "  expected output: 1 (' ')\n",
      "Step   4\n",
      "  input: 1 (' ')\n",
      "  expected output: 59 ('d')\n"
     ]
    }
   ],
   "source": [
    "x_batch, y_batch = get_batch(vectorized_songs, seq_length=5, batch_size=1)\n",
    "\n",
    "for i, (input_idx, target_idx) in enumerate(zip(x_batch[0], y_batch[0])):\n",
    "    print(\"Step {:3d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx.item()])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx.item()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model\n",
    " - what is the embedding vector how do we create it? criteria of choosing the embedding_dim\n",
    " - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_dim,hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size= hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim,hidden_size,num_layers=2,batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size,vocab_size)\n",
    "        \n",
    "    def init_hidden(self,batch_size,device):\n",
    "        return (torch.zeros(2, batch_size, self.hidden_size).to(device),\n",
    "                torch.zeros(2, batch_size, self.hidden_size).to(device))\n",
    "        \n",
    "    def forward(self, x, state=None, return_state=False):\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        if state is None:\n",
    "            state = self.init_hidden(x.size(0), x.device)\n",
    "        out, state = self.lstm(x, state)  #what do we call this phenomena in python?\n",
    "\n",
    "        out = self.fc(out)\n",
    "        return out if not return_state else (out, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (embedding): Embedding(83, 256)\n",
      "  (lstm): LSTM(256, 1024, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=1024, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model! Build a simple model with default hyperparameters. You\n",
    "#     will get the chance to change these later.\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256 #how do we decide this embedding_dim ?\n",
    "hidden_size = 1024\n",
    "batch_size = 8\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = LSTMModel(vocab_size, embedding_dim, hidden_size).to(device)\n",
    "\n",
    "# print out a summary of the model\n",
    "print(model)\n",
    "\n",
    "# if my LSTM model is bidirectional?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:       torch.Size([32, 100])  # (batch_size, sequence_length)\n",
      "Prediction shape:  torch.Size([32, 100, 83]) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "# Test the model with some sample data\n",
    "\n",
    "x, y = get_batch(vectorized_songs, seq_length=100, batch_size=32)\n",
    "x = x.to(device)\n",
    "y = y.to(device)\n",
    "\n",
    "pred = model(x)\n",
    "print(\"Input shape:      \", x.shape, \" # (batch_size, sequence_length)\")\n",
    "print(\"Prediction shape: \", pred.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- why do we need to predict from the categorical distribution to sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([73, 37,  0, 71, 76, 69, 71, 33, 76, 46, 49, 77, 29, 62, 34,  0, 60,\n",
       "       70, 18, 56, 80, 21, 66, 77, 47, 65, 54, 39, 27, 74,  1, 53, 65, 15,\n",
       "       65, 14, 72, 50, 73, 37,  5, 46, 80, 67, 12,  2, 33, 58, 54, 68, 56,\n",
       "       61,  8, 65, 53, 49, 38, 44, 27, 57, 64, 73, 35, 75, 73, 30, 45, 23,\n",
       "       65, 51, 45, 22, 74, 49, 42, 69, 48,  1, 19, 26, 61,  9, 47, 26, 13,\n",
       "        1, 81, 51, 34, 38, 57, 61, 79, 47, 47,  8, 68, 48, 68, 65],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = torch.multinomial(torch.softmax(pred[0], dim=-1), num_samples=1)\n",
    "sampled_indices = sampled_indices.squeeze(-1).cpu().numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " \"!\\nBee2 fedB|egfe dBAF|DEFA dBAF|DEFD E3:|!\\n\\nX:357\\nT:Tommy Peoples'\\nZ: id:dc-reel-333\\nM:C\\nL:1/8\\nK:B M\"\n",
      "\n",
      "Next Char Predictions: \n",
      " \"rL\\npunpHuUXvDgI\\neo6ay9kvVj^NBs ]j3j2qYrL'Uyl0!Hc^maf,j]XMSBbirJtrET<jZT:sXQnW 7Af-VA1 zZIMbfxVV,mWmj\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[x[0].cpu()])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this data as we see isn't predicting the correct next character, how can it be made better? Training the model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining the loss function ###\n",
    "\n",
    "# '''TODO: define the compute_loss function to compute and return the loss between\n",
    "#     the true labels and predictions (logits). '''\n",
    "cross_entropy = nn.CrossEntropyLoss() # instantiates the function\n",
    "def compute_loss(labels, logits):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      labels: (batch_size, sequence_length)\n",
    "      logits: (batch_size, sequence_length, vocab_size)\n",
    "\n",
    "    Output:\n",
    "      loss: scalar cross entropy loss over the batch and sequence length\n",
    "    \"\"\"\n",
    "\n",
    "    # Batch the labels so that the shape of the labels should be (B * L,)\n",
    "    batched_labels = labels.view(-1)\n",
    "\n",
    "    ''' TODO: Batch the logits so that the shape of the logits should be (B * L, V) '''\n",
    "    batched_logits = logits.view(-1) \n",
    "    \"\"\" TODO \"\"\" # TODO\n",
    "\n",
    "    '''TODO: Compute the cross-entropy loss using the batched  next characters and predictions'''\n",
    "    loss = cross_entropy(batched_logits,batched_labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### compute the loss on the predictions from the untrained model from earlier. ###\n",
    "y.shape  # (batch_size, sequence_length)\n",
    "pred.shape  # (batch_size, sequence_length, vocab_size)\n",
    "\n",
    "'''TODO: compute the loss using the true next characters from the example batch\n",
    "    and the predictions from the untrained model several cells above'''\n",
    "example_batch_loss = compute_loss(y, pred) # TODO\n",
    "\n",
    "print(f\"Prediction shape: {pred.shape} # (batch_size, sequence_length, vocab_size)\")\n",
    "print(f\"scalar_loss:      {example_batch_loss.mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameter setting and optimization ###\n",
    "import os\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Model parameters:\n",
    "params = dict(\n",
    "  num_training_iterations = 3000,  # Increase this to train longer\n",
    "  batch_size = 8,  # Experiment between 1 and 64\n",
    "  seq_length = 100,  # Experiment between 50 and 500\n",
    "  learning_rate = 5e-3,  # Experiment between 1e-5 and 1e-1\n",
    "  embedding_dim = 256,\n",
    "  hidden_size = 1024,  # Experiment between 1 and 2048\n",
    ")\n",
    "\n",
    "# Checkpoint location:\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"my_ckpt\")\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define optimizer and the training operation\n",
    "model = LSTMModel(params['seq_length'],params['embedding_dim'],params['hidden_size'])\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(),params['learning_rate'])\n",
    "\n",
    "def train_step(x, y):\n",
    "  # Set the model's mode to train\n",
    "  model.train()\n",
    "  # Zero gradients for every step\n",
    "  optimizer.zero_grad()\n",
    "  # Forward pass\n",
    "  '''TODO: feed the current input into the model and generate predictions'''\n",
    "  y_hat = model(x)\n",
    "  # Compute the loss\n",
    "  '''TODO: compute the loss!'''\n",
    "  loss = compute_loss(y,y_hat)\n",
    "  # Backward pass\n",
    "  '''TODO: complete the gradient computation and update step.\n",
    "    Remember that in PyTorch there are two steps to the training loop:\n",
    "    1. Backpropagate the loss\n",
    "    2. Update the model parameters using the optimizer\n",
    "  '''\n",
    "  # find the grad of the loss\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  \n",
    "  return loss\n",
    "\n",
    "history = []\n",
    "plotter = mdl.util.PeriodicPlotter(sec=2, xlabel='Iterations', ylabel='Loss')\n",
    "experiment = create_experiment()\n",
    "\n",
    "if hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n",
    "for iter in tqdm(range(params[\"num_training_iterations\"])):\n",
    "\n",
    "    # Grab a batch and propagate it through the network\n",
    "    x_batch, y_batch = get_batch(vectorized_songs, params[\"seq_length\"], params[\"batch_size\"])\n",
    "\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    x_batch = torch.tensor(x_batch, dtype=torch.long).to(device)\n",
    "    y_batch = torch.tensor(y_batch, dtype=torch.long).to(device)\n",
    "\n",
    "    # Take a train step\n",
    "    loss = train_step(x_batch, y_batch)\n",
    "\n",
    "    # Log the loss to the Comet interface\n",
    "    experiment.log_metric(\"loss\", loss.item(), step=iter)\n",
    "\n",
    "    # Update the progress bar and visualize within notebook\n",
    "    history.append(loss.item())\n",
    "    plotter.plot(history)\n",
    "\n",
    "    # Save model checkpoint\n",
    "    if iter % 100 == 0:\n",
    "        torch.save(model.state_dict(), checkpoint_prefix)\n",
    "\n",
    "# Save the final trained model\n",
    "torch.save(model.state_dict(), checkpoint_prefix)\n",
    "experiment.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a Comet experiment to track our training run ###\n",
    "\n",
    "def create_experiment():\n",
    "  # end any prior experiments\n",
    "  if 'experiment' in locals():\n",
    "    experiment.end()\n",
    "\n",
    "  # initiate the comet experiment for tracking\n",
    "  experiment = comet_ml.Experiment(\n",
    "                  api_key=COMET_API_KEY,\n",
    "                  project_name=\"6S191_Lab1_Part2\")\n",
    "  # log our hyperparameters, defined above, to the experiment\n",
    "  for param, value in params.items():\n",
    "    experiment.log_parameter(param, value)\n",
    "  experiment.flush()\n",
    "\n",
    "  return experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Give me a simple data which needs to be fed to RNN , i will first apply the data preprocessing.\n",
    "sentence = \"I love programming and the artificial intelligence\"\n",
    "step 1: tokenization\n",
    "step 2: vocabulary creation\n",
    "step 3: numerical encoding, but how?\n",
    "step 4: input and target sequences\n",
    "step 5: padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (word to index): {'I': 0, 'love': 1, 'machine': 2, 'learning': 3, 'and': 4, 'artificial': 5, 'intelligence.': 6}\n",
      "Encoded Data: [0, 1, 2, 3, 4, 5, 6]\n",
      "Embeddings Shape: torch.Size([7, 8])\n",
      "Embeddings:\n",
      "tensor([[ 0.4897,  2.0657, -1.6545,  0.8639, -0.9981, -0.5676, -0.0398, -1.6505],\n",
      "        [ 0.3315, -2.8302, -0.0334, -0.2669, -1.5473,  0.2918,  1.6548,  0.0204],\n",
      "        [-0.3538,  0.4534, -1.7879,  2.0494,  0.4566,  1.6449, -1.2526,  2.7362],\n",
      "        [ 1.4092,  0.3717, -1.7387,  1.0052,  0.6025,  0.0623,  0.4721, -0.8658],\n",
      "        [ 1.1505, -0.6553,  0.3737,  1.2305, -0.0820,  0.3193, -0.6720, -0.0614],\n",
      "        [-1.4942,  0.5852, -0.4864,  0.4107,  0.5174,  2.0180, -0.0797,  0.0875],\n",
      "        [-0.4090, -0.5345,  0.1570,  2.1780,  0.7160,  1.2304,  1.1751, -0.8326]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "input_sentence=\"I love machine learning and artificial intelligence.\"\n",
    "input_ar = np.array(input_sentence.split(\" \"))\n",
    "vocabulary  = {v:k for k ,v in enumerate(input_ar)}\n",
    "encoded_data = list(vocabulary.values())\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "encoded_data_tensor = torch.tensor(encoded_data,dtype=torch.long)\n",
    "\n",
    "embedding_dim = 8 # what is the meaning of this ?\n",
    "embedding_layer = nn.Embedding(num_embeddings=vocab_size,embedding_dim=embedding_dim)\n",
    "\n",
    "# Forward pass: Generate embeddings for the input sequence\n",
    "embeddings = embedding_layer(encoded_data_tensor)\n",
    "\n",
    "print(\"Vocabulary (word to index):\", vocabulary)\n",
    "print(\"Encoded Data:\", encoded_data)\n",
    "print(\"Embeddings Shape:\", embeddings.shape)  # Should be (sequence_length, embedding_dim)\n",
    "print(\"Embeddings:\")\n",
    "print(embeddings)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_data = label_encoder.fit_transform(input_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 5, 6, 4, 1, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res is  80\n"
     ]
    }
   ],
   "source": [
    "# use of the Call dunder method in python\n",
    "class MyCallable:\n",
    "    def __init__(self,factor):\n",
    "        self.factor = factor\n",
    "    \n",
    "    def __call__(self,x):\n",
    "        return x * self.factor\n",
    "    \n",
    "m = MyCallable(10)\n",
    "res = m(8)\n",
    "print(\"res is \",res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I': 0,\n",
       " 'love': 1,\n",
       " 'machine': 2,\n",
       " 'learning': 3,\n",
       " 'and': 4,\n",
       " 'artificial': 5,\n",
       " 'intelligence.': 6}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[59, 60, 59,  1, 32]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.11.0.86-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\pantm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from opencv-python) (1.26.4)\n",
      "Downloading opencv_python-4.11.0.86-cp37-abi3-win_amd64.whl (39.5 MB)\n",
      "   ---------------------------------------- 0.0/39.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/39.5 MB 640.0 kB/s eta 0:01:02\n",
      "   ---------------------------------------- 0.3/39.5 MB 3.3 MB/s eta 0:00:12\n",
      "    --------------------------------------- 0.7/39.5 MB 5.9 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 1.2/39.5 MB 7.1 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 1.7/39.5 MB 7.6 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 2.1/39.5 MB 8.5 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 2.6/39.5 MB 8.6 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 3.1/39.5 MB 9.0 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 3.5/39.5 MB 9.8 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 4.1/39.5 MB 9.6 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 4.5/39.5 MB 9.6 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 5.0/39.5 MB 9.6 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 5.4/39.5 MB 9.9 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 5.9/39.5 MB 9.7 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 6.4/39.5 MB 10.0 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 6.8/39.5 MB 10.1 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 7.0/39.5 MB 10.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 7.5/39.5 MB 9.8 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 7.9/39.5 MB 9.7 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 8.4/39.5 MB 9.8 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 8.9/39.5 MB 9.8 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 9.4/39.5 MB 10.0 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 9.9/39.5 MB 10.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 10.3/39.5 MB 10.4 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 10.8/39.5 MB 10.9 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 11.3/39.5 MB 10.7 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 11.8/39.5 MB 10.7 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 12.2/39.5 MB 10.7 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 12.6/39.5 MB 10.7 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 13.1/39.5 MB 10.7 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 13.7/39.5 MB 10.7 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 14.0/39.5 MB 10.7 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 14.5/39.5 MB 10.6 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 15.0/39.5 MB 10.9 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 15.5/39.5 MB 10.7 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 16.0/39.5 MB 10.7 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 16.5/39.5 MB 10.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 16.8/39.5 MB 10.7 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 17.2/39.5 MB 10.7 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 17.7/39.5 MB 10.9 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 18.3/39.5 MB 10.9 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 18.6/39.5 MB 10.7 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 19.0/39.5 MB 10.9 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 19.5/39.5 MB 10.9 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 20.1/39.5 MB 10.9 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 20.6/39.5 MB 10.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 21.0/39.5 MB 11.1 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 21.5/39.5 MB 10.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 21.9/39.5 MB 10.7 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 22.4/39.5 MB 10.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 22.9/39.5 MB 11.1 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 23.3/39.5 MB 10.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 23.8/39.5 MB 10.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 24.3/39.5 MB 10.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 24.7/39.5 MB 10.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 25.3/39.5 MB 10.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 25.7/39.5 MB 10.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 26.2/39.5 MB 10.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 26.7/39.5 MB 10.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 27.1/39.5 MB 10.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 27.4/39.5 MB 10.7 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 27.8/39.5 MB 10.6 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 28.0/39.5 MB 10.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 28.3/39.5 MB 10.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 28.5/39.5 MB 9.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 28.8/39.5 MB 9.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 29.0/39.5 MB 9.5 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 29.2/39.5 MB 9.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 29.5/39.5 MB 9.4 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 29.9/39.5 MB 9.2 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 30.2/39.5 MB 9.1 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 30.5/39.5 MB 8.8 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 30.7/39.5 MB 8.6 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 30.9/39.5 MB 8.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 31.1/39.5 MB 8.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 31.6/39.5 MB 8.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 32.0/39.5 MB 8.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 32.3/39.5 MB 8.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 32.7/39.5 MB 8.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 33.2/39.5 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 33.6/39.5 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 33.8/39.5 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 34.3/39.5 MB 7.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 34.6/39.5 MB 7.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 34.9/39.5 MB 7.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 35.2/39.5 MB 7.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 35.5/39.5 MB 7.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 35.9/39.5 MB 7.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 36.5/39.5 MB 7.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 36.9/39.5 MB 7.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 37.4/39.5 MB 7.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 37.9/39.5 MB 7.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 38.2/39.5 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.7/39.5 MB 8.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.2/39.5 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.5/39.5 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.5/39.5 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.5/39.5 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.5/39.5 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.5/39.5 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 39.5/39.5 MB 7.3 MB/s eta 0:00:00\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.11.0.86\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\pantm\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

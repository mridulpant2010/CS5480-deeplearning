{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "The objective of this assignment is to independently develop a classification model for each of the two provided datasets. You will experiment with three different algorithms (excluding transformers) and analyze their performance\n",
    "\n",
    "Contains titles, abstracts, and topic labels of research articles. Your task is\n",
    "to predict the topic based on the title and abstract. \n",
    "\n",
    "what algorithms i can use for the label based classification.\n",
    "\n",
    "- multiclass classification algorithm\n",
    "  - MLP\n",
    "  - naive bayes\n",
    "  - Logistic Regression utilising softmax\n",
    "  - SVMs / decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\pantm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.1.5)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\pantm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openpyxl) (2.0.0)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\pantm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     ------------------ ------------------- 20.5/41.5 kB 108.9 kB/s eta 0:00:01\n",
      "     ---------------------------- --------- 30.7/41.5 kB 145.2 kB/s eta 0:00:01\n",
      "     -------------------------------------- 41.5/41.5 kB 166.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\pantm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\pantm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.5 MB 445.2 kB/s eta 0:00:04\n",
      "    --------------------------------------- 0.0/1.5 MB 445.2 kB/s eta 0:00:04\n",
      "    --------------------------------------- 0.0/1.5 MB 445.2 kB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.1/1.5 MB 262.6 kB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.1/1.5 MB 262.6 kB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.1/1.5 MB 204.8 kB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.1/1.5 MB 204.8 kB/s eta 0:00:08\n",
      "   -- ------------------------------------- 0.1/1.5 MB 229.7 kB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.1/1.5 MB 256.7 kB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.1/1.5 MB 256.7 kB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.1/1.5 MB 234.3 kB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.1/1.5 MB 234.3 kB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.1/1.5 MB 234.3 kB/s eta 0:00:06\n",
      "   --- ------------------------------------ 0.1/1.5 MB 218.6 kB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.1/1.5 MB 218.6 kB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.1/1.5 MB 202.9 kB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.1/1.5 MB 202.9 kB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.1/1.5 MB 202.9 kB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 0.2/1.5 MB 196.7 kB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 0.2/1.5 MB 196.7 kB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 0.2/1.5 MB 192.1 kB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 0.2/1.5 MB 192.1 kB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 0.2/1.5 MB 190.4 kB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 0.2/1.5 MB 190.4 kB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 0.2/1.5 MB 198.6 kB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 0.2/1.5 MB 196.7 kB/s eta 0:00:07\n",
      "   ------ --------------------------------- 0.2/1.5 MB 209.4 kB/s eta 0:00:07\n",
      "   ------- -------------------------------- 0.3/1.5 MB 218.6 kB/s eta 0:00:06\n",
      "   ------- -------------------------------- 0.3/1.5 MB 218.6 kB/s eta 0:00:06\n",
      "   ------- -------------------------------- 0.3/1.5 MB 218.6 kB/s eta 0:00:06\n",
      "   ------- -------------------------------- 0.3/1.5 MB 221.2 kB/s eta 0:00:06\n",
      "   -------- ------------------------------- 0.3/1.5 MB 221.1 kB/s eta 0:00:06\n",
      "   -------- ------------------------------- 0.3/1.5 MB 228.4 kB/s eta 0:00:06\n",
      "   --------- ------------------------------ 0.4/1.5 MB 239.6 kB/s eta 0:00:05\n",
      "   --------- ------------------------------ 0.4/1.5 MB 239.6 kB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 0.4/1.5 MB 250.1 kB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 0.4/1.5 MB 255.7 kB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 0.4/1.5 MB 255.7 kB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 0.4/1.5 MB 256.0 kB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 0.4/1.5 MB 256.0 kB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 0.4/1.5 MB 256.0 kB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 0.4/1.5 MB 239.5 kB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 0.4/1.5 MB 239.5 kB/s eta 0:00:05\n",
      "   ------------ --------------------------- 0.5/1.5 MB 238.4 kB/s eta 0:00:05\n",
      "   ------------ --------------------------- 0.5/1.5 MB 238.4 kB/s eta 0:00:05\n",
      "   ------------ --------------------------- 0.5/1.5 MB 234.1 kB/s eta 0:00:05\n",
      "   ------------ --------------------------- 0.5/1.5 MB 234.1 kB/s eta 0:00:05\n",
      "   ------------- -------------------------- 0.5/1.5 MB 235.2 kB/s eta 0:00:05\n",
      "   ------------- -------------------------- 0.5/1.5 MB 235.2 kB/s eta 0:00:05\n",
      "   ------------- -------------------------- 0.5/1.5 MB 232.7 kB/s eta 0:00:05\n",
      "   ------------- -------------------------- 0.5/1.5 MB 232.7 kB/s eta 0:00:05\n",
      "   ------------- -------------------------- 0.5/1.5 MB 232.7 kB/s eta 0:00:05\n",
      "   ------------- -------------------------- 0.5/1.5 MB 226.0 kB/s eta 0:00:05\n",
      "   ------------- -------------------------- 0.5/1.5 MB 226.0 kB/s eta 0:00:05\n",
      "   -------------- ------------------------- 0.5/1.5 MB 227.3 kB/s eta 0:00:05\n",
      "   -------------- ------------------------- 0.5/1.5 MB 227.3 kB/s eta 0:00:05\n",
      "   -------------- ------------------------- 0.6/1.5 MB 224.2 kB/s eta 0:00:05\n",
      "   --------------- ------------------------ 0.6/1.5 MB 226.7 kB/s eta 0:00:05\n",
      "   --------------- ------------------------ 0.6/1.5 MB 229.3 kB/s eta 0:00:04\n",
      "   --------------- ------------------------ 0.6/1.5 MB 229.3 kB/s eta 0:00:04\n",
      "   --------------- ------------------------ 0.6/1.5 MB 229.3 kB/s eta 0:00:04\n",
      "   --------------- ------------------------ 0.6/1.5 MB 229.3 kB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 0.6/1.5 MB 219.7 kB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 0.6/1.5 MB 219.7 kB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 0.6/1.5 MB 219.7 kB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 0.6/1.5 MB 216.1 kB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 0.6/1.5 MB 216.1 kB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 0.6/1.5 MB 214.9 kB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 0.7/1.5 MB 217.4 kB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 0.7/1.5 MB 219.6 kB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 0.7/1.5 MB 219.6 kB/s eta 0:00:04\n",
      "   ------------------ --------------------- 0.7/1.5 MB 219.6 kB/s eta 0:00:04\n",
      "   ------------------ --------------------- 0.7/1.5 MB 222.8 kB/s eta 0:00:04\n",
      "   ------------------- -------------------- 0.7/1.5 MB 228.1 kB/s eta 0:00:04\n",
      "   -------------------- ------------------- 0.8/1.5 MB 231.1 kB/s eta 0:00:04\n",
      "   -------------------- ------------------- 0.8/1.5 MB 231.1 kB/s eta 0:00:04\n",
      "   -------------------- ------------------- 0.8/1.5 MB 228.8 kB/s eta 0:00:04\n",
      "   -------------------- ------------------- 0.8/1.5 MB 230.6 kB/s eta 0:00:04\n",
      "   -------------------- ------------------- 0.8/1.5 MB 230.6 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 0.8/1.5 MB 227.4 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 0.8/1.5 MB 227.4 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 0.8/1.5 MB 227.4 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 0.8/1.5 MB 227.1 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 0.8/1.5 MB 227.1 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 0.8/1.5 MB 227.1 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 0.8/1.5 MB 227.1 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 0.8/1.5 MB 227.1 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 0.8/1.5 MB 227.1 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 0.8/1.5 MB 227.1 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 0.8/1.5 MB 227.1 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 0.8/1.5 MB 227.1 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 0.8/1.5 MB 227.1 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 0.8/1.5 MB 227.1 kB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 0.8/1.5 MB 205.0 kB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 0.8/1.5 MB 205.0 kB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 0.8/1.5 MB 202.0 kB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 0.8/1.5 MB 202.0 kB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 0.9/1.5 MB 201.7 kB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 0.9/1.5 MB 201.7 kB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 0.9/1.5 MB 201.7 kB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 0.9/1.5 MB 198.3 kB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 0.9/1.5 MB 198.3 kB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 0.9/1.5 MB 198.3 kB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 0.9/1.5 MB 197.3 kB/s eta 0:00:04\n",
      "   ------------------------ --------------- 0.9/1.5 MB 199.1 kB/s eta 0:00:03\n",
      "   ------------------------ --------------- 0.9/1.5 MB 199.1 kB/s eta 0:00:03\n",
      "   ------------------------ --------------- 0.9/1.5 MB 198.6 kB/s eta 0:00:03\n",
      "   ------------------------- -------------- 1.0/1.5 MB 199.7 kB/s eta 0:00:03\n",
      "   ------------------------- -------------- 1.0/1.5 MB 199.7 kB/s eta 0:00:03\n",
      "   ------------------------- -------------- 1.0/1.5 MB 198.6 kB/s eta 0:00:03\n",
      "   -------------------------- ------------- 1.0/1.5 MB 200.2 kB/s eta 0:00:03\n",
      "   -------------------------- ------------- 1.0/1.5 MB 200.2 kB/s eta 0:00:03\n",
      "   -------------------------- ------------- 1.0/1.5 MB 201.2 kB/s eta 0:00:03\n",
      "   -------------------------- ------------- 1.0/1.5 MB 201.2 kB/s eta 0:00:03\n",
      "   -------------------------- ------------- 1.0/1.5 MB 200.7 kB/s eta 0:00:03\n",
      "   --------------------------- ------------ 1.0/1.5 MB 201.7 kB/s eta 0:00:03\n",
      "   --------------------------- ------------ 1.0/1.5 MB 201.8 kB/s eta 0:00:03\n",
      "   --------------------------- ------------ 1.0/1.5 MB 201.8 kB/s eta 0:00:03\n",
      "   --------------------------- ------------ 1.0/1.5 MB 201.8 kB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 1.1/1.5 MB 201.5 kB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 1.1/1.5 MB 203.0 kB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 1.1/1.5 MB 203.0 kB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 1.1/1.5 MB 203.0 kB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 200.8 kB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 200.8 kB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 199.9 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 199.9 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 199.9 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 199.9 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 199.9 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 195.2 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 195.2 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 195.2 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 1.1/1.5 MB 194.0 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 1.1/1.5 MB 194.0 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 1.1/1.5 MB 194.0 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 1.2/1.5 MB 192.9 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 1.2/1.5 MB 192.9 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 1.2/1.5 MB 192.9 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 1.2/1.5 MB 189.6 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 1.2/1.5 MB 189.6 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 1.2/1.5 MB 190.1 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 1.2/1.5 MB 190.1 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 1.2/1.5 MB 190.1 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 1.2/1.5 MB 188.4 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 1.2/1.5 MB 188.4 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 1.2/1.5 MB 188.8 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 1.2/1.5 MB 188.8 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 1.2/1.5 MB 188.8 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 1.2/1.5 MB 187.9 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 1.2/1.5 MB 187.9 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 1.2/1.5 MB 187.9 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 1.3/1.5 MB 186.4 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 187.3 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 187.3 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 187.1 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 188.7 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 1.3/1.5 MB 190.0 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.3/1.5 MB 190.6 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.3/1.5 MB 190.6 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.4/1.5 MB 191.0 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.4/1.5 MB 191.0 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.4/1.5 MB 190.3 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.4/1.5 MB 190.3 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.4/1.5 MB 190.3 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.4/1.5 MB 189.5 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.4/1.5 MB 189.5 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.4/1.5 MB 190.2 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.4/1.5 MB 190.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.5 MB 192.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.5 MB 192.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.5/1.5 MB 192.3 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.5/1.5 MB 192.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 192.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 193.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 193.7 kB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp311-cp311-win_amd64.whl (274 kB)\n",
      "   ---------------------------------------- 0.0/274.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/274.1 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/274.1 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/274.1 kB ? eta -:--:--\n",
      "   ---- ---------------------------------- 30.7/274.1 kB 330.3 kB/s eta 0:00:01\n",
      "   ----- --------------------------------- 41.0/274.1 kB 245.8 kB/s eta 0:00:01\n",
      "   -------- ------------------------------ 61.4/274.1 kB 297.7 kB/s eta 0:00:01\n",
      "   -------- ------------------------------ 61.4/274.1 kB 297.7 kB/s eta 0:00:01\n",
      "   ---------- ---------------------------- 71.7/274.1 kB 231.0 kB/s eta 0:00:01\n",
      "   ------------- ------------------------- 92.2/274.1 kB 262.6 kB/s eta 0:00:01\n",
      "   ------------- ------------------------- 92.2/274.1 kB 262.6 kB/s eta 0:00:01\n",
      "   ------------- ------------------------- 92.2/274.1 kB 262.6 kB/s eta 0:00:01\n",
      "   -------------- ----------------------- 102.4/274.1 kB 227.0 kB/s eta 0:00:01\n",
      "   ----------------- -------------------- 122.9/274.1 kB 240.2 kB/s eta 0:00:01\n",
      "   ----------------- -------------------- 122.9/274.1 kB 240.2 kB/s eta 0:00:01\n",
      "   ------------------ ------------------- 133.1/274.1 kB 224.6 kB/s eta 0:00:01\n",
      "   --------------------- ---------------- 153.6/274.1 kB 229.4 kB/s eta 0:00:01\n",
      "   --------------------- ---------------- 153.6/274.1 kB 229.4 kB/s eta 0:00:01\n",
      "   --------------------- ---------------- 153.6/274.1 kB 229.4 kB/s eta 0:00:01\n",
      "   ------------------------ ------------- 174.1/274.1 kB 227.9 kB/s eta 0:00:01\n",
      "   ------------------------ ------------- 174.1/274.1 kB 227.9 kB/s eta 0:00:01\n",
      "   ------------------------- ------------ 184.3/274.1 kB 218.4 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 204.8/274.1 kB 222.4 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 215.0/274.1 kB 222.1 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 215.0/274.1 kB 222.1 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 235.5/274.1 kB 228.9 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 256.0/274.1 kB 234.7 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 256.0/274.1 kB 234.7 kB/s eta 0:00:01\n",
      "   -------------------------------------- 274.1/274.1 kB 234.5 kB/s eta 0:00:00\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "   ---------------------------------------- 0.0/98.2 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 10.2/98.2 kB ? eta -:--:--\n",
      "   ------------ --------------------------- 30.7/98.2 kB 435.7 kB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 41.0/98.2 kB 393.8 kB/s eta 0:00:01\n",
      "   ------------------------- -------------- 61.4/98.2 kB 469.7 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 81.9/98.2 kB 459.5 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 81.9/98.2 kB 459.5 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 81.9/98.2 kB 459.5 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 81.9/98.2 kB 459.5 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 81.9/98.2 kB 459.5 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 98.2/98.2 kB 216.6 kB/s eta 0:00:00\n",
      "Installing collected packages: regex, click, nltk\n",
      "Successfully installed click-8.1.8 nltk-3.9.1 regex-2024.11.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\pantm\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ID                                              TITLE  \\\n",
      "0          1        Reconstructing Subject-Specific Effect Maps   \n",
      "1          2                 Rotation Invariance Neural Network   \n",
      "2          3  Spherical polyharmonics and Poisson kernels fo...   \n",
      "3          4  A finite element approximation for the stochas...   \n",
      "4          5  Comparative study of Discrete Wavelet Transfor...   \n",
      "...      ...                                                ...   \n",
      "20967  20968  Contemporary machine learning: a guide for pra...   \n",
      "20968  20969  Uniform diamond coatings on WC-Co hard alloy c...   \n",
      "20969  20970  Analysing Soccer Games with Clustering and Con...   \n",
      "20970  20971  On the Efficient Simulation of the Left-Tail o...   \n",
      "20971  20972   Why optional stopping is a problem for Bayesians   \n",
      "\n",
      "                                                ABSTRACT  Computer Science  \\\n",
      "0        Predictive models allow subject-specific inf...                 1   \n",
      "1        Rotation invariance and translation invarian...                 1   \n",
      "2        We introduce and develop the notion of spher...                 0   \n",
      "3        The stochastic Landau--Lifshitz--Gilbert (LL...                 0   \n",
      "4        Fourier-transform infra-red (FTIR) spectra o...                 1   \n",
      "...                                                  ...               ...   \n",
      "20967    Machine learning is finding increasingly bro...                 1   \n",
      "20968    Polycrystalline diamond coatings have been g...                 0   \n",
      "20969    We present a new approach for identifying si...                 1   \n",
      "20970    The sum of Log-normal variates is encountere...                 0   \n",
      "20971    Recently, optional stopping has been a subje...                 0   \n",
      "\n",
      "       Physics  Mathematics  Statistics  Quantitative Biology  \\\n",
      "0            0            0           0                     0   \n",
      "1            0            0           0                     0   \n",
      "2            0            1           0                     0   \n",
      "3            0            1           0                     0   \n",
      "4            0            0           1                     0   \n",
      "...        ...          ...         ...                   ...   \n",
      "20967        1            0           0                     0   \n",
      "20968        1            0           0                     0   \n",
      "20969        0            0           0                     0   \n",
      "20970        0            1           1                     0   \n",
      "20971        0            1           1                     0   \n",
      "\n",
      "       Quantitative Finance  \n",
      "0                         0  \n",
      "1                         0  \n",
      "2                         0  \n",
      "3                         0  \n",
      "4                         0  \n",
      "...                     ...  \n",
      "20967                     0  \n",
      "20968                     0  \n",
      "20969                     0  \n",
      "20970                     0  \n",
      "20971                     0  \n",
      "\n",
      "[20972 rows x 9 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20972 entries, 0 to 20971\n",
      "Data columns (total 9 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   ID                    20972 non-null  int64 \n",
      " 1   TITLE                 20972 non-null  object\n",
      " 2   ABSTRACT              20972 non-null  object\n",
      " 3   Computer Science      20972 non-null  int64 \n",
      " 4   Physics               20972 non-null  int64 \n",
      " 5   Mathematics           20972 non-null  int64 \n",
      " 6   Statistics            20972 non-null  int64 \n",
      " 7   Quantitative Biology  20972 non-null  int64 \n",
      " 8   Quantitative Finance  20972 non-null  int64 \n",
      "dtypes: int64(7), object(2)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel(\"Dataset-1.xlsx\")\n",
    "print(df)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>Computer Science</th>\n",
       "      <th>Physics</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>Statistics</th>\n",
       "      <th>Quantitative Biology</th>\n",
       "      <th>Quantitative Finance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Reconstructing Subject-Specific Effect Maps</td>\n",
       "      <td>Predictive models allow subject-specific inf...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Rotation Invariance Neural Network</td>\n",
       "      <td>Rotation invariance and translation invarian...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
       "      <td>We introduce and develop the notion of spher...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>A finite element approximation for the stochas...</td>\n",
       "      <td>The stochastic Landau--Lifshitz--Gilbert (LL...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
       "      <td>Fourier-transform infra-red (FTIR) spectra o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>On maximizing the fundamental frequency of the...</td>\n",
       "      <td>Let $\\Omega \\subset \\mathbb{R}^n$ be a bound...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>On the rotation period and shape of the hyperb...</td>\n",
       "      <td>We observed the newly discovered hyperbolic ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Adverse effects of polymer coating on heat tra...</td>\n",
       "      <td>The ability of metallic nanoparticles to sup...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>SPH calculations of Mars-scale collisions: the...</td>\n",
       "      <td>We model large-scale ($\\approx$2000km) impac...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>$\\mathcal{R}_{0}$ fails to predict the outbrea...</td>\n",
       "      <td>Time varying susceptibility of host at indiv...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                              TITLE  \\\n",
       "0   1        Reconstructing Subject-Specific Effect Maps   \n",
       "1   2                 Rotation Invariance Neural Network   \n",
       "2   3  Spherical polyharmonics and Poisson kernels fo...   \n",
       "3   4  A finite element approximation for the stochas...   \n",
       "4   5  Comparative study of Discrete Wavelet Transfor...   \n",
       "5   6  On maximizing the fundamental frequency of the...   \n",
       "6   7  On the rotation period and shape of the hyperb...   \n",
       "7   8  Adverse effects of polymer coating on heat tra...   \n",
       "8   9  SPH calculations of Mars-scale collisions: the...   \n",
       "9  10  $\\mathcal{R}_{0}$ fails to predict the outbrea...   \n",
       "\n",
       "                                            ABSTRACT  Computer Science  \\\n",
       "0    Predictive models allow subject-specific inf...                 1   \n",
       "1    Rotation invariance and translation invarian...                 1   \n",
       "2    We introduce and develop the notion of spher...                 0   \n",
       "3    The stochastic Landau--Lifshitz--Gilbert (LL...                 0   \n",
       "4    Fourier-transform infra-red (FTIR) spectra o...                 1   \n",
       "5    Let $\\Omega \\subset \\mathbb{R}^n$ be a bound...                 0   \n",
       "6    We observed the newly discovered hyperbolic ...                 0   \n",
       "7    The ability of metallic nanoparticles to sup...                 0   \n",
       "8    We model large-scale ($\\approx$2000km) impac...                 0   \n",
       "9    Time varying susceptibility of host at indiv...                 0   \n",
       "\n",
       "   Physics  Mathematics  Statistics  Quantitative Biology  \\\n",
       "0        0            0           0                     0   \n",
       "1        0            0           0                     0   \n",
       "2        0            1           0                     0   \n",
       "3        0            1           0                     0   \n",
       "4        0            0           1                     0   \n",
       "5        0            1           0                     0   \n",
       "6        1            0           0                     0   \n",
       "7        1            0           0                     0   \n",
       "8        1            0           0                     0   \n",
       "9        0            0           0                     1   \n",
       "\n",
       "   Quantitative Finance  \n",
       "0                     0  \n",
       "1                     0  \n",
       "2                     0  \n",
       "3                     0  \n",
       "4                     0  \n",
       "5                     0  \n",
       "6                     0  \n",
       "7                     0  \n",
       "8                     0  \n",
       "9                     0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Data distribution\n",
    "## which graphs or plots better suggests the data distributions? -> histogram, boxplots\n",
    "# import matplotlib.pyplot as plt \n",
    "# plt.bar(X[\"Title\"],y)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.87      0.82      1692\n",
      "           1       0.93      0.81      0.87      1226\n",
      "           2       0.82      0.76      0.79      1150\n",
      "           3       0.73      0.79      0.76      1069\n",
      "           4       0.67      0.18      0.28       122\n",
      "           5       0.86      0.27      0.41        45\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      5304\n",
      "   macro avg       0.80      0.61      0.65      5304\n",
      "weighted avg       0.81      0.80      0.80      5304\n",
      " samples avg       0.80      0.82      0.79      5304\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "## train-test split\n",
    "\n",
    "# X -> TITLE, ABSTRACT and Y -> labels(Phy, Maths, Stats, Quantitative Biology, Quant Finance)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix,ConfusionMatrixDisplay\n",
    "\n",
    "df['text'] = df['TITLE'] + ' ' + df['ABSTRACT']\n",
    "X = df['text']\n",
    "y = df[['Computer Science','Physics','Mathematics','Statistics','Quantitative Biology','Quantitative Finance']]\n",
    "\n",
    "\n",
    "# feature engineering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "X_train, X_test, y_train,y_test = train_test_split(X_tfidf,y,test_size=0.2,random_state=42)\n",
    "\n",
    "# why have we chosen the MultinomialNB \n",
    "nb = MultinomialNB(alpha=0.1)\n",
    "multi_target_nb = MultiOutputClassifier(nb)\n",
    "multi_target_nb.fit(X_train,y_train)\n",
    "\n",
    "y_pred = multi_target_nb.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Computer Science</th>\n",
       "      <th>Physics</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>Statistics</th>\n",
       "      <th>Quantitative Biology</th>\n",
       "      <th>Quantitative Finance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20967</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20968</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20969</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20970</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20971</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20972 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Computer Science  Physics  Mathematics  Statistics  \\\n",
       "0                     1        0            0           0   \n",
       "1                     1        0            0           0   \n",
       "2                     0        0            1           0   \n",
       "3                     0        0            1           0   \n",
       "4                     1        0            0           1   \n",
       "...                 ...      ...          ...         ...   \n",
       "20967                 1        1            0           0   \n",
       "20968                 0        1            0           0   \n",
       "20969                 1        0            0           0   \n",
       "20970                 0        0            1           1   \n",
       "20971                 0        0            1           1   \n",
       "\n",
       "       Quantitative Biology  Quantitative Finance  \n",
       "0                         0                     0  \n",
       "1                         0                     0  \n",
       "2                         0                     0  \n",
       "3                         0                     0  \n",
       "4                         0                     0  \n",
       "...                     ...                   ...  \n",
       "20967                     0                     0  \n",
       "20968                     0                     0  \n",
       "20969                     0                     0  \n",
       "20970                     0                     0  \n",
       "20971                     0                     0  \n",
       "\n",
       "[20972 rows x 6 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model building and training\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#TODO: based on my data distribution how do I decide which classifier to choose from naive_bayes scikit library\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1237: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, binary problems will be fit as proper binary  logistic regression models (as if multi_class='ovr' were set). Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1237: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, binary problems will be fit as proper binary  logistic regression models (as if multi_class='ovr' were set). Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1237: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, binary problems will be fit as proper binary  logistic regression models (as if multi_class='ovr' were set). Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1237: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, binary problems will be fit as proper binary  logistic regression models (as if multi_class='ovr' were set). Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1237: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, binary problems will be fit as proper binary  logistic regression models (as if multi_class='ovr' were set). Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1237: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, binary problems will be fit as proper binary  logistic regression models (as if multi_class='ovr' were set). Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is:  0.6522050059594756\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "def data_preprocessing(X,y):\n",
    "    # utilize the logistic regression for the multi-label classification problem. \n",
    "    # preprocess the text data using techniques like TF-IDF or word embeddings to convert them into numerical features.\n",
    "    vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "    X_tfidf = vectorizer.fit_transform(X)\n",
    "    X_train, X_test, y_train,y_test = train_test_split(X_tfidf,y,test_size=0.2,random_state=42)\n",
    "    # which one to chose and when for example b/w the TFIDF one and the word embeddings\n",
    "    base_model = LogisticRegression(multi_class='multinomial',solver='lbfgs',max_iter=1000,C=1.0)\n",
    "    multi_model = MultiOutputClassifier(base_model)\n",
    "    multi_model.fit(X_train,y_train)\n",
    "    y_pred = multi_model.predict(X_test)\n",
    "    print(\"Accuracy is: \",accuracy_score(y_test,y_pred))\n",
    "    \n",
    "data_preprocessing(X,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X = df['TITLE'] + ' ' + df['ABSTRACT']\n",
    "y = df[['Computer Science','Physics','Mathematics','Statistics','Quantitative Biology','Quantitative Finance']]\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "X_train, X_test, y_train,y_test = train_test_split(X_tfidf,y,test_size=0.2,random_state=42)\n",
    "\n",
    "# what is the kernel in SVM \n",
    "svm = SVC(kernel='linear',probability=True)\n",
    "multi_svm = MultiOutputClassifier(svm)\n",
    "multi_svm.fit(X_train,y_train)\n",
    "y_pred = multi_svm.predict(X_train)\n",
    "accuracy = accuracy_score(y_pred,y_test)\n",
    "print(\" accuracy score is \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 968333 stored elements and shape (16777, 5000)>\n",
      "  Coords\tValues\n",
      "  (0, 3820)\t0.057549140282289156\n",
      "  (0, 1180)\t0.034975370011717924\n",
      "  (0, 2691)\t0.054608614269348654\n",
      "  (0, 2864)\t0.03929349174465749\n",
      "  (0, 533)\t0.06990351224906194\n",
      "  (0, 231)\t0.044584790426710603\n",
      "  (0, 3925)\t0.03320578834359145\n",
      "  (0, 4597)\t0.048511194831599845\n",
      "  (0, 4575)\t0.052226884662400525\n",
      "  (0, 772)\t0.1197778819647435\n",
      "  (0, 4622)\t0.03828805449116095\n",
      "  (0, 4013)\t0.06507155526518951\n",
      "  (0, 518)\t0.06069618323610198\n",
      "  (0, 4061)\t0.06567256104409243\n",
      "  (0, 4805)\t0.04313413205367747\n",
      "  (0, 4133)\t0.046508041276487444\n",
      "  (0, 1642)\t0.07025751636434542\n",
      "  (0, 3138)\t0.05287711693222406\n",
      "  (0, 4265)\t0.1630848877448597\n",
      "  (0, 4844)\t0.06397713507924703\n",
      "  (0, 2822)\t0.06032517927336378\n",
      "  (0, 3655)\t0.356016629598416\n",
      "  (0, 1092)\t0.05863877897990759\n",
      "  (0, 4927)\t0.06648295291202731\n",
      "  (0, 3728)\t0.047469363612620954\n",
      "  :\t:\n",
      "  (16776, 1850)\t0.061654724205917034\n",
      "  (16776, 162)\t0.059438322367934636\n",
      "  (16776, 4740)\t0.06163062667260762\n",
      "  (16776, 3233)\t0.07084240435415212\n",
      "  (16776, 819)\t0.2355308657121376\n",
      "  (16776, 3029)\t0.07667153460280664\n",
      "  (16776, 2272)\t0.07767940640710393\n",
      "  (16776, 1929)\t0.07060380214657971\n",
      "  (16776, 3827)\t0.1455106008974924\n",
      "  (16776, 1629)\t0.14783343812192784\n",
      "  (16776, 4193)\t0.06867053016261697\n",
      "  (16776, 3754)\t0.08907473647599562\n",
      "  (16776, 2511)\t0.24848546650199\n",
      "  (16776, 3753)\t0.08558797155270173\n",
      "  (16776, 3752)\t0.16426990268642042\n",
      "  (16776, 3707)\t0.09867175231864216\n",
      "  (16776, 4054)\t0.07833990436538768\n",
      "  (16776, 971)\t0.07032302157631981\n",
      "  (16776, 3041)\t0.08034627700204014\n",
      "  (16776, 4352)\t0.22956913148753771\n",
      "  (16776, 1767)\t0.07084240435415212\n",
      "  (16776, 4719)\t0.5333173542765791\n",
      "  (16776, 4289)\t0.07518944073350177\n",
      "  (16776, 2304)\t0.07539131478190132\n",
      "  (16776, 4449)\t0.08926595971724112\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "- what is RBF kernel in svm : fits linear model in transformed space\n",
    "- what is the penalty in logistic regression\n",
    "\n",
    "\n",
    "## MLP Implementation for the classification task\n",
    "\n",
    "### questions\n",
    "  - how do the input size decided ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      precision    recall  f1-score   support\n",
      "\n",
      "    Computer Science       0.74      0.74      0.74      1692\n",
      "             Physics       0.77      0.84      0.81      1226\n",
      "         Mathematics       0.72      0.75      0.73      1150\n",
      "          Statistics       0.63      0.64      0.63      1069\n",
      "Quantitative Biology       0.46      0.26      0.33       122\n",
      "Quantitative Finance       0.73      0.36      0.48        45\n",
      "\n",
      "           micro avg       0.72      0.73      0.72      5304\n",
      "           macro avg       0.67      0.60      0.62      5304\n",
      "        weighted avg       0.71      0.73      0.72      5304\n",
      "         samples avg       0.74      0.76      0.73      5304\n",
      "\n",
      "Overall Accuracy: 0.5275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#TODO: try to implement the following and compare the results.\n",
    "\n",
    "'''\n",
    "Best Parameters: {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (5,), 'solver': 'sgd'}\n",
    "Best Score: 0.9987261146496815\n",
    "'''\n",
    "\n",
    "\n",
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(5,10),activation='relu',solver='adam',alpha=0.0001)\n",
    "mlp_classifier.fit(X_train,y_train)\n",
    "y_pred = mlp_classifier.predict(X_test)\n",
    "\n",
    "# Round predictions to 0 or 1\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "print(classification_report(y_test, y_pred_binary, target_names=[\n",
    "    'Computer Science', 'Physics', 'Mathematics', 'Statistics', \n",
    "    'Quantitative Biology', 'Quantitative Finance'\n",
    "]))\n",
    "\n",
    "# Calculate overall accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "print(f\"Overall Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.68791136\n",
      "Iteration 2, loss = 1.33266176\n",
      "Iteration 3, loss = 0.96995388\n",
      "Iteration 4, loss = 0.82904822\n",
      "Iteration 5, loss = 0.71221854\n",
      "Iteration 6, loss = 0.60149460\n",
      "Iteration 7, loss = 0.48855522\n",
      "Iteration 8, loss = 0.37332579\n",
      "Iteration 9, loss = 0.26664911\n",
      "Iteration 10, loss = 0.17548167\n",
      "Iteration 11, loss = 0.10798356\n",
      "Iteration 12, loss = 0.06522094\n",
      "Iteration 13, loss = 0.03973699\n",
      "Iteration 14, loss = 0.02578171\n",
      "Iteration 15, loss = 0.01789852\n",
      "Iteration 16, loss = 0.01312397\n",
      "Iteration 17, loss = 0.01018717\n",
      "Iteration 18, loss = 0.00817570\n",
      "Iteration 19, loss = 0.00670836\n",
      "Iteration 20, loss = 0.00563122\n",
      "Iteration 21, loss = 0.00478494\n",
      "Iteration 22, loss = 0.00412408\n",
      "Iteration 23, loss = 0.00362020\n",
      "Iteration 24, loss = 0.00323461\n",
      "Iteration 25, loss = 0.00292752\n",
      "Iteration 26, loss = 0.00268365\n",
      "Iteration 27, loss = 0.00248500\n",
      "Iteration 28, loss = 0.00231987\n",
      "Iteration 29, loss = 0.00217825\n",
      "Iteration 30, loss = 0.00205868\n",
      "Iteration 31, loss = 0.00195466\n",
      "Iteration 32, loss = 0.00186548\n",
      "Iteration 33, loss = 0.00178905\n",
      "Iteration 34, loss = 0.00172187\n",
      "Iteration 35, loss = 0.00166268\n",
      "Iteration 36, loss = 0.00161136\n",
      "Iteration 37, loss = 0.00156535\n",
      "Iteration 38, loss = 0.00152515\n",
      "Iteration 39, loss = 0.00148891\n",
      "Iteration 40, loss = 0.00145679\n",
      "Iteration 41, loss = 0.00142794\n",
      "Iteration 42, loss = 0.00140153\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "    Computer Science       0.79      0.80      0.79      1692\n",
      "             Physics       0.85      0.85      0.85      1226\n",
      "         Mathematics       0.79      0.78      0.78      1150\n",
      "          Statistics       0.72      0.69      0.71      1069\n",
      "Quantitative Biology       0.58      0.34      0.42       122\n",
      "Quantitative Finance       0.78      0.40      0.53        45\n",
      "\n",
      "           micro avg       0.79      0.77      0.78      5304\n",
      "           macro avg       0.75      0.64      0.68      5304\n",
      "        weighted avg       0.79      0.77      0.78      5304\n",
      "         samples avg       0.80      0.81      0.78      5304\n",
      "\n",
      "Overall Accuracy: 0.6005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "# Define the MLP model\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64, 32),  # Three hidden layers\n",
    "    activation='relu',                 # ReLU activation function\n",
    "    solver='adam',                     # Adam optimizer\n",
    "    alpha=0.0001,                      # L2 regularization parameter\n",
    "    batch_size='auto',                 # Automatic batch size\n",
    "    learning_rate='adaptive',          # Adaptive learning rate\n",
    "    max_iter=1000,                     # Maximum number of iterations\n",
    "    random_state=42,                   # For reproducibility\n",
    "    verbose=True                       # Print progress\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "mlp.fit(X_train, y_train)\n",
    "# Make predictions\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Round predictions to 0 or 1\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Print performance metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_binary, target_names=[\n",
    "    'Computer Science', 'Physics', 'Mathematics', 'Statistics', \n",
    "    'Quantitative Biology', 'Quantitative Finance'\n",
    "]))\n",
    "\n",
    "# Calculate overall accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "print(f\"Overall Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question2: Contains text from certain domains and you have to predict the domain.\n",
    "\n",
    "- i shall be using dataprocessing\n",
    "- tf-idf followed by the naive bayes classification and \n",
    "- word embeddings using the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ID                                            Content         Domain\n",
      "0        1  engali Binodiini Ekti Natir Putul Chaalchitro ...  Entertainment\n",
      "1        2   ChiefsAholic A Wolf In Chiefs Clothing articl...  Entertainment\n",
      "2        3  Kabandha Your Rating Write a review Optional C...  Entertainment\n",
      "3        4  In Bruges 2008 R 1h 47m IMDb RATING 79 10 474K...  Entertainment\n",
      "4        5  Men in Black 2012 PG13 1h 46m IMDb RATING 68 1...  Entertainment\n",
      "...    ...                                                ...            ...\n",
      "3922  3923   Kerala with its Munnar Wayanad Kochi Alleppey...        Tourism\n",
      "3923  3924   Netaji Subhash Chandra Bose Airport CCU Kolka...        Tourism\n",
      "3924  3925  Spanning over 800 square kilometres in the Alw...        Tourism\n",
      "3925  3926  Located in the Eastern part of India West Beng...        Tourism\n",
      "3926  3927   Maharaja Bir Bikram Airport IXA Agartala Agar...        Tourism\n",
      "\n",
      "[3927 rows x 3 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3927 entries, 0 to 3926\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   ID       3927 non-null   int64 \n",
      " 1   Content  3927 non-null   object\n",
      " 2   Domain   3927 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 92.2+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_a2d2 = pd.read_excel(\"A2D2.xlsx\")\n",
    "print(df_a2d2)\n",
    "df_a2d2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform data preprocessing stemming and lemmitization\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import nltk\n",
    "# Ensure required resources are downloaded\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Content</th>\n",
       "      <th>Domain</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>engali Binodiini Ekti Natir Putul Chaalchitro ...</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>engali binodiini ekti natir putul chaalchitro ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ChiefsAholic A Wolf In Chiefs Clothing articl...</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>chiefsaholic wolf chief clothing articleshowcm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Kabandha Your Rating Write a review Optional C...</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>kabandha rating write review optional characte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>In Bruges 2008 R 1h 47m IMDb RATING 79 10 474K...</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>bruges r h imdb rating k rating rate popularit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Men in Black 2012 PG13 1h 46m IMDb RATING 68 1...</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>men black pg h imdb rating k rating rate popul...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                            Content         Domain  \\\n",
       "0   1  engali Binodiini Ekti Natir Putul Chaalchitro ...  Entertainment   \n",
       "1   2   ChiefsAholic A Wolf In Chiefs Clothing articl...  Entertainment   \n",
       "2   3  Kabandha Your Rating Write a review Optional C...  Entertainment   \n",
       "3   4  In Bruges 2008 R 1h 47m IMDb RATING 79 10 474K...  Entertainment   \n",
       "4   5  Men in Black 2012 PG13 1h 46m IMDb RATING 68 1...  Entertainment   \n",
       "\n",
       "                                      processed_text  \n",
       "0  engali binodiini ekti natir putul chaalchitro ...  \n",
       "1  chiefsaholic wolf chief clothing articleshowcm...  \n",
       "2  kabandha rating write review optional characte...  \n",
       "3  bruges r h imdb rating k rating rate popularit...  \n",
       "4  men black pg h imdb rating k rating rate popul...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    #text = re.sub(r'^a-zA-Z\\s','',text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = word_tokenize(text) # internally how it works show with an example.\n",
    "    #remove stopwords\n",
    "    tokens=[word for word in tokens if word not in stopwords.words(\"english\")]\n",
    "    # lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df_a2d2['processed_text'] = df_a2d2['Content'].apply(preprocess_text)\n",
    "df_a2d2.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf based modeld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Entertainment', 'Healthcare', 'Sports', 'Technology', 'Tourism'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_a2d2['Domain'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X = df_a2d2['processed_text']\n",
    "y = df_a2d2['Domain']\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "X_train, X_test, y_train,y_test = train_test_split(X_tfidf,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9987277353689568\n",
      "\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Entertainment       0.99      1.00      1.00       146\n",
      "   Healthcare       1.00      1.00      1.00       193\n",
      "       Sports       1.00      1.00      1.00        73\n",
      "   Technology       1.00      1.00      1.00       260\n",
      "      Tourism       1.00      1.00      1.00       114\n",
      "\n",
      "     accuracy                           1.00       786\n",
      "    macro avg       1.00      1.00      1.00       786\n",
      " weighted avg       1.00      1.00      1.00       786\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "multinomial_nb = MultinomialNB()\n",
    "multinomial_nb.fit(X_train,y_train)\n",
    "y_pred = multinomial_nb.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM for question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Entertainment       1.00      1.00      1.00       146\n",
      "   Healthcare       1.00      1.00      1.00       193\n",
      "       Sports       1.00      1.00      1.00        73\n",
      "   Technology       1.00      1.00      1.00       260\n",
      "      Tourism       1.00      1.00      1.00       114\n",
      "\n",
      "     accuracy                           1.00       786\n",
      "    macro avg       1.00      1.00      1.00       786\n",
      " weighted avg       1.00      1.00      1.00       786\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_a2d2['processed_text'], df_a2d2['Domain'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 3: Apply TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "svm_classifier = SVC(kernel='linear',random_state=42)\n",
    "svm_classifier.fit(X_train_tfidf,y_train)\n",
    "y_pred  = svm_classifier.predict(X_test_tfidf)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply MLP algorithm\n",
    "- in my case let us say i have some 3XXX rows and 1 column, what shall be my ideal input, hidden and output layers.\n",
    "  - outputlayer: 5\n",
    "  - hidden layers:  \n",
    "  - input layers: \n",
    "  - what shall be the size a single, 2 hidden or 3 hidden layers architecture\n",
    "- how do we check for the overfitting or the underfitting of my data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.48580668\n",
      "Iteration 2, loss = 1.42508813\n",
      "Iteration 3, loss = 1.36335755\n",
      "Iteration 4, loss = 1.30025341\n",
      "Iteration 5, loss = 1.23852523\n",
      "Iteration 6, loss = 1.17945750\n",
      "Iteration 7, loss = 1.12293472\n",
      "Iteration 8, loss = 1.06916004\n",
      "Iteration 9, loss = 1.01801782\n",
      "Iteration 10, loss = 0.96865724\n",
      "Iteration 11, loss = 0.92087251\n",
      "Iteration 12, loss = 0.87401414\n",
      "Iteration 13, loss = 0.82838432\n",
      "Iteration 14, loss = 0.78417650\n",
      "Iteration 15, loss = 0.74137804\n",
      "Iteration 16, loss = 0.70066267\n",
      "Iteration 17, loss = 0.66201728\n",
      "Iteration 18, loss = 0.62554298\n",
      "Iteration 19, loss = 0.59168109\n",
      "Iteration 20, loss = 0.56010180\n",
      "Iteration 21, loss = 0.53087872\n",
      "Iteration 22, loss = 0.50406639\n",
      "Iteration 23, loss = 0.47946030\n",
      "Iteration 24, loss = 0.45683488\n",
      "Iteration 25, loss = 0.43624696\n",
      "Iteration 26, loss = 0.41740905\n",
      "Iteration 27, loss = 0.40031668\n",
      "Iteration 28, loss = 0.38487584\n",
      "Iteration 29, loss = 0.37069409\n",
      "Iteration 30, loss = 0.35787277\n",
      "Iteration 31, loss = 0.34622807\n",
      "Iteration 32, loss = 0.33572463\n",
      "Iteration 33, loss = 0.32602363\n",
      "Iteration 34, loss = 0.31724426\n",
      "Iteration 35, loss = 0.30930050\n",
      "Iteration 36, loss = 0.30195828\n",
      "Iteration 37, loss = 0.29522331\n",
      "Iteration 38, loss = 0.28903660\n",
      "Iteration 39, loss = 0.28337970\n",
      "Iteration 40, loss = 0.27813634\n",
      "Iteration 41, loss = 0.27325777\n",
      "Iteration 42, loss = 0.26871950\n",
      "Iteration 43, loss = 0.26438461\n",
      "Iteration 44, loss = 0.26030812\n",
      "Iteration 45, loss = 0.25639667\n",
      "Iteration 46, loss = 0.25257643\n",
      "Iteration 47, loss = 0.24877370\n",
      "Iteration 48, loss = 0.24489278\n",
      "Iteration 49, loss = 0.24072264\n",
      "Iteration 50, loss = 0.23629724\n",
      "Iteration 51, loss = 0.23130293\n",
      "Iteration 52, loss = 0.22579798\n",
      "Iteration 53, loss = 0.21965116\n",
      "Iteration 54, loss = 0.21264259\n",
      "Iteration 55, loss = 0.20502653\n",
      "Iteration 56, loss = 0.19687452\n",
      "Iteration 57, loss = 0.18801719\n",
      "Iteration 58, loss = 0.17884610\n",
      "Iteration 59, loss = 0.16949253\n",
      "Iteration 60, loss = 0.16025381\n",
      "Iteration 61, loss = 0.15125034\n",
      "Iteration 62, loss = 0.14227960\n",
      "Iteration 63, loss = 0.13391799\n",
      "Iteration 64, loss = 0.12600864\n",
      "Iteration 65, loss = 0.11863371\n",
      "Iteration 66, loss = 0.11154554\n",
      "Iteration 67, loss = 0.10521284\n",
      "Iteration 68, loss = 0.09916089\n",
      "Iteration 69, loss = 0.09364276\n",
      "Iteration 70, loss = 0.08852637\n",
      "Iteration 71, loss = 0.08371477\n",
      "Iteration 72, loss = 0.07931000\n",
      "Iteration 73, loss = 0.07517996\n",
      "Iteration 74, loss = 0.07130329\n",
      "Iteration 75, loss = 0.06786019\n",
      "Iteration 76, loss = 0.06456693\n",
      "Iteration 77, loss = 0.06156492\n",
      "Iteration 78, loss = 0.05878323\n",
      "Iteration 79, loss = 0.05618635\n",
      "Iteration 80, loss = 0.05378460\n",
      "Iteration 81, loss = 0.05154186\n",
      "Iteration 82, loss = 0.04945350\n",
      "Iteration 83, loss = 0.04747631\n",
      "Iteration 84, loss = 0.04563507\n",
      "Iteration 85, loss = 0.04391431\n",
      "Iteration 86, loss = 0.04229313\n",
      "Iteration 87, loss = 0.04076430\n",
      "Iteration 88, loss = 0.03931897\n",
      "Iteration 89, loss = 0.03795525\n",
      "Iteration 90, loss = 0.03666209\n",
      "Iteration 91, loss = 0.03544962\n",
      "Iteration 92, loss = 0.03429696\n",
      "Iteration 93, loss = 0.03318765\n",
      "Iteration 94, loss = 0.03215396\n",
      "Iteration 95, loss = 0.03114734\n",
      "Iteration 96, loss = 0.03021786\n",
      "Iteration 97, loss = 0.02931015\n",
      "Iteration 98, loss = 0.02844850\n",
      "Iteration 99, loss = 0.02762829\n",
      "Iteration 100, loss = 0.02685301\n",
      "Iteration 101, loss = 0.02609610\n",
      "Iteration 102, loss = 0.02538138\n",
      "Iteration 103, loss = 0.02469812\n",
      "Iteration 104, loss = 0.02403613\n",
      "Iteration 105, loss = 0.02340962\n",
      "Iteration 106, loss = 0.02280087\n",
      "Iteration 107, loss = 0.02222579\n",
      "Iteration 108, loss = 0.02166264\n",
      "Iteration 109, loss = 0.02112358\n",
      "Iteration 110, loss = 0.02060896\n",
      "Iteration 111, loss = 0.02011708\n",
      "Iteration 112, loss = 0.01964247\n",
      "Iteration 113, loss = 0.01917022\n",
      "Iteration 114, loss = 0.01872924\n",
      "Iteration 115, loss = 0.01829931\n",
      "Iteration 116, loss = 0.01788645\n",
      "Iteration 117, loss = 0.01748366\n",
      "Iteration 118, loss = 0.01709850\n",
      "Iteration 119, loss = 0.01672695\n",
      "Iteration 120, loss = 0.01636932\n",
      "Iteration 121, loss = 0.01601986\n",
      "Iteration 122, loss = 0.01568142\n",
      "Iteration 123, loss = 0.01535703\n",
      "Iteration 124, loss = 0.01503984\n",
      "Iteration 125, loss = 0.01473485\n",
      "Iteration 126, loss = 0.01443901\n",
      "Iteration 127, loss = 0.01415211\n",
      "Iteration 128, loss = 0.01387491\n",
      "Iteration 129, loss = 0.01360191\n",
      "Iteration 130, loss = 0.01334476\n",
      "Iteration 131, loss = 0.01308988\n",
      "Iteration 132, loss = 0.01284316\n",
      "Iteration 133, loss = 0.01260444\n",
      "Iteration 134, loss = 0.01237256\n",
      "Iteration 135, loss = 0.01214703\n",
      "Iteration 136, loss = 0.01192503\n",
      "Iteration 137, loss = 0.01171352\n",
      "Iteration 138, loss = 0.01150386\n",
      "Iteration 139, loss = 0.01130411\n",
      "Iteration 140, loss = 0.01110564\n",
      "Iteration 141, loss = 0.01091689\n",
      "Iteration 142, loss = 0.01072908\n",
      "Iteration 143, loss = 0.01054835\n",
      "Iteration 144, loss = 0.01037307\n",
      "Iteration 145, loss = 0.01020022\n",
      "Iteration 146, loss = 0.01003270\n",
      "Iteration 147, loss = 0.00986924\n",
      "Iteration 148, loss = 0.00970943\n",
      "Iteration 149, loss = 0.00955533\n",
      "Iteration 150, loss = 0.00940409\n",
      "Iteration 151, loss = 0.00925574\n",
      "Iteration 152, loss = 0.00911204\n",
      "Iteration 153, loss = 0.00897270\n",
      "Iteration 154, loss = 0.00883461\n",
      "Iteration 155, loss = 0.00870220\n",
      "Iteration 156, loss = 0.00857028\n",
      "Iteration 157, loss = 0.00844265\n",
      "Iteration 158, loss = 0.00831738\n",
      "Iteration 159, loss = 0.00819570\n",
      "Iteration 160, loss = 0.00807640\n",
      "Iteration 161, loss = 0.00795865\n",
      "Iteration 162, loss = 0.00784617\n",
      "Iteration 163, loss = 0.00773374\n",
      "Iteration 164, loss = 0.00762488\n",
      "Iteration 165, loss = 0.00751975\n",
      "Iteration 166, loss = 0.00741553\n",
      "Iteration 167, loss = 0.00731358\n",
      "Iteration 168, loss = 0.00721206\n",
      "Iteration 169, loss = 0.00711440\n",
      "Iteration 170, loss = 0.00702096\n",
      "Iteration 171, loss = 0.00692516\n",
      "Iteration 172, loss = 0.00683455\n",
      "Iteration 173, loss = 0.00674419\n",
      "Iteration 174, loss = 0.00665709\n",
      "Iteration 175, loss = 0.00657205\n",
      "Iteration 176, loss = 0.00648713\n",
      "Iteration 177, loss = 0.00640462\n",
      "Iteration 178, loss = 0.00632333\n",
      "Iteration 179, loss = 0.00624448\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 1.0\n",
      "\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Entertainment       1.00      1.00      1.00       146\n",
      "   Healthcare       1.00      1.00      1.00       193\n",
      "       Sports       1.00      1.00      1.00        73\n",
      "   Technology       1.00      1.00      1.00       260\n",
      "      Tourism       1.00      1.00      1.00       114\n",
      "\n",
      "     accuracy                           1.00       786\n",
      "    macro avg       1.00      1.00      1.00       786\n",
      " weighted avg       1.00      1.00      1.00       786\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import  MLPClassifier\n",
    "\n",
    "mlp_classifier = MLPClassifier(\n",
    "    hidden_layer_sizes=(3,),#(128, 64, 32),  # Three hidden layers\n",
    "    activation='tanh',                 # ReLU activation function\n",
    "    solver='sgd',                     # Adam optimizer\n",
    "    alpha=0.0001,                      # L2 regularization parameter\n",
    "    batch_size='auto',                 # Automatic batch size\n",
    "    learning_rate='adaptive',          # Adaptive learning rate\n",
    "    max_iter=1000,                     # Maximum number of iterations\n",
    "    random_state=42,                   # For reproducibility\n",
    "    verbose=True                       # Print progress\n",
    ")\n",
    "\n",
    "mlp_classifier.fit(X_train,y_train)\n",
    "y_pred = mlp_classifier.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (5,), 'solver': 'sgd'}\n",
      "Best Score: 0.9987261146496815\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Define the model\n",
    "model = MLPClassifier(max_iter=1000, random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(5,), (3,), (25, 5)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.0001, 0.001],\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=5)\n",
    "grid_search.fit(X_train,y_train)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us try this without the lemmitization step and check with \n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X = df_a2d2['Content']\n",
    "y = df_a2d2['Domain']\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "X_train, X_test, y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2758    Over the last few years haptic technology has ...\n",
      "3696    Abu Dhabi Airport 5h 15m One way 10891 Onwards...\n",
      "3864    A millennia old tradition of storytelling Maha...\n",
      "2510    The Redmi Note 11 Pro 5G is priced starting at...\n",
      "2172    Amphenol Commercial USB Type C Connector Syste...\n",
      "                              ...                        \n",
      "1130    ncird divis offic ncird cdc skip directli site...\n",
      "1294    Considerations The symptoms of an ear infectio...\n",
      "860     us govern releas first nation one health plan ...\n",
      "3507    Famous for watersports Popular for water sport...\n",
      "3174    Blockchain AI research lab YeagerAI has announ...\n",
      "Name: Content, Length: 3141, dtype: object\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Over the last few years haptic technology has undergone significant advancements across a wide range of industries Haptic devices are designed to create a sense of touch through vibration force or motion in environments where the opportunities for audio and visual feedback are limited From consumer electronics to automotive interfaces haptic feedback has become an integral feature that can enhance user experiences and create nextgen humanmachine interfaces HMI There has been a dynamic transition from conventional solutions to piezobased haptic technologies which provide speedy response times over highfrequency bandwidths in ultracompact form factors These devices have become essential in todays automobiles where limited space and power efficiencies dictate how haptic feedback can be employed on steering wheels dashboards and displays Figure 1 In todays modern cars haptic technologies play a key role on steering wheels dashboards and displays Image from Adobe Stock licensed The National Highway Traffic Safety Administration NHTSA reports that driver distraction in the US claimed the lives of 3308 lives in 2022 Haptic feedback that mimics the tactile feel of buttons keys and sliders on touch displays can help ensure drivers remain focused on the road and free of distraction But as vehicles become more advanced and equipped with the latest features space becomes a valued commodity and haptic feedback designs that might have been applicable several years ago are far from ideal today The State of Haptics Conventional haptic solutionsthose that took advantage of linear resonant actuators LRAs and solenoid solutionswere in widespread use in the automobile industry at a time when COVID19 began interrupting supply chains Manufacturing plant closures transportation restrictions and component shortages all had a negative impact on haptics helping to drive the cost of components to an alltime high during that time China a leading manufacturer of those components was facing import restrictions During all that the automotive industry was prioritizing the development of haptic feedback systems to enhance safety comfort and convenience Pandemic conditions ended up sparking innovation that increased the adoption of digital cockpits infotainment systems feedback for touchscreens and tactile buttons and controls Postpandemic saw the growth of driverassistance technologies that leveraged feedback for intuitive interfaces Haptics are continuing to play a pivotal role in enhancing the situational awareness of drivers and improving safety in autonomous vehicles Challenges in Haptic Feedback Design As with any industry there are challenges when it comes to haptic feedback designs for automobiles Integration for example requires coordinating with others along the design chain including manufacturers suppliers and technology providers Designing systems to enhance user experience while mitigating distractions is another challenge especially when it comes to balancing feedback intensity and duration for alerts and notifications Environmental factors also come into play with design considerations needed for variations in temperature vibration noise and lighting conditions Of course cost and complexity play a pivotal challenge as haptic feedback designs drive up manufacturing costs potentially leaving customers to consider if those systems provide any value Addressing those considerations when designing haptic systems requires collaboration and consideration to meet the needs of customers The new generation of haptic solutions had to move past LRAs and ERMs These conventional solutions are limited in the type of feedback they can provide whereas piezoelectric actuators make it possible for different highresolution feedback types vibration pulses clicks etc while increasing the amount of mass that could be pushed Figure 2 Figure 2 Piezoelectric actuators go beyond LRAs and ERMs by enabling different highresolution feedback types while also boosting the amount of mass that can be pushed Starter Kits Ease the Way While new piezoelectric designs provide increased performance and enhancements they are not a onesizefitsall solution and integration can be a challenge To overcome those issues companies have begun developing starter kits with a focus on space constraints and mechanical integration These kits are designed to introduce mechanical designers and engineers to haptic feedback and show how mechanical integration works using a combination of physical and digital design products For example TDKs PowerHap Development Starter Kit provides an introduction to haptic feedback technology complete with reference designs and a quickstart guide that engineers can use for fast prototyping While the kit is ideal for automotive applications it can also be employed to provide feedback for smartphones appliances ATMs vending machines gaming devices industrial equipment and medical devices This is in part due to the unique design of the PowerHap actuators a design which makes them compact yet powerful The basis of the PowerHap device is a piezo ceramic element to which stainlesssteel bows or cymbals are attached to both the top and bottom sides Figure 3 Figure 3 The PowerHap actuator is a piezo ceramic element to which stainlesssteel bows are attached The bows act as mechanical amplifiers similar to levers The bows act as mechanical amplifiers similar to levers increasing the displacement caused when the piezo element is excited with a driving voltage The bows amplify the contraction caused by the piezo effect by a factor of up to 15 making it possible to move very heavy masses such as automotive displaysFigure 4 below shows the PowerHap designed into a display module Figure 4 Shown here is the PowerHap actuator designed into a display module Wider Range of Feedback The piezo element allows PowerHap to produce a wider range of feedback over traditional solutions such as eccentric rotary motors and linear resonant actuators Unlike these entirely mechanical solutions piezo actuators can operate across a wide range of frequencies amplitude and waveforms to produce multiple types and different degrees of tactile feedback PowerHap actuators allow engineers to tune the feedback and surface effects by adjusting their frequency signal and amplitude Whats more they can also function as sensors by applying pressure on the bows which generates an electric charge The PowerHap Starter Kit comes packed with a seamless button assembly a round button assembly a driver board additional PowerHap devices a USB cable and a quickstart user guide Figure 5 There are additional addons to further enhance prototyping capabilities including additional sensors for specific applications such as automotive displays buttons and modules wearable technology and even a stylus that mimics the sensation of writing on paper Figure 5 The PowerHap Starter Kit comes with a seamless button assembly a round button assembly a driver board additional PowerHap devices a USB cable and a quickstart user guide Use Cases and RealWorld Examples Haptics are increasingly being integrated into ARVR applications where tactility and sensation can be critical elements in an immersive experience Imagine training to use equipment in a hazardous environment without actually being there or the ability to manipulate parts of an engine during its design phase to see how it functions before its ever built Digital design company Weart developed its TouchDiver haptic glove to make it possible to do such things The company used PowerHap actuators to make its dream a reality taking advantage of these devices compact design ability to generate a significant amplitude and wide bandwidth The TouchDiver glove provides tactile feedback through pressure force texture and temperature sensations allowing users to interact with virtual elements and environments With the PowerHap actuators the glove can render material roughness levels accurately All About the Driver Experience Automotive has been the premier application for haptics over the last few years and for good reasons The shift toward connected vehicles has driven the need for haptic feedback in advanced infotainment systems and interactive interfaces and manufacturers have prioritized feedback to enhance safety and usability for the next generation of vehicles Haptic feedback is also needed for autonomous vehicles where trust and safety are critical issues for drivers and passengers Perhaps the number one reason haptic design is at the top of the development pole is that it drives innovation and creates new possibilities for immersive user experiences TDKs PowerHap Development Starter Kit was engineered from the desire to make designing haptics easier and they have achieved that goal and more The automotive industry is capitalizing on the latest piezobased actuators over conventional solutions offering increased response times varied tactile sensations and compact form factors PowerHap actuators mitigate the challenges associated with conventional actuators making it easier to develop prototypes without added costs and integration issues driving innovation and immersive experiences Except where otherwise indicated all images used courtesy of TDK Electronics'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_train)\n\u001b[0;32m      2\u001b[0m multinomial_nb \u001b[38;5;241m=\u001b[39m MultinomialNB()\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmultinomial_nb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m multinomial_nb\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_score(y_test, y_pred))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\naive_bayes.py:735\u001b[0m, in \u001b[0;36m_BaseDiscreteNB.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    716\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit Naive Bayes classifier according to X, y.\u001b[39;00m\n\u001b[0;32m    717\u001b[0m \n\u001b[0;32m    718\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    733\u001b[0m \u001b[38;5;124;03m        Returns the instance itself.\u001b[39;00m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 735\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    736\u001b[0m     _, n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    738\u001b[0m     labelbin \u001b[38;5;241m=\u001b[39m LabelBinarizer()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\naive_bayes.py:581\u001b[0m, in \u001b[0;36m_BaseDiscreteNB._check_X_y\u001b[1;34m(self, X, y, reset)\u001b[0m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_X_y\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    580\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate X and y in fit methods.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:2961\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2959\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m   2960\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2961\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2962\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1370\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m     )\n\u001b[0;32m   1368\u001b[0m ensure_all_finite \u001b[38;5;241m=\u001b[39m _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[1;32m-> 1370\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1384\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1387\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1389\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1055\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1053\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1054\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1055\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m   1057\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1058\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m   1059\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_array_api.py:839\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[0;32m    837\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 839\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    841\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    842\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\series.py:1031\u001b[0m, in \u001b[0;36mSeries.__array__\u001b[1;34m(self, dtype, copy)\u001b[0m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    982\u001b[0m \u001b[38;5;124;03mReturn the values as a NumPy array.\u001b[39;00m\n\u001b[0;32m    983\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;124;03m      dtype='datetime64[ns]')\u001b[39;00m\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1030\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1031\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(values, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_copy_on_write() \u001b[38;5;129;01mand\u001b[39;00m astype_is_view(values\u001b[38;5;241m.\u001b[39mdtype, arr\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[0;32m   1033\u001b[0m     arr \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mview()\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Over the last few years haptic technology has undergone significant advancements across a wide range of industries Haptic devices are designed to create a sense of touch through vibration force or motion in environments where the opportunities for audio and visual feedback are limited From consumer electronics to automotive interfaces haptic feedback has become an integral feature that can enhance user experiences and create nextgen humanmachine interfaces HMI There has been a dynamic transition from conventional solutions to piezobased haptic technologies which provide speedy response times over highfrequency bandwidths in ultracompact form factors These devices have become essential in todays automobiles where limited space and power efficiencies dictate how haptic feedback can be employed on steering wheels dashboards and displays Figure 1 In todays modern cars haptic technologies play a key role on steering wheels dashboards and displays Image from Adobe Stock licensed The National Highway Traffic Safety Administration NHTSA reports that driver distraction in the US claimed the lives of 3308 lives in 2022 Haptic feedback that mimics the tactile feel of buttons keys and sliders on touch displays can help ensure drivers remain focused on the road and free of distraction But as vehicles become more advanced and equipped with the latest features space becomes a valued commodity and haptic feedback designs that might have been applicable several years ago are far from ideal today The State of Haptics Conventional haptic solutionsthose that took advantage of linear resonant actuators LRAs and solenoid solutionswere in widespread use in the automobile industry at a time when COVID19 began interrupting supply chains Manufacturing plant closures transportation restrictions and component shortages all had a negative impact on haptics helping to drive the cost of components to an alltime high during that time China a leading manufacturer of those components was facing import restrictions During all that the automotive industry was prioritizing the development of haptic feedback systems to enhance safety comfort and convenience Pandemic conditions ended up sparking innovation that increased the adoption of digital cockpits infotainment systems feedback for touchscreens and tactile buttons and controls Postpandemic saw the growth of driverassistance technologies that leveraged feedback for intuitive interfaces Haptics are continuing to play a pivotal role in enhancing the situational awareness of drivers and improving safety in autonomous vehicles Challenges in Haptic Feedback Design As with any industry there are challenges when it comes to haptic feedback designs for automobiles Integration for example requires coordinating with others along the design chain including manufacturers suppliers and technology providers Designing systems to enhance user experience while mitigating distractions is another challenge especially when it comes to balancing feedback intensity and duration for alerts and notifications Environmental factors also come into play with design considerations needed for variations in temperature vibration noise and lighting conditions Of course cost and complexity play a pivotal challenge as haptic feedback designs drive up manufacturing costs potentially leaving customers to consider if those systems provide any value Addressing those considerations when designing haptic systems requires collaboration and consideration to meet the needs of customers The new generation of haptic solutions had to move past LRAs and ERMs These conventional solutions are limited in the type of feedback they can provide whereas piezoelectric actuators make it possible for different highresolution feedback types vibration pulses clicks etc while increasing the amount of mass that could be pushed Figure 2 Figure 2 Piezoelectric actuators go beyond LRAs and ERMs by enabling different highresolution feedback types while also boosting the amount of mass that can be pushed Starter Kits Ease the Way While new piezoelectric designs provide increased performance and enhancements they are not a onesizefitsall solution and integration can be a challenge To overcome those issues companies have begun developing starter kits with a focus on space constraints and mechanical integration These kits are designed to introduce mechanical designers and engineers to haptic feedback and show how mechanical integration works using a combination of physical and digital design products For example TDKs PowerHap Development Starter Kit provides an introduction to haptic feedback technology complete with reference designs and a quickstart guide that engineers can use for fast prototyping While the kit is ideal for automotive applications it can also be employed to provide feedback for smartphones appliances ATMs vending machines gaming devices industrial equipment and medical devices This is in part due to the unique design of the PowerHap actuators a design which makes them compact yet powerful The basis of the PowerHap device is a piezo ceramic element to which stainlesssteel bows or cymbals are attached to both the top and bottom sides Figure 3 Figure 3 The PowerHap actuator is a piezo ceramic element to which stainlesssteel bows are attached The bows act as mechanical amplifiers similar to levers The bows act as mechanical amplifiers similar to levers increasing the displacement caused when the piezo element is excited with a driving voltage The bows amplify the contraction caused by the piezo effect by a factor of up to 15 making it possible to move very heavy masses such as automotive displaysFigure 4 below shows the PowerHap designed into a display module Figure 4 Shown here is the PowerHap actuator designed into a display module Wider Range of Feedback The piezo element allows PowerHap to produce a wider range of feedback over traditional solutions such as eccentric rotary motors and linear resonant actuators Unlike these entirely mechanical solutions piezo actuators can operate across a wide range of frequencies amplitude and waveforms to produce multiple types and different degrees of tactile feedback PowerHap actuators allow engineers to tune the feedback and surface effects by adjusting their frequency signal and amplitude Whats more they can also function as sensors by applying pressure on the bows which generates an electric charge The PowerHap Starter Kit comes packed with a seamless button assembly a round button assembly a driver board additional PowerHap devices a USB cable and a quickstart user guide Figure 5 There are additional addons to further enhance prototyping capabilities including additional sensors for specific applications such as automotive displays buttons and modules wearable technology and even a stylus that mimics the sensation of writing on paper Figure 5 The PowerHap Starter Kit comes with a seamless button assembly a round button assembly a driver board additional PowerHap devices a USB cable and a quickstart user guide Use Cases and RealWorld Examples Haptics are increasingly being integrated into ARVR applications where tactility and sensation can be critical elements in an immersive experience Imagine training to use equipment in a hazardous environment without actually being there or the ability to manipulate parts of an engine during its design phase to see how it functions before its ever built Digital design company Weart developed its TouchDiver haptic glove to make it possible to do such things The company used PowerHap actuators to make its dream a reality taking advantage of these devices compact design ability to generate a significant amplitude and wide bandwidth The TouchDiver glove provides tactile feedback through pressure force texture and temperature sensations allowing users to interact with virtual elements and environments With the PowerHap actuators the glove can render material roughness levels accurately All About the Driver Experience Automotive has been the premier application for haptics over the last few years and for good reasons The shift toward connected vehicles has driven the need for haptic feedback in advanced infotainment systems and interactive interfaces and manufacturers have prioritized feedback to enhance safety and usability for the next generation of vehicles Haptic feedback is also needed for autonomous vehicles where trust and safety are critical issues for drivers and passengers Perhaps the number one reason haptic design is at the top of the development pole is that it drives innovation and creates new possibilities for immersive user experiences TDKs PowerHap Development Starter Kit was engineered from the desire to make designing haptics easier and they have achieved that goal and more The automotive industry is capitalizing on the latest piezobased actuators over conventional solutions offering increased response times varied tactile sensations and compact form factors PowerHap actuators mitigate the challenges associated with conventional actuators making it easier to develop prototypes without added costs and integration issues driving innovation and immersive experiences Except where otherwise indicated all images used courtesy of TDK Electronics'"
     ]
    }
   ],
   "source": [
    "print(X_train)\n",
    "multinomial_nb = MultinomialNB()\n",
    "multinomial_nb.fit(X_train,y_train)\n",
    "y_pred = multinomial_nb.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

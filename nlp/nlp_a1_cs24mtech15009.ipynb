{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "The objective of this assignment is to independently develop a classification model for each of the two provided datasets. You will experiment with three different algorithms (excluding transformers) and analyze their performance\n",
    "\n",
    "Contains titles, abstracts, and topic labels of research articles. Your task is\n",
    "to predict the topic based on the title and abstract. \n",
    "\n",
    "what algorithms i can use for the label based classification.\n",
    "\n",
    "- multiclass classification algorithm\n",
    "  - MLP\n",
    "  - naive bayes\n",
    "  - Logistic Regression utilising softmax\n",
    "  - SVMs / decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\pantm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.1.5)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\pantm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openpyxl) (2.0.0)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\pantm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     ------------------ ------------------- 20.5/41.5 kB 108.9 kB/s eta 0:00:01\n",
      "     ---------------------------- --------- 30.7/41.5 kB 145.2 kB/s eta 0:00:01\n",
      "     -------------------------------------- 41.5/41.5 kB 166.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\pantm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\pantm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.5 MB 445.2 kB/s eta 0:00:04\n",
      "    --------------------------------------- 0.0/1.5 MB 445.2 kB/s eta 0:00:04\n",
      "    --------------------------------------- 0.0/1.5 MB 445.2 kB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.1/1.5 MB 262.6 kB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.1/1.5 MB 262.6 kB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.1/1.5 MB 204.8 kB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.1/1.5 MB 204.8 kB/s eta 0:00:08\n",
      "   -- ------------------------------------- 0.1/1.5 MB 229.7 kB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.1/1.5 MB 256.7 kB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.1/1.5 MB 256.7 kB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.1/1.5 MB 234.3 kB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.1/1.5 MB 234.3 kB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.1/1.5 MB 234.3 kB/s eta 0:00:06\n",
      "   --- ------------------------------------ 0.1/1.5 MB 218.6 kB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.1/1.5 MB 218.6 kB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.1/1.5 MB 202.9 kB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.1/1.5 MB 202.9 kB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.1/1.5 MB 202.9 kB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 0.2/1.5 MB 196.7 kB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 0.2/1.5 MB 196.7 kB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 0.2/1.5 MB 192.1 kB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 0.2/1.5 MB 192.1 kB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 0.2/1.5 MB 190.4 kB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 0.2/1.5 MB 190.4 kB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 0.2/1.5 MB 198.6 kB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 0.2/1.5 MB 196.7 kB/s eta 0:00:07\n",
      "   ------ --------------------------------- 0.2/1.5 MB 209.4 kB/s eta 0:00:07\n",
      "   ------- -------------------------------- 0.3/1.5 MB 218.6 kB/s eta 0:00:06\n",
      "   ------- -------------------------------- 0.3/1.5 MB 218.6 kB/s eta 0:00:06\n",
      "   ------- -------------------------------- 0.3/1.5 MB 218.6 kB/s eta 0:00:06\n",
      "   ------- -------------------------------- 0.3/1.5 MB 221.2 kB/s eta 0:00:06\n",
      "   -------- ------------------------------- 0.3/1.5 MB 221.1 kB/s eta 0:00:06\n",
      "   -------- ------------------------------- 0.3/1.5 MB 228.4 kB/s eta 0:00:06\n",
      "   --------- ------------------------------ 0.4/1.5 MB 239.6 kB/s eta 0:00:05\n",
      "   --------- ------------------------------ 0.4/1.5 MB 239.6 kB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 0.4/1.5 MB 250.1 kB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 0.4/1.5 MB 255.7 kB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 0.4/1.5 MB 255.7 kB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 0.4/1.5 MB 256.0 kB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 0.4/1.5 MB 256.0 kB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 0.4/1.5 MB 256.0 kB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 0.4/1.5 MB 239.5 kB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 0.4/1.5 MB 239.5 kB/s eta 0:00:05\n",
      "   ------------ --------------------------- 0.5/1.5 MB 238.4 kB/s eta 0:00:05\n",
      "   ------------ --------------------------- 0.5/1.5 MB 238.4 kB/s eta 0:00:05\n",
      "   ------------ --------------------------- 0.5/1.5 MB 234.1 kB/s eta 0:00:05\n",
      "   ------------ --------------------------- 0.5/1.5 MB 234.1 kB/s eta 0:00:05\n",
      "   ------------- -------------------------- 0.5/1.5 MB 235.2 kB/s eta 0:00:05\n",
      "   ------------- -------------------------- 0.5/1.5 MB 235.2 kB/s eta 0:00:05\n",
      "   ------------- -------------------------- 0.5/1.5 MB 232.7 kB/s eta 0:00:05\n",
      "   ------------- -------------------------- 0.5/1.5 MB 232.7 kB/s eta 0:00:05\n",
      "   ------------- -------------------------- 0.5/1.5 MB 232.7 kB/s eta 0:00:05\n",
      "   ------------- -------------------------- 0.5/1.5 MB 226.0 kB/s eta 0:00:05\n",
      "   ------------- -------------------------- 0.5/1.5 MB 226.0 kB/s eta 0:00:05\n",
      "   -------------- ------------------------- 0.5/1.5 MB 227.3 kB/s eta 0:00:05\n",
      "   -------------- ------------------------- 0.5/1.5 MB 227.3 kB/s eta 0:00:05\n",
      "   -------------- ------------------------- 0.6/1.5 MB 224.2 kB/s eta 0:00:05\n",
      "   --------------- ------------------------ 0.6/1.5 MB 226.7 kB/s eta 0:00:05\n",
      "   --------------- ------------------------ 0.6/1.5 MB 229.3 kB/s eta 0:00:04\n",
      "   --------------- ------------------------ 0.6/1.5 MB 229.3 kB/s eta 0:00:04\n",
      "   --------------- ------------------------ 0.6/1.5 MB 229.3 kB/s eta 0:00:04\n",
      "   --------------- ------------------------ 0.6/1.5 MB 229.3 kB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 0.6/1.5 MB 219.7 kB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 0.6/1.5 MB 219.7 kB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 0.6/1.5 MB 219.7 kB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 0.6/1.5 MB 216.1 kB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 0.6/1.5 MB 216.1 kB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 0.6/1.5 MB 214.9 kB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 0.7/1.5 MB 217.4 kB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 0.7/1.5 MB 219.6 kB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 0.7/1.5 MB 219.6 kB/s eta 0:00:04\n",
      "   ------------------ --------------------- 0.7/1.5 MB 219.6 kB/s eta 0:00:04\n",
      "   ------------------ --------------------- 0.7/1.5 MB 222.8 kB/s eta 0:00:04\n",
      "   ------------------- -------------------- 0.7/1.5 MB 228.1 kB/s eta 0:00:04\n",
      "   -------------------- ------------------- 0.8/1.5 MB 231.1 kB/s eta 0:00:04\n",
      "   -------------------- ------------------- 0.8/1.5 MB 231.1 kB/s eta 0:00:04\n",
      "   -------------------- ------------------- 0.8/1.5 MB 228.8 kB/s eta 0:00:04\n",
      "   -------------------- ------------------- 0.8/1.5 MB 230.6 kB/s eta 0:00:04\n",
      "   -------------------- ------------------- 0.8/1.5 MB 230.6 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 0.8/1.5 MB 227.4 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 0.8/1.5 MB 227.4 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 0.8/1.5 MB 227.4 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 0.8/1.5 MB 227.1 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 0.8/1.5 MB 227.1 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 0.8/1.5 MB 227.1 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 0.8/1.5 MB 227.1 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 0.8/1.5 MB 227.1 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 0.8/1.5 MB 227.1 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 0.8/1.5 MB 227.1 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 0.8/1.5 MB 227.1 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 0.8/1.5 MB 227.1 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 0.8/1.5 MB 227.1 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 0.8/1.5 MB 227.1 kB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 0.8/1.5 MB 205.0 kB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 0.8/1.5 MB 205.0 kB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 0.8/1.5 MB 202.0 kB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 0.8/1.5 MB 202.0 kB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 0.9/1.5 MB 201.7 kB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 0.9/1.5 MB 201.7 kB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 0.9/1.5 MB 201.7 kB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 0.9/1.5 MB 198.3 kB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 0.9/1.5 MB 198.3 kB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 0.9/1.5 MB 198.3 kB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 0.9/1.5 MB 197.3 kB/s eta 0:00:04\n",
      "   ------------------------ --------------- 0.9/1.5 MB 199.1 kB/s eta 0:00:03\n",
      "   ------------------------ --------------- 0.9/1.5 MB 199.1 kB/s eta 0:00:03\n",
      "   ------------------------ --------------- 0.9/1.5 MB 198.6 kB/s eta 0:00:03\n",
      "   ------------------------- -------------- 1.0/1.5 MB 199.7 kB/s eta 0:00:03\n",
      "   ------------------------- -------------- 1.0/1.5 MB 199.7 kB/s eta 0:00:03\n",
      "   ------------------------- -------------- 1.0/1.5 MB 198.6 kB/s eta 0:00:03\n",
      "   -------------------------- ------------- 1.0/1.5 MB 200.2 kB/s eta 0:00:03\n",
      "   -------------------------- ------------- 1.0/1.5 MB 200.2 kB/s eta 0:00:03\n",
      "   -------------------------- ------------- 1.0/1.5 MB 201.2 kB/s eta 0:00:03\n",
      "   -------------------------- ------------- 1.0/1.5 MB 201.2 kB/s eta 0:00:03\n",
      "   -------------------------- ------------- 1.0/1.5 MB 200.7 kB/s eta 0:00:03\n",
      "   --------------------------- ------------ 1.0/1.5 MB 201.7 kB/s eta 0:00:03\n",
      "   --------------------------- ------------ 1.0/1.5 MB 201.8 kB/s eta 0:00:03\n",
      "   --------------------------- ------------ 1.0/1.5 MB 201.8 kB/s eta 0:00:03\n",
      "   --------------------------- ------------ 1.0/1.5 MB 201.8 kB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 1.1/1.5 MB 201.5 kB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 1.1/1.5 MB 203.0 kB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 1.1/1.5 MB 203.0 kB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 1.1/1.5 MB 203.0 kB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 200.8 kB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 200.8 kB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 199.9 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 199.9 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 199.9 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 199.9 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 199.9 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 195.2 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 195.2 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 195.2 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 1.1/1.5 MB 194.0 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 1.1/1.5 MB 194.0 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 1.1/1.5 MB 194.0 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 1.2/1.5 MB 192.9 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 1.2/1.5 MB 192.9 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 1.2/1.5 MB 192.9 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 1.2/1.5 MB 189.6 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 1.2/1.5 MB 189.6 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 1.2/1.5 MB 190.1 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 1.2/1.5 MB 190.1 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 1.2/1.5 MB 190.1 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 1.2/1.5 MB 188.4 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 1.2/1.5 MB 188.4 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 1.2/1.5 MB 188.8 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 1.2/1.5 MB 188.8 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 1.2/1.5 MB 188.8 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 1.2/1.5 MB 187.9 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 1.2/1.5 MB 187.9 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 1.2/1.5 MB 187.9 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 1.3/1.5 MB 186.4 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 187.3 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 187.3 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 187.1 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 188.7 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 1.3/1.5 MB 190.0 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.3/1.5 MB 190.6 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.3/1.5 MB 190.6 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.4/1.5 MB 191.0 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.4/1.5 MB 191.0 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.4/1.5 MB 190.3 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.4/1.5 MB 190.3 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.4/1.5 MB 190.3 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.4/1.5 MB 189.5 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.4/1.5 MB 189.5 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.4/1.5 MB 190.2 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.4/1.5 MB 190.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.5 MB 192.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.5 MB 192.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.5/1.5 MB 192.3 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.5/1.5 MB 192.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 192.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 193.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 193.7 kB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp311-cp311-win_amd64.whl (274 kB)\n",
      "   ---------------------------------------- 0.0/274.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/274.1 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/274.1 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/274.1 kB ? eta -:--:--\n",
      "   ---- ---------------------------------- 30.7/274.1 kB 330.3 kB/s eta 0:00:01\n",
      "   ----- --------------------------------- 41.0/274.1 kB 245.8 kB/s eta 0:00:01\n",
      "   -------- ------------------------------ 61.4/274.1 kB 297.7 kB/s eta 0:00:01\n",
      "   -------- ------------------------------ 61.4/274.1 kB 297.7 kB/s eta 0:00:01\n",
      "   ---------- ---------------------------- 71.7/274.1 kB 231.0 kB/s eta 0:00:01\n",
      "   ------------- ------------------------- 92.2/274.1 kB 262.6 kB/s eta 0:00:01\n",
      "   ------------- ------------------------- 92.2/274.1 kB 262.6 kB/s eta 0:00:01\n",
      "   ------------- ------------------------- 92.2/274.1 kB 262.6 kB/s eta 0:00:01\n",
      "   -------------- ----------------------- 102.4/274.1 kB 227.0 kB/s eta 0:00:01\n",
      "   ----------------- -------------------- 122.9/274.1 kB 240.2 kB/s eta 0:00:01\n",
      "   ----------------- -------------------- 122.9/274.1 kB 240.2 kB/s eta 0:00:01\n",
      "   ------------------ ------------------- 133.1/274.1 kB 224.6 kB/s eta 0:00:01\n",
      "   --------------------- ---------------- 153.6/274.1 kB 229.4 kB/s eta 0:00:01\n",
      "   --------------------- ---------------- 153.6/274.1 kB 229.4 kB/s eta 0:00:01\n",
      "   --------------------- ---------------- 153.6/274.1 kB 229.4 kB/s eta 0:00:01\n",
      "   ------------------------ ------------- 174.1/274.1 kB 227.9 kB/s eta 0:00:01\n",
      "   ------------------------ ------------- 174.1/274.1 kB 227.9 kB/s eta 0:00:01\n",
      "   ------------------------- ------------ 184.3/274.1 kB 218.4 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 204.8/274.1 kB 222.4 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 215.0/274.1 kB 222.1 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 215.0/274.1 kB 222.1 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 235.5/274.1 kB 228.9 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 256.0/274.1 kB 234.7 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 256.0/274.1 kB 234.7 kB/s eta 0:00:01\n",
      "   -------------------------------------- 274.1/274.1 kB 234.5 kB/s eta 0:00:00\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "   ---------------------------------------- 0.0/98.2 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 10.2/98.2 kB ? eta -:--:--\n",
      "   ------------ --------------------------- 30.7/98.2 kB 435.7 kB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 41.0/98.2 kB 393.8 kB/s eta 0:00:01\n",
      "   ------------------------- -------------- 61.4/98.2 kB 469.7 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 81.9/98.2 kB 459.5 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 81.9/98.2 kB 459.5 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 81.9/98.2 kB 459.5 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 81.9/98.2 kB 459.5 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 81.9/98.2 kB 459.5 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 98.2/98.2 kB 216.6 kB/s eta 0:00:00\n",
      "Installing collected packages: regex, click, nltk\n",
      "Successfully installed click-8.1.8 nltk-3.9.1 regex-2024.11.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\pantm\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ID                                              TITLE  \\\n",
      "0          1        Reconstructing Subject-Specific Effect Maps   \n",
      "1          2                 Rotation Invariance Neural Network   \n",
      "2          3  Spherical polyharmonics and Poisson kernels fo...   \n",
      "3          4  A finite element approximation for the stochas...   \n",
      "4          5  Comparative study of Discrete Wavelet Transfor...   \n",
      "...      ...                                                ...   \n",
      "20967  20968  Contemporary machine learning: a guide for pra...   \n",
      "20968  20969  Uniform diamond coatings on WC-Co hard alloy c...   \n",
      "20969  20970  Analysing Soccer Games with Clustering and Con...   \n",
      "20970  20971  On the Efficient Simulation of the Left-Tail o...   \n",
      "20971  20972   Why optional stopping is a problem for Bayesians   \n",
      "\n",
      "                                                ABSTRACT  Computer Science  \\\n",
      "0        Predictive models allow subject-specific inf...                 1   \n",
      "1        Rotation invariance and translation invarian...                 1   \n",
      "2        We introduce and develop the notion of spher...                 0   \n",
      "3        The stochastic Landau--Lifshitz--Gilbert (LL...                 0   \n",
      "4        Fourier-transform infra-red (FTIR) spectra o...                 1   \n",
      "...                                                  ...               ...   \n",
      "20967    Machine learning is finding increasingly bro...                 1   \n",
      "20968    Polycrystalline diamond coatings have been g...                 0   \n",
      "20969    We present a new approach for identifying si...                 1   \n",
      "20970    The sum of Log-normal variates is encountere...                 0   \n",
      "20971    Recently, optional stopping has been a subje...                 0   \n",
      "\n",
      "       Physics  Mathematics  Statistics  Quantitative Biology  \\\n",
      "0            0            0           0                     0   \n",
      "1            0            0           0                     0   \n",
      "2            0            1           0                     0   \n",
      "3            0            1           0                     0   \n",
      "4            0            0           1                     0   \n",
      "...        ...          ...         ...                   ...   \n",
      "20967        1            0           0                     0   \n",
      "20968        1            0           0                     0   \n",
      "20969        0            0           0                     0   \n",
      "20970        0            1           1                     0   \n",
      "20971        0            1           1                     0   \n",
      "\n",
      "       Quantitative Finance  \n",
      "0                         0  \n",
      "1                         0  \n",
      "2                         0  \n",
      "3                         0  \n",
      "4                         0  \n",
      "...                     ...  \n",
      "20967                     0  \n",
      "20968                     0  \n",
      "20969                     0  \n",
      "20970                     0  \n",
      "20971                     0  \n",
      "\n",
      "[20972 rows x 9 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20972 entries, 0 to 20971\n",
      "Data columns (total 9 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   ID                    20972 non-null  int64 \n",
      " 1   TITLE                 20972 non-null  object\n",
      " 2   ABSTRACT              20972 non-null  object\n",
      " 3   Computer Science      20972 non-null  int64 \n",
      " 4   Physics               20972 non-null  int64 \n",
      " 5   Mathematics           20972 non-null  int64 \n",
      " 6   Statistics            20972 non-null  int64 \n",
      " 7   Quantitative Biology  20972 non-null  int64 \n",
      " 8   Quantitative Finance  20972 non-null  int64 \n",
      "dtypes: int64(7), object(2)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel(\"Dataset-1.xlsx\")\n",
    "print(df)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>Computer Science</th>\n",
       "      <th>Physics</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>Statistics</th>\n",
       "      <th>Quantitative Biology</th>\n",
       "      <th>Quantitative Finance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Reconstructing Subject-Specific Effect Maps</td>\n",
       "      <td>Predictive models allow subject-specific inf...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Rotation Invariance Neural Network</td>\n",
       "      <td>Rotation invariance and translation invarian...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
       "      <td>We introduce and develop the notion of spher...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>A finite element approximation for the stochas...</td>\n",
       "      <td>The stochastic Landau--Lifshitz--Gilbert (LL...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
       "      <td>Fourier-transform infra-red (FTIR) spectra o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>On maximizing the fundamental frequency of the...</td>\n",
       "      <td>Let $\\Omega \\subset \\mathbb{R}^n$ be a bound...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>On the rotation period and shape of the hyperb...</td>\n",
       "      <td>We observed the newly discovered hyperbolic ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Adverse effects of polymer coating on heat tra...</td>\n",
       "      <td>The ability of metallic nanoparticles to sup...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>SPH calculations of Mars-scale collisions: the...</td>\n",
       "      <td>We model large-scale ($\\approx$2000km) impac...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>$\\mathcal{R}_{0}$ fails to predict the outbrea...</td>\n",
       "      <td>Time varying susceptibility of host at indiv...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                              TITLE  \\\n",
       "0   1        Reconstructing Subject-Specific Effect Maps   \n",
       "1   2                 Rotation Invariance Neural Network   \n",
       "2   3  Spherical polyharmonics and Poisson kernels fo...   \n",
       "3   4  A finite element approximation for the stochas...   \n",
       "4   5  Comparative study of Discrete Wavelet Transfor...   \n",
       "5   6  On maximizing the fundamental frequency of the...   \n",
       "6   7  On the rotation period and shape of the hyperb...   \n",
       "7   8  Adverse effects of polymer coating on heat tra...   \n",
       "8   9  SPH calculations of Mars-scale collisions: the...   \n",
       "9  10  $\\mathcal{R}_{0}$ fails to predict the outbrea...   \n",
       "\n",
       "                                            ABSTRACT  Computer Science  \\\n",
       "0    Predictive models allow subject-specific inf...                 1   \n",
       "1    Rotation invariance and translation invarian...                 1   \n",
       "2    We introduce and develop the notion of spher...                 0   \n",
       "3    The stochastic Landau--Lifshitz--Gilbert (LL...                 0   \n",
       "4    Fourier-transform infra-red (FTIR) spectra o...                 1   \n",
       "5    Let $\\Omega \\subset \\mathbb{R}^n$ be a bound...                 0   \n",
       "6    We observed the newly discovered hyperbolic ...                 0   \n",
       "7    The ability of metallic nanoparticles to sup...                 0   \n",
       "8    We model large-scale ($\\approx$2000km) impac...                 0   \n",
       "9    Time varying susceptibility of host at indiv...                 0   \n",
       "\n",
       "   Physics  Mathematics  Statistics  Quantitative Biology  \\\n",
       "0        0            0           0                     0   \n",
       "1        0            0           0                     0   \n",
       "2        0            1           0                     0   \n",
       "3        0            1           0                     0   \n",
       "4        0            0           1                     0   \n",
       "5        0            1           0                     0   \n",
       "6        1            0           0                     0   \n",
       "7        1            0           0                     0   \n",
       "8        1            0           0                     0   \n",
       "9        0            0           0                     1   \n",
       "\n",
       "   Quantitative Finance  \n",
       "0                     0  \n",
       "1                     0  \n",
       "2                     0  \n",
       "3                     0  \n",
       "4                     0  \n",
       "5                     0  \n",
       "6                     0  \n",
       "7                     0  \n",
       "8                     0  \n",
       "9                     0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Data distribution\n",
    "## which graphs or plots better suggests the data distributions? -> histogram, boxplots\n",
    "# import matplotlib.pyplot as plt \n",
    "# plt.bar(X[\"Title\"],y)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.87      0.82      1692\n",
      "           1       0.93      0.81      0.87      1226\n",
      "           2       0.82      0.76      0.79      1150\n",
      "           3       0.73      0.79      0.76      1069\n",
      "           4       0.67      0.18      0.28       122\n",
      "           5       0.86      0.27      0.41        45\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      5304\n",
      "   macro avg       0.80      0.61      0.65      5304\n",
      "weighted avg       0.81      0.80      0.80      5304\n",
      " samples avg       0.80      0.82      0.79      5304\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "## train-test split\n",
    "\n",
    "# X -> TITLE, ABSTRACT and Y -> labels(Phy, Maths, Stats, Quantitative Biology, Quant Finance)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix,ConfusionMatrixDisplay\n",
    "\n",
    "df['text'] = df['TITLE'] + ' ' + df['ABSTRACT']\n",
    "X = df['text']\n",
    "y = df[['Computer Science','Physics','Mathematics','Statistics','Quantitative Biology','Quantitative Finance']]\n",
    "\n",
    "\n",
    "# feature engineering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "X_train, X_test, y_train,y_test = train_test_split(X_tfidf,y,test_size=0.2,random_state=42)\n",
    "\n",
    "# why have we chosen the MultinomialNB \n",
    "nb = MultinomialNB(alpha=0.1)\n",
    "multi_target_nb = MultiOutputClassifier(nb)\n",
    "multi_target_nb.fit(X_train,y_train)\n",
    "\n",
    "y_pred = multi_target_nb.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Computer Science</th>\n",
       "      <th>Physics</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>Statistics</th>\n",
       "      <th>Quantitative Biology</th>\n",
       "      <th>Quantitative Finance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20967</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20968</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20969</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20970</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20971</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20972 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Computer Science  Physics  Mathematics  Statistics  \\\n",
       "0                     1        0            0           0   \n",
       "1                     1        0            0           0   \n",
       "2                     0        0            1           0   \n",
       "3                     0        0            1           0   \n",
       "4                     1        0            0           1   \n",
       "...                 ...      ...          ...         ...   \n",
       "20967                 1        1            0           0   \n",
       "20968                 0        1            0           0   \n",
       "20969                 1        0            0           0   \n",
       "20970                 0        0            1           1   \n",
       "20971                 0        0            1           1   \n",
       "\n",
       "       Quantitative Biology  Quantitative Finance  \n",
       "0                         0                     0  \n",
       "1                         0                     0  \n",
       "2                         0                     0  \n",
       "3                         0                     0  \n",
       "4                         0                     0  \n",
       "...                     ...                   ...  \n",
       "20967                     0                     0  \n",
       "20968                     0                     0  \n",
       "20969                     0                     0  \n",
       "20970                     0                     0  \n",
       "20971                     0                     0  \n",
       "\n",
       "[20972 rows x 6 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model building and training\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# based on my data distribution how do I decide which classifier to choose from naive_bayes scikit library\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1237: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, binary problems will be fit as proper binary  logistic regression models (as if multi_class='ovr' were set). Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1237: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, binary problems will be fit as proper binary  logistic regression models (as if multi_class='ovr' were set). Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1237: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, binary problems will be fit as proper binary  logistic regression models (as if multi_class='ovr' were set). Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1237: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, binary problems will be fit as proper binary  logistic regression models (as if multi_class='ovr' were set). Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1237: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, binary problems will be fit as proper binary  logistic regression models (as if multi_class='ovr' were set). Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1237: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, binary problems will be fit as proper binary  logistic regression models (as if multi_class='ovr' were set). Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is:  0.6522050059594756\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "def data_preprocessing(X,y):\n",
    "    # utilize the logistic regression for the multi-label classification problem. \n",
    "    # preprocess the text data using techniques like TF-IDF or word embeddings to convert them into numerical features.\n",
    "    vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "    X_tfidf = vectorizer.fit_transform(X)\n",
    "    X_train, X_test, y_train,y_test = train_test_split(X_tfidf,y,test_size=0.2,random_state=42)\n",
    "    # which one to chose and when for example b/w the TFIDF one and the word embeddings\n",
    "    base_model = LogisticRegression(multi_class='multinomial',solver='lbfgs',max_iter=1000,C=1.0)\n",
    "    multi_model = MultiOutputClassifier(base_model)\n",
    "    multi_model.fit(X_train,y_train)\n",
    "    y_pred = multi_model.predict(X_test)\n",
    "    print(\"Accuracy is: \",accuracy_score(y_test,y_pred))\n",
    "    \n",
    "data_preprocessing(X,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X = df['TITLE'] + ' ' + df['ABSTRACT']\n",
    "y = df[['Computer Science','Physics','Mathematics','Statistics','Quantitative Biology','Quantitative Finance']]\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "X_train, X_test, y_train,y_test = train_test_split(X_tfidf,y,test_size=0.2,random_state=42)\n",
    "\n",
    "# what is the kernel in SVM \n",
    "svm = SVC(kernel='linear',probability=True)\n",
    "multi_svm = MultiOutputClassifier(svm)\n",
    "multi_svm.fit(X_train,y_train)\n",
    "y_pred = multi_svm.predict(X_train)\n",
    "accuracy = accuracy_score(y_pred,y_test)\n",
    "print(\" accuracy score is \", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "- what is RBF kernel in svm : fits linear model in transformed space\n",
    "- what is the penalty in logistic regression\n",
    "\n",
    "\n",
    "## MLP Implementation for the classification task\n",
    "\n",
    "### questions\n",
    "  - how do the input size decided ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.68791136\n",
      "Iteration 2, loss = 1.33266176\n",
      "Iteration 3, loss = 0.96995388\n",
      "Iteration 4, loss = 0.82904822\n",
      "Iteration 5, loss = 0.71221854\n",
      "Iteration 6, loss = 0.60149460\n",
      "Iteration 7, loss = 0.48855522\n",
      "Iteration 8, loss = 0.37332579\n",
      "Iteration 9, loss = 0.26664911\n",
      "Iteration 10, loss = 0.17548167\n",
      "Iteration 11, loss = 0.10798356\n",
      "Iteration 12, loss = 0.06522094\n",
      "Iteration 13, loss = 0.03973699\n",
      "Iteration 14, loss = 0.02578171\n",
      "Iteration 15, loss = 0.01789852\n",
      "Iteration 16, loss = 0.01312397\n",
      "Iteration 17, loss = 0.01018717\n",
      "Iteration 18, loss = 0.00817570\n",
      "Iteration 19, loss = 0.00670836\n",
      "Iteration 20, loss = 0.00563122\n",
      "Iteration 21, loss = 0.00478494\n",
      "Iteration 22, loss = 0.00412408\n",
      "Iteration 23, loss = 0.00362020\n",
      "Iteration 24, loss = 0.00323461\n",
      "Iteration 25, loss = 0.00292752\n",
      "Iteration 26, loss = 0.00268365\n",
      "Iteration 27, loss = 0.00248500\n",
      "Iteration 28, loss = 0.00231987\n",
      "Iteration 29, loss = 0.00217825\n",
      "Iteration 30, loss = 0.00205868\n",
      "Iteration 31, loss = 0.00195466\n",
      "Iteration 32, loss = 0.00186548\n",
      "Iteration 33, loss = 0.00178905\n",
      "Iteration 34, loss = 0.00172187\n",
      "Iteration 35, loss = 0.00166268\n",
      "Iteration 36, loss = 0.00161136\n",
      "Iteration 37, loss = 0.00156535\n",
      "Iteration 38, loss = 0.00152515\n",
      "Iteration 39, loss = 0.00148891\n",
      "Iteration 40, loss = 0.00145679\n",
      "Iteration 41, loss = 0.00142794\n",
      "Iteration 42, loss = 0.00140153\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "    Computer Science       0.79      0.80      0.79      1692\n",
      "             Physics       0.85      0.85      0.85      1226\n",
      "         Mathematics       0.79      0.78      0.78      1150\n",
      "          Statistics       0.72      0.69      0.71      1069\n",
      "Quantitative Biology       0.58      0.34      0.42       122\n",
      "Quantitative Finance       0.78      0.40      0.53        45\n",
      "\n",
      "           micro avg       0.79      0.77      0.78      5304\n",
      "           macro avg       0.75      0.64      0.68      5304\n",
      "        weighted avg       0.79      0.77      0.78      5304\n",
      "         samples avg       0.80      0.81      0.78      5304\n",
      "\n",
      "Overall Accuracy: 0.6005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pantm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "# Define the MLP model\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64, 32),  # Three hidden layers\n",
    "    activation='relu',                 # ReLU activation function\n",
    "    solver='adam',                     # Adam optimizer\n",
    "    alpha=0.0001,                      # L2 regularization parameter\n",
    "    batch_size='auto',                 # Automatic batch size\n",
    "    learning_rate='adaptive',          # Adaptive learning rate\n",
    "    max_iter=1000,                     # Maximum number of iterations\n",
    "    random_state=42,                   # For reproducibility\n",
    "    verbose=True                       # Print progress\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "mlp.fit(X_train, y_train)\n",
    "# Make predictions\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Round predictions to 0 or 1\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Print performance metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_binary, target_names=[\n",
    "    'Computer Science', 'Physics', 'Mathematics', 'Statistics', \n",
    "    'Quantitative Biology', 'Quantitative Finance'\n",
    "]))\n",
    "\n",
    "# Calculate overall accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "print(f\"Overall Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question2: Contains text from certain domains and you have to predict the domain.\n",
    "\n",
    "- i shall be using dataprocessing\n",
    "- tf-idf followed by the naive bayes classification and \n",
    "- word embeddings using the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ID                                            Content         Domain\n",
      "0        1  engali Binodiini Ekti Natir Putul Chaalchitro ...  Entertainment\n",
      "1        2   ChiefsAholic A Wolf In Chiefs Clothing articl...  Entertainment\n",
      "2        3  Kabandha Your Rating Write a review Optional C...  Entertainment\n",
      "3        4  In Bruges 2008 R 1h 47m IMDb RATING 79 10 474K...  Entertainment\n",
      "4        5  Men in Black 2012 PG13 1h 46m IMDb RATING 68 1...  Entertainment\n",
      "...    ...                                                ...            ...\n",
      "3922  3923   Kerala with its Munnar Wayanad Kochi Alleppey...        Tourism\n",
      "3923  3924   Netaji Subhash Chandra Bose Airport CCU Kolka...        Tourism\n",
      "3924  3925  Spanning over 800 square kilometres in the Alw...        Tourism\n",
      "3925  3926  Located in the Eastern part of India West Beng...        Tourism\n",
      "3926  3927   Maharaja Bir Bikram Airport IXA Agartala Agar...        Tourism\n",
      "\n",
      "[3927 rows x 3 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3927 entries, 0 to 3926\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   ID       3927 non-null   int64 \n",
      " 1   Content  3927 non-null   object\n",
      " 2   Domain   3927 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 92.2+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_a2d2 = pd.read_excel(\"A2D2.xlsx\")\n",
    "print(df_a2d2)\n",
    "df_a2d2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform data preprocessing stemming and lemmitization\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import nltk\n",
    "# Ensure required resources are downloaded\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Content</th>\n",
       "      <th>Domain</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>engali Binodiini Ekti Natir Putul Chaalchitro ...</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>engalibinodiiniektinatirputulchaalchitroframek...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ChiefsAholic A Wolf In Chiefs Clothing articl...</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>chiefsaholicwolfchiefclothingarticleshowcmsclo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Kabandha Your Rating Write a review Optional C...</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>kabandharatingwritereviewoptionalcharacterrema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>In Bruges 2008 R 1h 47m IMDb RATING 79 10 474K...</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>brugesrhimdbratingkratingratepopularityplaytra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Men in Black 2012 PG13 1h 46m IMDb RATING 68 1...</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>menblackpghimdbratingkratingratepopularityplay...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                            Content         Domain  \\\n",
       "0   1  engali Binodiini Ekti Natir Putul Chaalchitro ...  Entertainment   \n",
       "1   2   ChiefsAholic A Wolf In Chiefs Clothing articl...  Entertainment   \n",
       "2   3  Kabandha Your Rating Write a review Optional C...  Entertainment   \n",
       "3   4  In Bruges 2008 R 1h 47m IMDb RATING 79 10 474K...  Entertainment   \n",
       "4   5  Men in Black 2012 PG13 1h 46m IMDb RATING 68 1...  Entertainment   \n",
       "\n",
       "                                      processed_text  \n",
       "0  engalibinodiiniektinatirputulchaalchitroframek...  \n",
       "1  chiefsaholicwolfchiefclothingarticleshowcmsclo...  \n",
       "2  kabandharatingwritereviewoptionalcharacterrema...  \n",
       "3  brugesrhimdbratingkratingratepopularityplaytra...  \n",
       "4  menblackpghimdbratingkratingratepopularityplay...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    #text = re.sub(r'^a-zA-Z\\s','',text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = word_tokenize(text) # internally how it works show with an example.\n",
    "    #remove stopwords\n",
    "    tokens=[word for word in tokens if word not in stopwords.words(\"english\")]\n",
    "    # lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ''.join(tokens)\n",
    "\n",
    "df_a2d2['processed_text'] = df_a2d2['Content'].apply(preprocess_text)\n",
    "df_a2d2.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID                                            Content         Domain  \\\n",
      "0   1  engali Binodini Ekti Natir Putul Chaalchitro f...  Entertainment   \n",
      "1   2  ChiefsAholic A Wolf In Chiefs Clothing article...  Entertainment   \n",
      "2   3  Kabandha Your Rating Write a review Optional C...  Entertainment   \n",
      "3   4  In Bruges 2008 R 1h 47m IMDb RATING 79 10 474K...  Entertainment   \n",
      "4   5  Men in Black 2012 PG13 1h 46m IMDb RATING 68 1...  Entertainment   \n",
      "\n",
      "                                      processed_text  \n",
      "0  engali binodini ekti natir putul chaalchitro f...  \n",
      "1      chiefsaholic wolf chief clothing article show  \n",
      "2         kabandha rating write review optional char  \n",
      "3                           bruges r h imdb rating k  \n",
      "4                         men black pg h imdb rating  \n"
     ]
    }
   ],
   "source": [
    "#trying the same code above after stemming \n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    # Lemmatize tokens\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    # Join tokens back into a single string with spaces\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Example dataset (replace with your actual dataset)\n",
    "data = pd.DataFrame({\n",
    "    'ID': [1, 2, 3, 4, 5],\n",
    "    'Content': [\n",
    "        \"engali Binodini Ekti Natir Putul Chaalchitro frame...\",\n",
    "        \"ChiefsAholic A Wolf In Chiefs Clothing article show...\",\n",
    "        \"Kabandha Your Rating Write a review Optional Char...\",\n",
    "        \"In Bruges 2008 R 1h 47m IMDb RATING 79 10 474K...\",\n",
    "        \"Men in Black 2012 PG13 1h 46m IMDb RATING 68 1...\"\n",
    "    ],\n",
    "    'Domain': [\"Entertainment\", \"Entertainment\", \"Entertainment\", \"Entertainment\", \"Entertainment\"]\n",
    "})\n",
    "\n",
    "# Apply preprocessing to the 'Content' column\n",
    "data['processed_text'] = data['Content'].apply(preprocess_text)\n",
    "\n",
    "# Display processed data\n",
    "print(data[['ID', 'Content', 'Domain', 'processed_text']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

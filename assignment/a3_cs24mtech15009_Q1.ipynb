{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "1. Word Representation using Word2Vec: Implement the skip-gram model with negative\n",
    "sampling loss function for word embedding generation. Your implementation should include: [20\n",
    " Marks]\n",
    " (b) Implement the skip-gram model from scratch with negative sampling loss. [4]\n",
    " (c) Derive and implement the gradients for backpropagation. [4]\n",
    " (d) Train your model on the text8 dataset with appropriate hyperparameters (specify your choices\n",
    " and justify them). [3]\n",
    " (e) Evaluate the quality of your embeddings through: [4]\n",
    " • Visualization using SVD to project the embeddings to 2D space.\n",
    " • Word similarity analysis for semantically related words (e.g., “king”- “man” + “woman”\n",
    " ≈ “queen”).\n",
    " (f) Discuss the impact of key hyperparameters (e.g., embedding dimension, context window size,\n",
    " number of negative samples) on the quality of the learned representations. [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Text8Dataset(Dataset):\n",
    "  def __init__(self,text,window_size=2, num_negative_samples=5):\n",
    "    self.window_size = window_size\n",
    "    self.num_negative_samples = num_negative_samples\n",
    "    self.word_counts = None\n",
    "    self.vocab_to_int = None\n",
    "    self.int_to_vocab = None\n",
    "    self.word_indices = None\n",
    "    self.skip_gram_pairs = []\n",
    "    self.sampling_weights = None\n",
    "\n",
    "    # Process the data\n",
    "    trimmed_words = self.preprocess_data(text)\n",
    "    self.create_lookup_tables(trimmed_words)\n",
    "    self.generate_skip_gram()\n",
    "    self.unigram_distribution()\n",
    "\n",
    "  def preprocess_data(self,text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' <PERIOD> ')\n",
    "    text = text.replace(',', ' <COMMA> ')\n",
    "    text = text.replace('\"', ' <QUOTATION_MARK> ')\n",
    "    text = text.replace(';', ' <SEMICOLON> ')\n",
    "    text = text.replace('!', ' <EXCLAMATION_MARK> ')\n",
    "    text = text.replace('?', ' <QUESTION_MARK> ')\n",
    "    text = text.replace('(', ' <LEFT_PAREN> ')\n",
    "    text = text.replace(')', ' <RIGHT_PAREN> ')\n",
    "    text = text.replace('--', ' <HYPHENS> ')\n",
    "    text = text.replace('?', ' <QUESTION_MARK> ')\n",
    "    text = text.replace(':', ' <COLON> ')\n",
    "    words = text.split()\n",
    "    self.word_counts = Counter(words)\n",
    "    trimmed_words = [word for word, count in self.word_counts.items() if count >= 5]\n",
    "    return trimmed_words\n",
    "\n",
    "  def create_lookup_tables(self,trimmed_words):\n",
    "    #sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    self.vocab = set(trimmed_words)\n",
    "    self.vocab_to_int = {word: ii for ii, word in enumerate(self.vocab)}\n",
    "    self.int_to_vocab = {ii: word for word, ii in self.vocab_to_int.items()}\n",
    "    self.word_indices = [self.vocab_to_int[word] for word in trimmed_words]\n",
    "    self.vocab_size= len(self.vocab)\n",
    "\n",
    "  def generate_skip_gram(self):\n",
    "      for i in range(len(self.word_indices)):\n",
    "          center_word = self.word_indices[i]\n",
    "          # Get context words within window\n",
    "          for j in range(max(0, i - self.window_size),\n",
    "                        min(len(self.word_indices), i + self.window_size + 1)):\n",
    "              if i != j:\n",
    "                  context_word = self.word_indices[j]\n",
    "                  self.skip_gram_pairs.append((center_word, context_word))\n",
    "\n",
    "  def unigram_distribution(self):\n",
    "    word_freqs = np.array([count for word, count in self.word_counts.most_common() if word in self.vocab_to_int])\n",
    "    word_freqs = word_freqs ** 0.75\n",
    "    self.sampling_weights = word_freqs / np.sum(word_freqs)\n",
    "\n",
    "\n",
    "  def subsampling_words(self,threshold=1e-5):\n",
    "    total_count = sum(self.word_counts.values())\n",
    "    word_freqs = {word: count/total_count for word, count in self.word_counts.items()}\n",
    "    p_drop = {word: 1 - np.sqrt(threshold/word_freqs[word]) for word in self.word_counts}\n",
    "    train_words = [word for word in self.word_counts if random.random() < (1 - p_drop[word])]\n",
    "    return train_words\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.skip_gram_pairs)\n",
    "\n",
    "  def __getitem__(self,idx):\n",
    "    center_word, context_word = self.skip_gram_pairs[idx]\n",
    "    self.sampling_weights_tensor = torch.from_numpy(self.sampling_weights).float()\n",
    "    neg_samples = torch.multinomial(\n",
    "        self.sampling_weights_tensor,\n",
    "        self.num_negative_samples,\n",
    "        replacement=True\n",
    "    ).numpy()\n",
    "    return (\n",
    "        torch.tensor(center_word, dtype=torch.long),\n",
    "        torch.tensor(context_word, dtype=torch.long),\n",
    "        torch.tensor(neg_samples, dtype=torch.long)\n",
    "      )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n",
      "Vocab size is: 285154\n",
      "subsample vocab size 250775\n"
     ]
    }
   ],
   "source": [
    "def data_preprocessing(file_path):\n",
    "    with open(file_path,'r') as file:\n",
    "        data=file.read()\n",
    "        print(\"read data\")\n",
    "    return data\n",
    "\n",
    "file_path = \"C:/users/pantm/Downloads/text8/text8.txt\"\n",
    "data = data_preprocessing(file_path)\n",
    "dataset = Text8Dataset(data)\n",
    "print(\"Vocab size is:\", len(dataset))\n",
    "# datasetiterator = iter(dataset)\n",
    "# center_word, context_word, neg_samples = next(datasetiterator)\n",
    "\n",
    "train_words = dataset.subsampling_words()\n",
    "print(\"subsample vocab size\",len(train_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SkipGram:\n",
    "    def __init__(self, vocab_size, embedding_dim, num_negative_samples=5, momentum=0.9,eta=0.01):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_negative_samples = num_negative_samples\n",
    "        self.momentum = momentum\n",
    "        self.train_losses=[]\n",
    "        self.test_losses=[]\n",
    "\n",
    "        # Initialize embeddings\n",
    "        self.W1 = torch.randn(vocab_size, embedding_dim) * 0.1\n",
    "        self.W2 = torch.randn(vocab_size, embedding_dim) * 0.1\n",
    "        \n",
    "        # init_range = np.sqrt(6.0 / (vocab_size + embedding_dim))\n",
    "        # self.W1= torch.FloatTensor(vocab_size, embedding_dim).uniform_(-init_range, init_range)\n",
    "        # self.W2 = torch.FloatTensor(vocab_size, embedding_dim).uniform_(-init_range, init_range)\n",
    "\n",
    "\n",
    "        self.eta = eta\n",
    "\n",
    "        # Initialize velocity terms for momentum\n",
    "        self.v_W1 = torch.zeros_like(self.W1)\n",
    "        self.v_W2 = torch.zeros_like(self.W2)\n",
    "\n",
    "    def forward(self, target_word, context_word, negative_samples):\n",
    "        target_emb = self.W1[target_word]\n",
    "        context_emb = self.W2[context_word]\n",
    "        neg_emb = self.W2[negative_samples]\n",
    "\n",
    "        pos_score = torch.sum(target_emb * context_emb, dim=1)\n",
    "        pos_loss = -F.logsigmoid(pos_score)\n",
    "\n",
    "        neg_score = torch.bmm(neg_emb, target_emb.unsqueeze(2)).squeeze()\n",
    "        neg_loss = -torch.sum(F.logsigmoid(-neg_score), dim=1)\n",
    "\n",
    "        return pos_loss + neg_loss, target_emb, context_emb, neg_emb\n",
    "\n",
    "    def compute_test_loss(self, test_dataloader):\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for target, context, neg_samples in test_dataloader:\n",
    "                loss, _, _, _ = self.forward(target, context, neg_samples)\n",
    "                total_loss += loss.mean().item()\n",
    "        return total_loss / len(test_dataloader)\n",
    "\n",
    "\n",
    "    def gradient_descent(self, train_dataloader,test_dataloader=None, n_epoch=10,early_stopping=5):\n",
    "        best_loss = float('inf')\n",
    "        no_improve = 0\n",
    "        for epoch in range(n_epoch):\n",
    "            train_loss=0\n",
    "            for target_word, context_word, negative_samples in train_dataloader:\n",
    "                loss, target_emb, context_emb, neg_emb = self.forward(target_word, context_word, negative_samples)\n",
    "                train_loss += loss.mean().item()\n",
    "\n",
    "                # Gradients for positive samples\n",
    "                d_pos = -torch.sigmoid(-torch.sum(target_emb * context_emb, dim=1)).unsqueeze(1) * context_emb\n",
    "                d_context = -torch.sigmoid(-torch.sum(target_emb * context_emb, dim=1)).unsqueeze(1) * target_emb\n",
    "\n",
    "                # Gradients for negative samples\n",
    "                d_neg = torch.sigmoid(torch.bmm(neg_emb, target_emb.unsqueeze(2)).squeeze()).unsqueeze(2) * neg_emb\n",
    "                d_target_neg = torch.sum(torch.sigmoid(torch.bmm(neg_emb, target_emb.unsqueeze(2)).squeeze()).unsqueeze(2) * neg_emb, dim=1)\n",
    "\n",
    "                # Compute momentum updates\n",
    "                self.v_W1[target_word] = self.momentum * self.v_W1[target_word] - self.eta * (d_pos + d_target_neg)\n",
    "                self.v_W2[context_word] = self.momentum * self.v_W2[context_word] - self.eta * d_context\n",
    "                self.v_W2[negative_samples] = self.momentum * self.v_W2[negative_samples] - self.eta * d_neg\n",
    "\n",
    "                # Apply updates\n",
    "                self.W1[target_word] += self.v_W1[target_word]\n",
    "                self.W2[context_word] += self.v_W2[context_word]\n",
    "                self.W2[negative_samples] += self.v_W2[negative_samples]\n",
    "            \n",
    "            avg_train_loss = train_loss / len(train_dataloader)\n",
    "            self.train_losses.append(avg_train_loss)\n",
    "            \n",
    "            if test_dataloader:\n",
    "                avg_test_loss = self.compute_test_loss(test_dataloader)\n",
    "                self.test_losses.append(avg_test_loss)\n",
    "                print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}\")\n",
    "                \n",
    "                # Early stopping\n",
    "                if avg_test_loss < best_loss:\n",
    "                    best_loss = avg_test_loss\n",
    "                    no_improve = 0\n",
    "                else:\n",
    "                    no_improve += 1\n",
    "                    if no_improve >= early_stopping:\n",
    "                        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                        break\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    def get_word_embedding(self, word_idx):\n",
    "        return self.W1[word_idx].detach().numpy()\n",
    "    \n",
    "    def plot_losses(self):\n",
    "        \"\"\"Plot training and test loss curves\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.train_losses, label='Training Loss')\n",
    "        if self.test_losses:\n",
    "            plt.plot(self.test_losses, label='Test Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Test Loss Over Epochs')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size is: 10713\n",
      "Epoch 1, Train Loss: 4.1753\n",
      "Epoch 2, Train Loss: 4.1597\n",
      "Epoch 3, Train Loss: 4.1460\n",
      "Epoch 4, Train Loss: 4.1335\n",
      "Epoch 5, Train Loss: 4.1253\n",
      "Epoch 6, Train Loss: 4.1161\n",
      "Epoch 7, Train Loss: 4.1079\n",
      "Epoch 8, Train Loss: 4.1015\n",
      "Epoch 9, Train Loss: 4.0953\n",
      "Epoch 10, Train Loss: 4.0893\n",
      "Epoch 11, Train Loss: 4.0846\n",
      "Epoch 12, Train Loss: 4.0801\n",
      "Epoch 13, Train Loss: 4.0758\n",
      "Epoch 14, Train Loss: 4.0725\n",
      "Epoch 15, Train Loss: 4.0692\n",
      "Epoch 16, Train Loss: 4.0651\n",
      "Epoch 17, Train Loss: 4.0623\n",
      "Epoch 18, Train Loss: 4.0602\n",
      "Epoch 19, Train Loss: 4.0576\n",
      "Epoch 20, Train Loss: 4.0544\n",
      "Epoch 21, Train Loss: 4.0525\n",
      "Epoch 22, Train Loss: 4.0505\n",
      "Epoch 23, Train Loss: 4.0487\n",
      "Epoch 24, Train Loss: 4.0467\n",
      "Epoch 25, Train Loss: 4.0449\n",
      "Epoch 26, Train Loss: 4.0434\n",
      "Epoch 27, Train Loss: 4.0418\n",
      "Epoch 28, Train Loss: 4.0401\n",
      "Epoch 29, Train Loss: 4.0388\n",
      "Epoch 30, Train Loss: 4.0370\n",
      "Epoch 31, Train Loss: 4.0358\n",
      "Epoch 32, Train Loss: 4.0346\n",
      "Epoch 33, Train Loss: 4.0333\n",
      "Epoch 34, Train Loss: 4.0319\n",
      "Epoch 35, Train Loss: 4.0310\n",
      "Epoch 36, Train Loss: 4.0299\n",
      "Epoch 37, Train Loss: 4.0287\n",
      "Epoch 38, Train Loss: 4.0276\n",
      "Epoch 39, Train Loss: 4.0265\n",
      "Epoch 40, Train Loss: 4.0257\n",
      "Epoch 41, Train Loss: 4.0245\n",
      "Epoch 42, Train Loss: 4.0235\n",
      "Epoch 43, Train Loss: 4.0228\n",
      "Epoch 44, Train Loss: 4.0219\n",
      "Epoch 45, Train Loss: 4.0210\n",
      "Epoch 46, Train Loss: 4.0201\n",
      "Epoch 47, Train Loss: 4.0190\n",
      "Epoch 48, Train Loss: 4.0186\n",
      "Epoch 49, Train Loss: 4.0176\n",
      "Epoch 50, Train Loss: 4.0166\n",
      "Epoch 51, Train Loss: 4.0157\n",
      "Epoch 52, Train Loss: 4.0151\n",
      "Epoch 53, Train Loss: 4.0141\n",
      "Epoch 54, Train Loss: 4.0133\n",
      "Epoch 55, Train Loss: 4.0126\n",
      "Epoch 56, Train Loss: 4.0116\n",
      "Epoch 57, Train Loss: 4.0108\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 92\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     text \u001b[38;5;241m=\u001b[39m data[:\u001b[38;5;241m4000000\u001b[39m]\n\u001b[1;32m---> 92\u001b[0m     model, vocab, idx_to_word \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_skip_gram_with_negative_sampling\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\n\u001b[0;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;66;03m# test_words = ['king', 'queen', 'man', 'woman', 'violent', 'word', 'term', 'describe', 'philosophy', 'chief', 'anarchists']\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;66;03m# # for each_word in test_words:\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;66;03m# #     similar_words = find_similar_words(each_word, model, vocab, idx_to_word)\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;66;03m# #     print(f\"Words similar to '{each_word}': {similar_words}\")\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     word_similarity_analysis(model\u001b[38;5;241m.\u001b[39mW2, vocab, idx_to_word)\n",
      "Cell \u001b[1;32mIn[17], line 8\u001b[0m, in \u001b[0;36mtrain_skip_gram_with_negative_sampling\u001b[1;34m(text, embedding_dim, window_size, num_negative_samples, batch_size, epochs, eta)\u001b[0m\n\u001b[0;32m      5\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m SkipGram(dataset\u001b[38;5;241m.\u001b[39mvocab_size, embedding_dim, num_negative_samples,eta)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, dataset\u001b[38;5;241m.\u001b[39mvocab_to_int, dataset\u001b[38;5;241m.\u001b[39mint_to_vocab\n",
      "Cell \u001b[1;32mIn[16], line 54\u001b[0m, in \u001b[0;36mSkipGram.gradient_descent\u001b[1;34m(self, train_dataloader, test_dataloader, n_epoch, early_stopping)\u001b[0m\n\u001b[0;32m     52\u001b[0m train_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target_word, context_word, negative_samples \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m---> 54\u001b[0m     loss, target_emb, context_emb, neg_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_word\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_word\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# Gradients for positive samples\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 31\u001b[0m, in \u001b[0;36mSkipGram.forward\u001b[1;34m(self, target_word, context_word, negative_samples)\u001b[0m\n\u001b[0;32m     28\u001b[0m context_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW2[context_word]\n\u001b[0;32m     29\u001b[0m neg_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW2[negative_samples]\n\u001b[1;32m---> 31\u001b[0m pos_score \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_emb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcontext_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m pos_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mF\u001b[38;5;241m.\u001b[39mlogsigmoid(pos_score)\n\u001b[0;32m     34\u001b[0m neg_score \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(neg_emb, target_emb\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_skip_gram_with_negative_sampling(text, embedding_dim=100, window_size=2,\n",
    "                                          num_negative_samples=5, batch_size=32, epochs=5,eta=0.01):\n",
    "    dataset = Text8Dataset(text, window_size, num_negative_samples)\n",
    "    print(\"Vocab size is:\", dataset.vocab_size)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = SkipGram(dataset.vocab_size, embedding_dim, num_negative_samples,eta)\n",
    "    model.gradient_descent(dataloader, n_epoch=epochs)\n",
    "\n",
    "    return model, dataset.vocab_to_int, dataset.int_to_vocab\n",
    "\n",
    "# def find_similar_words(word, model, vocab_to_int, idx_to_word, top_k=5):\n",
    "#     if word not in vocab:\n",
    "#         return []\n",
    "\n",
    "#     word_idx = vocab_to_int[word]\n",
    "#     word_vector = model.get_word_embedding(word_idx)\n",
    "\n",
    "#     similarities = []\n",
    "#     for idx in range(len(vocab)):\n",
    "#         if idx != word_idx:\n",
    "#             vector = model.get_word_embedding(idx)\n",
    "#             similarity = np.dot(word_vector, vector) / (np.linalg.norm(word_vector) * np.linalg.norm(vector))\n",
    "#             similarities.append((idx_to_word[idx], similarity))\n",
    "\n",
    "#     similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "#     return similarities[:top_k]\n",
    "\n",
    "\n",
    "def word_similarity_analysis(embeddings, vocab, idx_to_word, test_words=None):\n",
    "\n",
    "    def cosine_similarity(vec1, vec2):\n",
    "        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "    \n",
    "    def find_most_similar(target_word, top_k=5):\n",
    "        \"\"\"Find top_k most similar words to target_word\"\"\"\n",
    "        if target_word not in vocab:\n",
    "            return []\n",
    "            \n",
    "        target_vec = embeddings[vocab[target_word]]\n",
    "        similarities = []\n",
    "        \n",
    "        for word, idx in vocab.items():\n",
    "            if word != target_word:\n",
    "                sim = cosine_similarity(target_vec, embeddings[idx])\n",
    "                similarities.append((word, sim))\n",
    "        \n",
    "        return sorted(similarities, key=lambda x: -x[1])[:top_k]\n",
    "    \n",
    "    def solve_analogy(a, b, c, top_k=5):\n",
    "        if not all(w in vocab for w in [a, b, c]):\n",
    "            return []\n",
    "            \n",
    "        vec = embeddings[vocab[b]] - embeddings[vocab[a]] + embeddings[vocab[c]]\n",
    "        similarities = []\n",
    "        \n",
    "        for word, idx in vocab.items():\n",
    "            if word not in [a, b, c]:\n",
    "                sim = cosine_similarity(vec, embeddings[idx])\n",
    "                similarities.append((word, sim))\n",
    "        \n",
    "        return sorted(similarities, key=lambda x: -x[1])[:top_k]\n",
    "    \n",
    "\n",
    "    if test_words is None:\n",
    "        test_words = ['king', 'queen', 'man', 'woman', 'paris', 'france', 'london', 'england']\n",
    "    \n",
    "    print(\"\\nWord Similarity Analysis:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 1. Show similar words\n",
    "    print(\"\\nMost similar words:\")\n",
    "    for word in test_words[:4]:  \n",
    "        similar = find_most_similar(word)\n",
    "        print(f\"{word}: {[w for w, _ in similar]}\")\n",
    "    \n",
    "    # 2. Test analogies\n",
    "    print(\"\\nWord analogies:\")\n",
    "    analogies = [\n",
    "        ('king', 'man', 'woman'),\n",
    "        ('france', 'paris', 'london')\n",
    "    ]\n",
    "    \n",
    "    for a, b, c in analogies:\n",
    "        result = solve_analogy(a, b, c)\n",
    "        print(f\"{a} - {b} + {c} ≈ {result[0][0]} (similarity: {result[0][1]:.2f})\")\n",
    "        print(f\"Other candidates: {[w for w, _ in result[1:3]]}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    text = data[:4000000]\n",
    "    model, vocab, idx_to_word = train_skip_gram_with_negative_sampling(\n",
    "        text, embedding_dim=300, epochs=300, batch_size=128,eta=0.01\n",
    "    )\n",
    "\n",
    "    # test_words = ['king', 'queen', 'man', 'woman', 'violent', 'word', 'term', 'describe', 'philosophy', 'chief', 'anarchists']\n",
    "    # # for each_word in test_words:\n",
    "    # #     similar_words = find_similar_words(each_word, model, vocab, idx_to_word)\n",
    "    # #     print(f\"Words similar to '{each_word}': {similar_words}\")\n",
    "    \n",
    "    word_similarity_analysis(model.W2, vocab, idx_to_word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size is: 10713\n",
      "Epoch 1, Train Loss: 4.1589\n",
      "Epoch 2, Train Loss: 4.1587\n",
      "Epoch 3, Train Loss: 4.1584\n",
      "Epoch 4, Train Loss: 4.1583\n",
      "Epoch 5, Train Loss: 4.1581\n",
      "Epoch 6, Train Loss: 4.1579\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      2\u001b[0m     text \u001b[38;5;241m=\u001b[39m data[:\u001b[38;5;241m4000000\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m     model, vocab, idx_to_word \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_skip_gram_with_negative_sampling\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# test_words = ['king', 'queen', 'man', 'woman', 'violent', 'word', 'term', 'describe', 'philosophy', 'chief', 'anarchists']\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# # for each_word in test_words:\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# #     similar_words = find_similar_words(each_word, model, vocab, idx_to_word)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# #     print(f\"Words similar to '{each_word}': {similar_words}\")\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     word_similarity_analysis(model\u001b[38;5;241m.\u001b[39mW2, vocab, idx_to_word)\n",
      "Cell \u001b[1;32mIn[17], line 8\u001b[0m, in \u001b[0;36mtrain_skip_gram_with_negative_sampling\u001b[1;34m(text, embedding_dim, window_size, num_negative_samples, batch_size, epochs, eta)\u001b[0m\n\u001b[0;32m      5\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m SkipGram(dataset\u001b[38;5;241m.\u001b[39mvocab_size, embedding_dim, num_negative_samples,eta)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, dataset\u001b[38;5;241m.\u001b[39mvocab_to_int, dataset\u001b[38;5;241m.\u001b[39mint_to_vocab\n",
      "Cell \u001b[1;32mIn[18], line 57\u001b[0m, in \u001b[0;36mSkipGram.gradient_descent\u001b[1;34m(self, train_dataloader, test_dataloader, n_epoch, early_stopping)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epoch):\n\u001b[0;32m     56\u001b[0m     train_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 57\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtarget_word\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_word\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_emb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_word\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_word\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[4], line 82\u001b[0m, in \u001b[0;36mText8Dataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     80\u001b[0m center_word, context_word \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_gram_pairs[idx]\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_weights_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_weights)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m---> 82\u001b[0m neg_samples \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_weights_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_negative_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreplacement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     86\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39mtensor(center_word, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong),\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mtensor(context_word, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong),\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39mtensor(neg_samples, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m     91\u001b[0m   )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    text = data[:4000000]\n",
    "    model, vocab, idx_to_word = train_skip_gram_with_negative_sampling(\n",
    "        text, embedding_dim=300, epochs=300, batch_size=128,eta=0.01\n",
    "    )\n",
    "\n",
    "    # test_words = ['king', 'queen', 'man', 'woman', 'violent', 'word', 'term', 'describe', 'philosophy', 'chief', 'anarchists']\n",
    "    # # for each_word in test_words:\n",
    "    # #     similar_words = find_similar_words(each_word, model, vocab, idx_to_word)\n",
    "    # #     print(f\"Words similar to '{each_word}': {similar_words}\")\n",
    "    \n",
    "    word_similarity_analysis(model.W2, vocab, idx_to_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "viz_words = 280\n",
    "tsne = TSNE()\n",
    "embed_tsne = tsne.fit_transform(model.W1[:viz_words, :])\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "for idx in range(viz_words):\n",
    "    plt.scatter(*embed_tsne[idx, :], color='steelblue')\n",
    "    plt.annotate(dataset.int_to_vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

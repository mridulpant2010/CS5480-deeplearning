{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip Gram Model\n",
    "\n",
    "P(word|context) -> CBOW\n",
    "skip gram -> P(Wcontext | Wword) it provides the word embeddings , given the words it predicts context for the word embeddings. Why do we require the embeddings? These embeddings captures the semantic relationships among the words because one hot encoding doesn't captures the semantic relationship \n",
    "    - because one-hot encoding is a sparse matrix\n",
    "    - the euclidean distance of root(2) doesn't gurantees the far away \n",
    "Skip gram model thus captures the:\n",
    "    - semantic relationship\n",
    "    - syntactic relationship (explain it later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "1. Word Representation using Word2Vec: Implement the skip-gram model with negative\n",
    "sampling loss function for word embedding generation. Your implementation should include: [20\n",
    " Marks]\n",
    " (b) Implement the skip-gram model from scratch with negative sampling loss. [4]\n",
    " (c) Derive and implement the gradients for backpropagation. [4]\n",
    " (d) Train your model on the text8 dataset with appropriate hyperparameters (specify your choices\n",
    " and justify them). [3]\n",
    " (e) Evaluate the quality of your embeddings through: [4]\n",
    " • Visualization using SVD to project the embeddings to 2D space.\n",
    " • Word similarity analysis for semantically related words (e.g., “king”- “man” + “woman”\n",
    " ≈ “queen”).\n",
    " (f) Discuss the impact of key hyperparameters (e.g., embedding dimension, context window size,\n",
    " number of negative samples) on the quality of the learned representations. [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "class SkipGram:\n",
    "    def __init__(self,input,hidden,output=1):\n",
    "        self.hidden=hidden\n",
    "        self.output=output\n",
    "        self.input=input\n",
    "        self.W1 = torch.tensor([[np.random.normal() for _ in range(self.output)] for _ in range(self.input)]) #how do i initialize a tensor of the size input X hidden\n",
    "        self.W2 = torch.tensor([[np.random.normal() for _ in range(self.output)] for _ in range(self.input)])\n",
    "        self.b1 = torch.tensor([np.random.normal() for _ in range(self.hidden)])\n",
    "        self.b2 = torch.tensor([np.random.normal() for _ in range(self.output)])\n",
    "        \n",
    "        self.eta =0.01\n",
    "        \n",
    "    def forward(self,input_tensor):\n",
    "        f = torch.tanh(torch.add(torch.matmul(self.W1.T,input_tensor),self.b1))\n",
    "        g = torch.add(torch.matmul(self.W2.T,f),self.b2)\n",
    "        output= F.softmax(g)\n",
    "        # how do we find the log of this \n",
    "        return output , f\n",
    "    \n",
    "    def cross_entropy(self,X,y):\n",
    "        m = y.shape[0]\n",
    "        p = F.log_softmax(X,dim=1)\n",
    "        loss = -p[torch.arange(m), y].mean()\n",
    "        return loss\n",
    "    \n",
    "    def delta_cross_entropy(self,X,y):\n",
    "        m = y.shape[0]\n",
    "        grad = F.softmax(X).clone()\n",
    "        grad[torch.arange(m), y] -= 1\n",
    "        grad = grad/m\n",
    "        return grad\n",
    "    \n",
    "    def gradient_descent(self,X,y):\n",
    "        ##self.W1-=\n",
    "        #cross_entropy_loss = self.cross_entropy()\n",
    "        n_epoch = 10\n",
    "        \n",
    "        for _ in range(n_epoch):\n",
    "            prediction,f = self.forward(X)\n",
    "            loss = self.cross_entropy(prediction,y)\n",
    "            \n",
    "            # calculate the gradients of loss wrt crossentropy function\n",
    "            d_g = prediction.clone()\n",
    "            d_g[torch.arange(y.shape[0]), y] -= 1\n",
    "            d_g /= y.shape[0]\n",
    "\n",
    "            # calculate the deltaW1, deltaW2 and deltab1, deltab2 \n",
    "            deltaW2 = torch.matmul(f.T,d_g)\n",
    "            db2 = torch.sum(d_g,dim=0)\n",
    "            \n",
    "            d_f = torch.matmul(d_g, self.W2.T) * (1 - f**2) \n",
    "            deltaW1 = torch.matmul(X.T,d_f)\n",
    "            db1 = torch.sum(d_f,dim=0)\n",
    "            self.W1 -= self.eta*deltaW1\n",
    "            self.W2 -= self.eta*deltaW2\n",
    "            self.b1 -= self.eta*db1\n",
    "            self.b2 -= self.eta*db2\n",
    "            print(f'Epoch {_+1}, Loss: {loss.item():.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

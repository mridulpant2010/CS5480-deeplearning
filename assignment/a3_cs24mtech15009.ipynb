{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip Gram Model\n",
    "\n",
    "P(word|context) -> CBOW\n",
    "skip gram -> P(Wcontext | Wword) it provides the word embeddings , given the words it predicts context for the word embeddings. Why do we require the embeddings? These embeddings captures the semantic relationships among the words because one hot encoding doesn't captures the semantic relationship \n",
    "    - because one-hot encoding is a sparse matrix\n",
    "    - the euclidean distance of root(2) doesn't gurantees the far away \n",
    "Skip gram model thus captures the:\n",
    "    - semantic relationship\n",
    "    - syntactic relationship (explain it later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "1. Word Representation using Word2Vec: Implement the skip-gram model with negative\n",
    "sampling loss function for word embedding generation. Your implementation should include: [20\n",
    " Marks]\n",
    " (b) Implement the skip-gram model from scratch with negative sampling loss. [4]\n",
    " (c) Derive and implement the gradients for backpropagation. [4]\n",
    " (d) Train your model on the text8 dataset with appropriate hyperparameters (specify your choices\n",
    " and justify them). [3]\n",
    " (e) Evaluate the quality of your embeddings through: [4]\n",
    " • Visualization using SVD to project the embeddings to 2D space.\n",
    " • Word similarity analysis for semantically related words (e.g., “king”- “man” + “woman”\n",
    " ≈ “queen”).\n",
    " (f) Discuss the impact of key hyperparameters (e.g., embedding dimension, context window size,\n",
    " number of negative samples) on the quality of the learned representations. [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "class SkipGram:\n",
    "    def __init__(self,input,hidden,output=1):\n",
    "        self.hidden=hidden\n",
    "        self.output=output\n",
    "        self.input=input\n",
    "        self.W1 = torch.tensor([[np.random.normal() for _ in range(self.output)] for _ in range(self.input)]) #how do i initialize a tensor of the size input X hidden\n",
    "        self.W2 = torch.tensor([[np.random.normal() for _ in range(self.output)] for _ in range(self.input)])\n",
    "        self.b1 = torch.tensor([np.random.normal() for _ in range(self.hidden)])\n",
    "        self.b2 = torch.tensor([np.random.normal() for _ in range(self.output)])\n",
    "        \n",
    "        self.eta =0.01\n",
    "        \n",
    "    def forward(self,input_tensor):\n",
    "        f = torch.tanh(torch.add(torch.matmul(self.W1.T,input_tensor),self.b1))\n",
    "        g = torch.add(torch.matmul(self.W2.T,f),self.b2)\n",
    "        output= F.softmax(g)\n",
    "        # how do we find the log of this \n",
    "        return output , f\n",
    "    \n",
    "    def cross_entropy(self,X,y):\n",
    "        m = y.shape[0]\n",
    "        p = F.log_softmax(X,dim=1)\n",
    "        loss = -p[torch.arange(m), y].mean()\n",
    "        return loss\n",
    "    \n",
    "    def delta_cross_entropy(self,X,y):\n",
    "        m = y.shape[0]\n",
    "        grad = F.softmax(X).clone()\n",
    "        grad[torch.arange(m), y] -= 1\n",
    "        grad = grad/m\n",
    "        return grad\n",
    "    \n",
    "    def gradient_descent(self,X,y):\n",
    "        n_epoch = 10\n",
    "        \n",
    "        for _ in range(n_epoch):\n",
    "            prediction,f = self.forward(X)\n",
    "            loss = self.cross_entropy(prediction,y)\n",
    "            \n",
    "            # calculate the gradients of loss wrt crossentropy function\n",
    "            d_g = prediction.clone()\n",
    "            d_g[torch.arange(y.shape[0]), y] -= 1\n",
    "            d_g /= y.shape[0]\n",
    "\n",
    "            # calculate the deltaW1, deltaW2 and deltab1, deltab2 \n",
    "            deltaW2 = torch.matmul(f.T,d_g)\n",
    "            db2 = torch.sum(d_g,dim=0)\n",
    "            \n",
    "            d_f = torch.matmul(d_g, self.W2.T) * (1 - f**2) \n",
    "            deltaW1 = torch.matmul(X.T,d_f)\n",
    "            db1 = torch.sum(d_f,dim=0)\n",
    "            self.W1 -= self.eta*deltaW1\n",
    "            self.W2 -= self.eta*deltaW2\n",
    "            self.b1 -= self.eta*db1\n",
    "            self.b2 -= self.eta*db2\n",
    "            print(f'Epoch {_+1}, Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class SkipGram:\n",
    "    def __init__(self, input, hidden, output=1, neg_sample_size=5):\n",
    "        self.input = input\n",
    "        self.hidden = hidden\n",
    "        self.output = output\n",
    "        self.neg_sample_size = neg_sample_size  # Number of negative samples\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.W1 = torch.tensor(np.random.randn(input, hidden), dtype=torch.float32)  # Input × Hidden\n",
    "        self.W2 = torch.tensor(np.random.randn(hidden, output), dtype=torch.float32)  # Hidden × Output\n",
    "        self.b1 = torch.tensor(np.random.randn(hidden), dtype=torch.float32)  # Hidden bias\n",
    "        self.b2 = torch.tensor(np.random.randn(output), dtype=torch.float32)  # Output bias\n",
    "        \n",
    "        self.eta = 0.01  # Learning rate\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # Forward pass (same as before)\n",
    "        f = torch.tanh(torch.matmul(input_tensor, self.W1) + self.b1)  # Hidden layer activation\n",
    "        return f  # Return only hidden activations\n",
    "    \n",
    "    def sample_negatives(self, vocab_size, exclude_indices, n_samples):\n",
    "        \"\"\"\n",
    "        Negative sampling: Draw 'n_samples' random indices, excluding 'exclude_indices'.\n",
    "        \"\"\"\n",
    "        neg_indices = []\n",
    "        while len(neg_indices) < n_samples:\n",
    "            sample = np.random.randint(0, vocab_size)\n",
    "            if sample not in exclude_indices:  # Avoid sampling positive targets\n",
    "                neg_indices.append(sample)\n",
    "        return torch.tensor(neg_indices, dtype=torch.long)\n",
    "\n",
    "    def calculate_loss(self, f, target_word_idx, neg_sample_indices, W2, b2):\n",
    "        \"\"\"\n",
    "        Compute the binary cross-entropy loss for positive and negative samples.\n",
    "        \"\"\"\n",
    "        # Positive sample score\n",
    "        pos_score = torch.sigmoid(torch.matmul(f, W2[:, target_word_idx]) + b2[target_word_idx])\n",
    "        pos_loss = -torch.log(pos_score + 1e-9)  # Add small epsilon for numerical stability\n",
    "\n",
    "        # Negative samples score\n",
    "        neg_scores = torch.sigmoid(torch.matmul(f, W2[:, neg_sample_indices]) + b2[neg_sample_indices])\n",
    "        neg_loss = -torch.sum(torch.log(1 - neg_scores + 1e-9))  # Sum over negative samples\n",
    "\n",
    "        return pos_loss + neg_loss\n",
    "\n",
    "    def gradient_descent(self, input_tensor, target_indices, vocab_size):\n",
    "        \"\"\"\n",
    "        Perform gradient descent using negative sampling.\n",
    "        \"\"\"\n",
    "        n_epoch = 10\n",
    "        for epoch in range(n_epoch):\n",
    "            total_loss = 0.0\n",
    "            for i, input_vec in enumerate(input_tensor):\n",
    "                # Forward pass\n",
    "                f = self.forward(input_vec.unsqueeze(0))\n",
    "\n",
    "                # Positive target index\n",
    "                target_idx = target_indices[i]\n",
    "\n",
    "                # Negative sampling\n",
    "                neg_sample_indices = self.sample_negatives(vocab_size, [target_idx], self.neg_sample_size)\n",
    "\n",
    "                # Loss computation\n",
    "                loss = self.calculate_loss(f, target_idx, neg_sample_indices, self.W2, self.b2)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Gradients for W2 and b2\n",
    "                pos_score = torch.sigmoid(torch.matmul(f, self.W2[:, target_idx]) + self.b2[target_idx])\n",
    "                self.W2[:, target_idx] -= self.eta * (pos_score - 1) * f.squeeze()\n",
    "                self.b2[target_idx] -= self.eta * (pos_score - 1)\n",
    "\n",
    "                neg_scores = torch.sigmoid(torch.matmul(f, self.W2[:, neg_sample_indices]) + self.b2[neg_sample_indices])\n",
    "                for j, neg_idx in enumerate(neg_sample_indices):\n",
    "                    self.W2[:, neg_idx] -= self.eta * neg_scores[j] * f.squeeze()\n",
    "                    self.b2[neg_idx] -= self.eta * neg_scores[j]\n",
    "\n",
    "                # Gradients for W1 and b1\n",
    "                grad_f = (pos_score - 1) * self.W2[:, target_idx] + torch.sum(neg_scores.unsqueeze(1) * self.W2[:, neg_sample_indices], dim=1)\n",
    "                self.W1 -= self.eta * torch.matmul(input_vec.unsqueeze(1), grad_f.unsqueeze(0))\n",
    "                self.b1 -= self.eta * grad_f.squeeze()\n",
    "\n",
    "            print(f'Epoch {epoch+1}, Loss: {total_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    }
   ],
   "source": [
    "def data_preprocessing(file_path):\n",
    "    with open(file_path,'r') as file:\n",
    "        data=file.read()\n",
    "        print(\"read data\")\n",
    "    return data\n",
    "\n",
    "file_path = \"C:/users/pantm/Downloads/text8/text8.txt\"\n",
    "data = data_preprocessing(file_path)\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from collections import Counter\n",
    "def preprocess_text(corpus, min_count=5):\n",
    "    \"\"\"Preprocess text corpus into vocabulary and word indices\"\"\"\n",
    "    words = corpus.lower().split()\n",
    "    word_counts = Counter(words)\n",
    "    # Filter words that appear less than min_count times\n",
    "    vocab = {word: i for i, (word, count) in enumerate(word_counts.items()) if count >= min_count}\n",
    "    # Add UNK token for unknown words\n",
    "    vocab['UNK'] = len(vocab)\n",
    "    # Create reverse mapping\n",
    "    idx_to_word = {i: word for word, i in vocab.items()}\n",
    "    # Convert corpus to word indices\n",
    "    word_indices = [vocab.get(word, vocab['UNK']) for word in words]\n",
    "    return vocab, idx_to_word, word_indices\n",
    "\n",
    "def generate_skip_grams(word_indices, window_size=5):\n",
    "    \"\"\"Generate skip-gram pairs with context window\"\"\"\n",
    "    skip_grams = []\n",
    "    for i in range(len(word_indices)):\n",
    "        target_word = word_indices[i]\n",
    "        # Get context words within window\n",
    "        context_range = range(max(0, i - window_size), min(len(word_indices), i + window_size + 1))\n",
    "        for j in context_range:\n",
    "            if i != j:  # Skip the target word itself\n",
    "                skip_grams.append((target_word, word_indices[j]))\n",
    "    return skip_grams\n",
    "\n",
    "# Example usage with a small corpus\n",
    "vocab, idx_to_word, word_indices = preprocess_text(data, min_count=1)\n",
    "skip_grams = generate_skip_grams(word_indices, window_size=2)\n",
    "\n",
    "# Convert to tensors for model input\n",
    "input_tensor = torch.tensor([pair[0] for pair in skip_grams], dtype=torch.long)\n",
    "target_indices = torch.tensor([pair[1] for pair in skip_grams], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 253855\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "bad allocation",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#skip_grams = generate_skip_grams(word_indices, window_size=5)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m SkipGram(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mvocab_size, hidden\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, neg_sample_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 56\u001b[0m, in \u001b[0;36mSkipGram.gradient_descent\u001b[1;34m(self, input_tensor, target_indices, vocab_size)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epoch):\n\u001b[0;32m     55\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, input_vec \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     57\u001b[0m         \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m     58\u001b[0m         f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(input_vec\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;66;03m# Positive target index\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_tensor.py:1164\u001b[0m, in \u001b[0;36mTensor.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state():\n\u001b[0;32m   1156\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterating over a tensor might cause the trace to be incorrect. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1158\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing a tensor of different shape won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt change the number of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1162\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   1163\u001b[0m     )\n\u001b[1;32m-> 1164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: bad allocation"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "#skip_grams = generate_skip_grams(word_indices, window_size=5)\n",
    "model = SkipGram(input=vocab_size, hidden=300, neg_sample_size=10)\n",
    "model.gradient_descent(input_tensor, target_indices,vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class SkipGram:\n",
    "    def __init__(self, input_dim, hidden_dim, vocab_size, neg_sample_size=5):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.neg_sample_size = neg_sample_size  # Number of negative samples\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.W1 = torch.tensor(np.random.randn(input_dim, hidden_dim), dtype=torch.float32)  # Input × Hidden\n",
    "        self.W2 = torch.tensor(np.random.randn(hidden_dim, vocab_size), dtype=torch.float32)  # Hidden × Output\n",
    "        self.b1 = torch.tensor(np.zeros(hidden_dim), dtype=torch.float32)  # Hidden bias\n",
    "        self.b2 = torch.tensor(np.zeros(vocab_size), dtype=torch.float32)  # Output bias\n",
    "        \n",
    "        self.eta = 0.01  # Learning rate\n",
    "\n",
    "    def forward(self, input_vec):\n",
    "        \"\"\"Forward pass: Compute hidden layer activations and output scores.\"\"\"\n",
    "        f = torch.tanh(torch.matmul(input_vec, self.W1) + self.b1)  # Hidden layer activation\n",
    "        output = torch.matmul(f, self.W2) + self.b2  # Output scores\n",
    "        return f, output\n",
    "\n",
    "    def calculate_loss(self, output, target_idx, neg_sample_indices):\n",
    "        \"\"\"Binary cross-entropy loss for positive and negative samples.\"\"\"\n",
    "        # Positive sample score\n",
    "        pos_score = torch.sigmoid(output[0, target_idx])\n",
    "        pos_loss = -torch.log(pos_score + 1e-9)  # Positive sample loss\n",
    "\n",
    "        # Negative samples score\n",
    "        neg_scores = torch.sigmoid(output[0, neg_sample_indices])\n",
    "        neg_loss = -torch.sum(torch.log(1 - neg_scores + 1e-9))  # Negative sample loss\n",
    "\n",
    "        return pos_loss + neg_loss\n",
    "\n",
    "    def gradient_descent(self, input_tensor, target_indices, vocab_size):\n",
    "        \"\"\"Perform training with manual gradient calculation.\"\"\"\n",
    "        n_epoch = 25\n",
    "        for epoch in range(n_epoch):\n",
    "            total_loss = 0.0\n",
    "            for i, input_idx in enumerate(input_tensor):\n",
    "                input_vec = torch.zeros(1, self.input_dim)  # One-hot encoding for input word\n",
    "                input_vec[0, input_idx] = 1.0\n",
    "\n",
    "                # Forward pass\n",
    "                f, output = self.forward(input_vec)\n",
    "\n",
    "                # Negative sampling\n",
    "                neg_sample_indices = self.sample_negatives(vocab_size, [target_indices[i]], self.neg_sample_size)\n",
    "\n",
    "                # Loss computation\n",
    "                loss = self.calculate_loss(output, target_indices[i], neg_sample_indices)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Gradients for W2 and b2\n",
    "                pos_score = torch.sigmoid(output[0, target_indices[i]])\n",
    "                grad_W2_pos = (pos_score - 1) * f.squeeze()  # Gradient for positive sample\n",
    "                grad_b2_pos = (pos_score - 1)\n",
    "\n",
    "                neg_scores = torch.sigmoid(output[0, neg_sample_indices])\n",
    "                grad_W2_neg = torch.sum(neg_scores.unsqueeze(1) * f, dim=0)  # Gradient for negative samples\n",
    "                grad_b2_neg = neg_scores\n",
    "\n",
    "                grad_W2 = torch.zeros_like(self.W2)\n",
    "                grad_W2[:, target_indices[i]] = grad_W2_pos\n",
    "                for j, neg_idx in enumerate(neg_sample_indices):\n",
    "                    grad_W2[:, neg_idx] += grad_W2_neg[j]\n",
    "\n",
    "                grad_b2 = torch.zeros_like(self.b2)\n",
    "                grad_b2[target_indices[i]] = grad_b2_pos\n",
    "                for j, neg_idx in enumerate(neg_sample_indices):\n",
    "                    grad_b2[neg_idx] += grad_b2_neg[j]\n",
    "\n",
    "                # Gradients for W1 and b1\n",
    "                grad_f = (pos_score - 1) * self.W2[:, target_indices[i]] + torch.sum(neg_scores.unsqueeze(0) * self.W2[:, neg_sample_indices], dim=1)\n",
    "                grad_W1 = torch.matmul(input_vec.T, (1 - f**2) * grad_f.unsqueeze(0))  # Gradient for W1\n",
    "                grad_b1 = (1 - f**2).squeeze() * grad_f  # Gradient for b1\n",
    "\n",
    "                # Update parameters\n",
    "                self.W1 -= self.eta * grad_W1\n",
    "                self.W2 -= self.eta * grad_W2\n",
    "                self.b1 -= self.eta * grad_b1\n",
    "                self.b2 -= self.eta * grad_b2\n",
    "\n",
    "            print(f'Epoch {epoch+1}, Loss: {total_loss:.4f}')\n",
    "\n",
    "    def sample_negatives(self, vocab_size, exclude_indices, n_samples):\n",
    "        \"\"\"Negative sampling: Draw 'n_samples' random indices, excluding 'exclude_indices'.\"\"\"\n",
    "        neg_indices = []\n",
    "        while len(neg_indices) < n_samples:\n",
    "            sample = np.random.randint(0, vocab_size)\n",
    "            if sample not in exclude_indices:  # Avoid sampling positive targets\n",
    "                neg_indices.append(sample)\n",
    "        return torch.tensor(neg_indices, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Additional utility functions for text processing remain the same\n",
    "def preprocess_text(corpus, min_count=5):\n",
    "    \"\"\"Preprocess text corpus into vocabulary and word indices.\"\"\"\n",
    "    words = corpus.lower().split()\n",
    "    word_counts = Counter(words)\n",
    "    vocab = {word: i for i, (word, count) in enumerate(word_counts.items()) if count >= min_count}\n",
    "    vocab['UNK'] = len(vocab)\n",
    "    idx_to_word = {i: word for word, i in vocab.items()}\n",
    "    word_indices = [vocab.get(word, vocab['UNK']) for word in words]\n",
    "    return vocab, idx_to_word, word_indices\n",
    "\n",
    "def generate_skip_grams(word_indices, window_size=5):\n",
    "    \"\"\"Generate skip-gram pairs with context window.\"\"\"\n",
    "    skip_grams = []\n",
    "    for i in range(len(word_indices)):\n",
    "        target_word = word_indices[i]\n",
    "        context_range = range(max(0, i - window_size), min(len(word_indices), i + window_size + 1))\n",
    "        for j in context_range:\n",
    "            if i != j:\n",
    "                skip_grams.append((target_word, word_indices[j]))\n",
    "    return skip_grams\n",
    "\n",
    "# Example usage with dummy data\n",
    "vocab, idx_to_word, word_indices = preprocess_text(data, min_count=5)\n",
    "skip_grams = generate_skip_grams(word_indices, window_size=2)\n",
    "\n",
    "# Convert to tensors for model input\n",
    "input_tensor = torch.tensor([pair[0] for pair in skip_grams], dtype=torch.long)\n",
    "target_indices = torch.tensor([pair[1] for pair in skip_grams], dtype=torch.long)\n",
    "\n",
    "# Initialize and train the SkipGram model\n",
    "# model = SkipGram(input_dim=len(vocab), hidden_dim=50, vocab_size=len(vocab))\n",
    "# model.gradient_descent(input_tensor, target_indices, vocab_size=len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- debugging of the code let us try to do this code with \n",
    "- how skip gram uses the \n",
    "  - target, context and the negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71291 68020822\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab),len(skip_grams)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 72\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target, context, negative_samples \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m     70\u001b[0m     target, context, negative_samples \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mlong(), context\u001b[38;5;241m.\u001b[39mlong(), negative_samples\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m---> 72\u001b[0m     positive_score, negative_score \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m     loss \u001b[38;5;241m=\u001b[39m negative_sampling_loss(positive_score, negative_score)\n\u001b[0;32m     75\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[9], line 34\u001b[0m, in \u001b[0;36mSkipGram.forward\u001b[1;34m(self, target, context, negative_samples)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, target, context, negative_samples):\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# Positive pair score\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     target_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_embeddings(target)  \u001b[38;5;66;03m# [batch_size, embedding_dim]\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m     context_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch_size, embedding_dim]\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     positive_score \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmul(target_embedding, context_embedding)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Dot product\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     positive_score \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(positive_score)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# SkipGramDataset class\n",
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, context_pairs, vocab_size, num_negative_samples=5):\n",
    "        self.context_pairs = context_pairs\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_negative_samples = num_negative_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.context_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target, context = self.context_pairs[idx]\n",
    "        negative_samples = np.random.choice(\n",
    "            self.vocab_size, self.num_negative_samples, replace=False\n",
    "        )\n",
    "        return target, context, torch.LongTensor(negative_samples)\n",
    "\n",
    "# Define the Skip-Gram Model\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.target_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, target, context, negative_samples):\n",
    "        # Positive pair score\n",
    "        target_embedding = self.target_embeddings(target)  # [batch_size, embedding_dim]\n",
    "        context_embedding = self.context_embeddings(context)  # [batch_size, embedding_dim]\n",
    "        positive_score = torch.mul(target_embedding, context_embedding).sum(dim=1)  # Dot product\n",
    "        positive_score = torch.sigmoid(positive_score)\n",
    "\n",
    "        # Negative pair score\n",
    "        negative_context_embeddings = self.context_embeddings(negative_samples)  # [batch_size, num_neg_samples, embedding_dim]\n",
    "        negative_score = torch.bmm(negative_context_embeddings, target_embedding.unsqueeze(2)).squeeze()  # Dot product\n",
    "        negative_score = torch.sigmoid(-negative_score).sum(dim=1)  # Sum of negative scores\n",
    "\n",
    "        return positive_score, negative_score\n",
    "\n",
    "# Loss Function\n",
    "def negative_sampling_loss(positive_score, negative_score):\n",
    "    loss = -torch.log(positive_score + 1e-8) - torch.log(negative_score + 1e-8)\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "\n",
    "#vocab, idx_to_word, word_indices = preprocess_text(data, min_count=5)\n",
    "#skip_grams = generate_skip_grams(word_indices, window_size=2)\n",
    "\n",
    "# Generate Toy Data\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 64\n",
    "#context_pairs = [(np.random.randint(vocab_size), np.random.randint(vocab_size)) for _ in range(1000)]\n",
    "dataset = SkipGramDataset(skip_grams, vocab_size)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Model, Optimizer\n",
    "model = SkipGram(vocab_size, embedding_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "#Training Loop\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for target, context, negative_samples in dataloader:\n",
    "        target, context, negative_samples = target.long(), context.long(), negative_samples.long()\n",
    "\n",
    "        positive_score, negative_score = model(target, context, negative_samples)\n",
    "        loss = negative_sampling_loss(positive_score, negative_score)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    }
   ],
   "source": [
    "def data_preprocessing(file_path):\n",
    "    with open(file_path,'r') as file:\n",
    "        data=file.read()\n",
    "        print(\"read data\")\n",
    "    return data\n",
    "\n",
    "file_path = \"C:/users/pantm/Downloads/text8/text8.txt\"\n",
    "data = data_preprocessing(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New code with updated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class SkipGram:\n",
    "    def __init__(self, vocab_size, embedding_dim, num_negative_samples=5):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_negative_samples = num_negative_samples\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        self.W1 = torch.randn(vocab_size, embedding_dim) * 0.1\n",
    "        self.W2 = torch.randn(vocab_size, embedding_dim) * 0.1\n",
    "        \n",
    "        self.eta = 0.01\n",
    "        \n",
    "    def forward(self, target_word, context_word, negative_samples):\n",
    "        # Get embeddings\n",
    "        target_emb = self.W1[target_word]  # [batch_size, embedding_dim]\n",
    "        context_emb = self.W2[context_word]  # [batch_size, embedding_dim]\n",
    "        neg_emb = self.W2[negative_samples]  # [batch_size, num_neg_samples, embedding_dim]\n",
    "        \n",
    "        # Positive score\n",
    "        pos_score = torch.sum(target_emb * context_emb, dim=1)\n",
    "        pos_loss = -F.logsigmoid(pos_score)\n",
    "        \n",
    "        # Negative score\n",
    "        neg_score = torch.bmm(neg_emb, target_emb.unsqueeze(2)).squeeze()\n",
    "        neg_loss = -torch.sum(F.logsigmoid(-neg_score), dim=1)\n",
    "        \n",
    "        return pos_loss + neg_loss, target_emb, context_emb, neg_emb\n",
    "\n",
    "    def gradient_descent(self, dataloader, n_epoch=10):\n",
    "        for epoch in range(n_epoch):\n",
    "            total_loss = 0\n",
    "            for target_word, context_word, negative_samples in dataloader:\n",
    "                loss, target_emb, context_emb, neg_emb = self.forward(target_word, context_word, negative_samples)\n",
    "                total_loss += loss.mean().item()\n",
    "\n",
    "                # Gradients for positive samples\n",
    "                d_pos = -torch.sigmoid(-torch.sum(target_emb * context_emb, dim=1)).unsqueeze(1) * context_emb\n",
    "                d_context = -torch.sigmoid(-torch.sum(target_emb * context_emb, dim=1)).unsqueeze(1) * target_emb\n",
    "\n",
    "                # Gradients for negative samples\n",
    "                d_neg = torch.sigmoid(torch.bmm(neg_emb, target_emb.unsqueeze(2)).squeeze()).unsqueeze(2) * neg_emb\n",
    "                d_target_neg = torch.sum(torch.sigmoid(torch.bmm(neg_emb, target_emb.unsqueeze(2)).squeeze()).unsqueeze(2) * neg_emb, dim=1)\n",
    "\n",
    "                # Update embeddings\n",
    "                self.W1[target_word] -= self.eta * (d_pos + d_target_neg)\n",
    "                self.W2[context_word] -= self.eta * d_context\n",
    "                self.W2[negative_samples] -= self.eta * d_neg\n",
    "\n",
    "            print(f'Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.4f}')\n",
    "\n",
    "    def get_word_embedding(self, word_idx):\n",
    "        return self.W1[word_idx].detach().numpy()\n",
    "\n",
    "class SkipGramDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text, window_size=2, num_negative_samples=5):\n",
    "        words = text.lower().split()\n",
    "        word_counts = Counter(words)\n",
    "        self.vocab = {word: idx for idx, (word, _) in enumerate(word_counts.most_common())}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.idx_to_word = {idx: word for word, idx in self.vocab.items()}\n",
    "        \n",
    "        word_indices = [self.vocab[word] for word in words if word in self.vocab]\n",
    "        \n",
    "        self.skip_grams = []\n",
    "        for i in range(len(word_indices)):\n",
    "            for j in range(max(0, i - window_size), min(len(word_indices), i + window_size + 1)):\n",
    "                if i != j:\n",
    "                    self.skip_grams.append((word_indices[i], word_indices[j]))\n",
    "        \n",
    "        self.num_negative_samples = num_negative_samples\n",
    "        \n",
    "        word_freqs = np.array([count for _, count in word_counts.most_common()])\n",
    "        word_freqs = word_freqs ** 0.75\n",
    "        self.sampling_weights = word_freqs / np.sum(word_freqs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.skip_grams)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target_word, context_word = self.skip_grams[idx]\n",
    "        negative_samples = np.random.choice(\n",
    "            self.vocab_size, \n",
    "            size=self.num_negative_samples, \n",
    "            p=self.sampling_weights, \n",
    "            replace=False\n",
    "        )\n",
    "        return torch.tensor(target_word), torch.tensor(context_word), torch.tensor(negative_samples)\n",
    "\n",
    "def train_skip_gram_with_negative_sampling(text, embedding_dim=100, window_size=2, \n",
    "                                          num_negative_samples=5, batch_size=32, epochs=5):\n",
    "    dataset = SkipGramDataset(text, window_size, num_negative_samples)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model = SkipGram(dataset.vocab_size, embedding_dim, num_negative_samples)\n",
    "    model.gradient_descent(dataloader, n_epoch=epochs)\n",
    "    \n",
    "    return model, dataset.vocab, dataset.idx_to_word\n",
    "\n",
    "def find_similar_words(word, model, vocab, idx_to_word, top_k=5):\n",
    "    if word not in vocab:\n",
    "        return []\n",
    "    \n",
    "    word_idx = vocab[word]\n",
    "    word_vector = model.get_word_embedding(word_idx)\n",
    "    \n",
    "    similarities = []\n",
    "    for idx in range(len(vocab)):\n",
    "        if idx != word_idx:\n",
    "            vector = model.get_word_embedding(idx)\n",
    "            similarity = np.dot(word_vector, vector) / (np.linalg.norm(word_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((idx_to_word[idx], similarity))\n",
    "    \n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_k]\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    text = \"The wide road shimmered in the hot sun. The algorithm was developed to produce software.\"\n",
    "    model, vocab, idx_to_word = train_skip_gram_with_negative_sampling(\n",
    "        text, embedding_dim=50, epochs=10\n",
    "    )\n",
    "    \n",
    "    similar_words = find_similar_words(\"the\", model, vocab, idx_to_word)\n",
    "    print(f\"Words similar to 'the': {similar_words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, tensor([68, 43,  9, 86, 56]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def generate_skip_grams(word_indices, window_size=5):\n",
    "    \"\"\"Generate skip-gram pairs with context window\"\"\"\n",
    "    skip_grams = []\n",
    "    for i in range(len(word_indices)):\n",
    "        target_word = word_indices[i]\n",
    "        # Get context words within window\n",
    "        context_range = range(max(0, i - window_size), min(len(word_indices), i + window_size + 1))\n",
    "        for j in context_range:\n",
    "            if i != j:  # Skip the target word itself\n",
    "                skip_grams.append((target_word, word_indices[j]))\n",
    "    return skip_grams\n",
    "\n",
    "# Example usage with a small corpus\n",
    "#data = \"This is a sample text for skip-gram model with negative sampling. The model learns word embeddings by predicting context words.\"\n",
    " # Using min_count=1 for small example\n",
    "skip_grams = generate_skip_grams(word_indices, window_size=2)\n",
    "\n",
    "def generate_negative_samples(target_word, context_word, vocab_size, num_samples):\n",
    "    \"\"\"Generate negative samples that are different from the target-context pair\"\"\"\n",
    "    negative_samples = []\n",
    "    while len(negative_samples) < num_samples:\n",
    "        # Generate a random context word\n",
    "        neg_context = np.random.randint(0, vocab_size)\n",
    "        # Make sure it's not the same as the positive context\n",
    "        if neg_context != context_word and (target_word, neg_context) not in negative_samples:\n",
    "            negative_samples.append((target_word, neg_context))\n",
    "    return negative_samples\n",
    "\n",
    "# Generate negative samples\n",
    "num_negative_samples = 5\n",
    "negTrainSet = []\n",
    "\n",
    "\n",
    "\n",
    "totalWords = sum([freq**(3/4) for freq in vocab.values()])\n",
    "wordProb = {word:(freq/totalWords)**(3/4) for word, freq in vocab.items()}\n",
    "\n",
    "# Example Usage\n",
    "probabilities = list(wordProb.values())#[0.1, 0.2, 0.4, 0.3]  # Example probability distribution\n",
    "prob_table, alias_table = create_alias_table(probabilities)\n",
    "\n",
    "\n",
    "# Sample from the alias table\n",
    "samples = [alias_sample(prob_table, alias_table) for _ in range(1000)]\n",
    "print(\"Generated samples:\", samples)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for target_word, context_word in skip_grams:\n",
    "    # Generate negative samples for this target word\n",
    "    neg_samples = generate_negative_samples(target_word, context_word, len(vocab), num_negative_samples)\n",
    "    negTrainSet.extend(neg_samples)\n",
    "\n",
    "print(f\"Number of positive examples: {len(skip_grams)}\")\n",
    "print(f\"Number of negative examples: {len(negTrainSet)}\")\n",
    "# # Convert to tensors for model input\n",
    "# input_tensor = torch.tensor([pair[0] for pair in skip_grams], dtype=torch.long)\n",
    "# target_indices = torch.tensor([pair[1] for pair in skip_grams], dtype=torch.long)\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Vision Transformer\n",
    "- first understand the concept and then try to write the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

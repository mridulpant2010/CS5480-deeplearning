{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250801\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "vocab = Counter(data.lower().split())\n",
    "word_to_index = {word: idx for idx, (word, _) in enumerate(vocab.most_common())}\n",
    "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
    "\n",
    "\n",
    "total_count = sum(vocab.values())\n",
    "word_freq = {word: count / total_count for word, count in vocab.items()}\n",
    "subsample_prob = {word: 1 - np.sqrt(1e-5 / word_freq[word]) for word in vocab}\n",
    "tokens = [word for word in vocab if np.random.rand() > subsample_prob[word]]\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68020822"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "len(skip_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, context_pairs, vocab_size, num_negative_samples=5):\n",
    "        self.context_pairs = context_pairs\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_negative_samples = num_negative_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.context_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target, context = self.context_pairs[idx]\n",
    "        negative_samples = np.random.choice(\n",
    "            self.vocab_size, self.num_negative_samples, replace=False\n",
    "        )\n",
    "        return target, context, torch.LongTensor(negative_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    }
   ],
   "source": [
    "def data_preprocessing(file_path):\n",
    "    with open(file_path,'r') as file:\n",
    "        data=file.read()\n",
    "        print(\"read data\")\n",
    "    return data\n",
    "\n",
    "file_path = \"C:/users/pantm/Downloads/text8/text8.txt\"\n",
    "data = data_preprocessing(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n",
      "253855\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import random\n",
    "import string\n",
    "import torch\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def replace_punctuation_with_tokens(corpus):\n",
    "    token_map = {\n",
    "        r'\\.': ' <PERIOD> ',\n",
    "        r',': ' <COMMA> ',\n",
    "        r'\\?': ' <QUESTION> ',\n",
    "        r'!': ' <EXCLAMATION> ',\n",
    "        r':': ' <COLON> ',\n",
    "        r';': ' <SEMICOLON> '\n",
    "    }\n",
    "    for pattern, token in token_map.items():\n",
    "        corpus = re.sub(pattern, token, corpus)\n",
    "    return corpus\n",
    "\n",
    "def preprocess_text(corpus, min_count=5):\n",
    "    \"\"\"Preprocess text corpus into vocabulary and word indices.\"\"\"\n",
    "    # Lowercase and replace punctuation with tokens\n",
    "    corpus = replace_punctuation_with_tokens(corpus.lower())\n",
    "    # Split words and remove remaining punctuation\n",
    "    words = [word.strip(string.punctuation) for word in corpus.split() if word.strip(string.punctuation)]\n",
    "    # Count word frequencies and filter based on min_count\n",
    "    word_counts = Counter(words)\n",
    "    sorted_vocab = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    vocab = {word: i for i, (word, count) in enumerate(sorted_vocab) if count >= min_count}\n",
    "    \n",
    "    # Add UNK token for unknown words\n",
    "    vocab['UNK'] = len(vocab)\n",
    "    \n",
    "    idx_to_word = {i: word for word, i in vocab.items()}\n",
    "    \n",
    "    # Convert words to indices, replacing out-of-vocabulary words with 'UNK'\n",
    "    word_indices = [vocab.get(word, vocab['UNK']) for word in words]\n",
    "    \n",
    "    return vocab, idx_to_word, word_indices\n",
    "\n",
    "# Define the data variable before using it\n",
    "\n",
    "vocab, idx_to_word, word_indices = preprocess_text(data, min_count=1)  # Using min_count=1 for small example\n",
    "print(len(vocab))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsampling\n",
    "- how does subsampling helps ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17005207\n",
      "4980457\n",
      "0.2928783519071541\n"
     ]
    }
   ],
   "source": [
    "def subsample_words(int_words, threshold = 1e-5):\n",
    "  word_counts = Counter(int_words)\n",
    "  total_n_words = len(int_words)\n",
    "\n",
    "  freq_ratios = {word: count/total_n_words for word, count in word_counts.items()}\n",
    "  p_drop = {word: 1 - np.sqrt(threshold/freq_ratios[word]) for word in word_counts}\n",
    "\n",
    "  return [word for word in int_words if random.random() < (1 - p_drop[word])]\n",
    "\n",
    "\n",
    "train_words = subsample_words(word_indices)\n",
    "print(len(word_indices))\n",
    "print(len(train_words))\n",
    "print(len(train_words)/len(word_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_skip_grams(word_indices, window_size=5):\n",
    "    \"\"\"Generate skip-gram pairs with context window\"\"\"\n",
    "    skip_grams = []\n",
    "    for i in range(len(word_indices)):\n",
    "        target_word = word_indices[i]\n",
    "        # Get context words within window\n",
    "        context_range = range(max(0, i - window_size), min(len(word_indices), i + window_size + 1))\n",
    "        for j in context_range:\n",
    "            if i != j:  # Skip the target word itself\n",
    "                skip_grams.append((target_word, word_indices[j]))\n",
    "    return skip_grams\n",
    "\n",
    "skip_grams = generate_skip_grams(word_indices, window_size=2)\n",
    "\n",
    "def generate_negative_samples(target_word, context_word, vocab_size, num_samples):\n",
    "    \"\"\"Generate negative samples that are different from the target-context pair\"\"\"\n",
    "    negative_samples = []\n",
    "    while len(negative_samples) < num_samples:\n",
    "        # Generate a random context word\n",
    "        neg_context = np.random.randint(0, vocab_size)\n",
    "        # Make sure it's not the same as the positive context\n",
    "        if neg_context != context_word and (target_word, neg_context) not in negative_samples:\n",
    "            negative_samples.append((target_word, neg_context))\n",
    "    return negative_samples\n",
    "\n",
    "# Generate negative samples\n",
    "num_negative_samples = 5\n",
    "negTrainSet = []\n",
    "\n",
    "for target_word, context_word in skip_grams:\n",
    "    # Generate negative samples for this target word\n",
    "    neg_samples = generate_negative_samples(target_word, context_word, len(vocab), num_negative_samples)\n",
    "    negTrainSet.extend(neg_samples)\n",
    "\n",
    "print(f\"Number of positive examples: {len(skip_grams)}\")\n",
    "print(f\"Number of negative examples: {len(negTrainSet)}\")\n",
    "# # Convert to tensors for model input\n",
    "# input_tensor = torch.tensor([pair[0] for pair in skip_grams], dtype=torch.long)\n",
    "# target_indices = torch.tensor([pair[1] for pair in skip_grams], dtype=torch.long)\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class SkipGram:\n",
    "    def __init__(self, vocab_size, embedding_dim, num_negative_samples=5):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_negative_samples = num_negative_samples\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        self.W1 = torch.randn(vocab_size, embedding_dim) * 0.1\n",
    "        self.W2 = torch.randn(vocab_size, embedding_dim) * 0.1\n",
    "        \n",
    "        self.eta = 0.01\n",
    "        \n",
    "    def forward(self, target_word, context_word, negative_samples):\n",
    "        # Get embeddings\n",
    "        target_emb = self.W1[target_word]  # [batch_size, embedding_dim]\n",
    "        context_emb = self.W2[context_word]  # [batch_size, embedding_dim]\n",
    "        neg_emb = self.W2[negative_samples]  # [batch_size, num_neg_samples, embedding_dim]\n",
    "        \n",
    "        # Positive score\n",
    "        pos_score = torch.sum(target_emb * context_emb, dim=1)\n",
    "        pos_loss = -F.logsigmoid(pos_score)\n",
    "        \n",
    "        # Negative score\n",
    "        neg_score = torch.bmm(neg_emb, target_emb.unsqueeze(2)).squeeze()\n",
    "        neg_loss = -torch.sum(F.logsigmoid(-neg_score), dim=1)\n",
    "        \n",
    "        return pos_loss + neg_loss, target_emb, context_emb, neg_emb\n",
    "\n",
    "    def gradient_descent(self, dataloader, n_epoch=10):\n",
    "        for epoch in range(n_epoch):\n",
    "            total_loss = 0\n",
    "            for target_word, context_word, negative_samples in dataloader:\n",
    "                loss, target_emb, context_emb, neg_emb = self.forward(target_word, context_word, negative_samples)\n",
    "                total_loss += loss.mean().item()\n",
    "\n",
    "                # Gradients for positive samples\n",
    "                d_pos = -torch.sigmoid(-torch.sum(target_emb * context_emb, dim=1)).unsqueeze(1) * context_emb\n",
    "                d_context = -torch.sigmoid(-torch.sum(target_emb * context_emb, dim=1)).unsqueeze(1) * target_emb\n",
    "\n",
    "                # Gradients for negative samples\n",
    "                d_neg = torch.sigmoid(torch.bmm(neg_emb, target_emb.unsqueeze(2)).squeeze()).unsqueeze(2) * neg_emb\n",
    "                d_target_neg = torch.sum(torch.sigmoid(torch.bmm(neg_emb, target_emb.unsqueeze(2)).squeeze()).unsqueeze(2) * neg_emb, dim=1)\n",
    "\n",
    "                # Update embeddings\n",
    "                self.W1[target_word] -= self.eta * (d_pos + d_target_neg)\n",
    "                self.W2[context_word] -= self.eta * d_context\n",
    "                self.W2[negative_samples] -= self.eta * d_neg\n",
    "\n",
    "            print(f'Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.4f}')\n",
    "\n",
    "    def get_word_embedding(self, word_idx):\n",
    "        return self.W1[word_idx].detach().numpy()\n",
    "\n",
    "class SkipGramDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text, window_size=2, num_negative_samples=5):\n",
    "        words = text.lower().split()\n",
    "        word_counts = Counter(words)\n",
    "        self.vocab = {word: idx for idx, (word, _) in enumerate(word_counts.most_common())}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.idx_to_word = {idx: word for word, idx in self.vocab.items()}\n",
    "        \n",
    "        word_indices = [self.vocab[word] for word in words if word in self.vocab]\n",
    "        \n",
    "        self.skip_grams = []\n",
    "        for i in range(len(word_indices)):\n",
    "            for j in range(max(0, i - window_size), min(len(word_indices), i + window_size + 1)):\n",
    "                if i != j:\n",
    "                    self.skip_grams.append((word_indices[i], word_indices[j]))\n",
    "        \n",
    "        self.num_negative_samples = num_negative_samples\n",
    "        \n",
    "        word_freqs = np.array([count for _, count in word_counts.most_common()])\n",
    "        word_freqs = word_freqs ** 0.75\n",
    "        self.sampling_weights = word_freqs / np.sum(word_freqs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.skip_grams)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target_word, context_word = self.skip_grams[idx]\n",
    "        negative_samples = np.random.choice(\n",
    "            self.vocab_size, \n",
    "            size=self.num_negative_samples, \n",
    "            p=self.sampling_weights, \n",
    "            replace=False\n",
    "        )\n",
    "        return torch.tensor(target_word), torch.tensor(context_word), torch.tensor(negative_samples)\n",
    "\n",
    "def train_skip_gram_with_negative_sampling(text, embedding_dim=100, window_size=2, \n",
    "                                          num_negative_samples=5, batch_size=32, epochs=5):\n",
    "    dataset = SkipGramDataset(text, window_size, num_negative_samples)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model = SkipGram(dataset.vocab_size, embedding_dim, num_negative_samples)\n",
    "    model.gradient_descent(dataloader, n_epoch=epochs)\n",
    "    \n",
    "    return model, dataset.vocab, dataset.idx_to_word\n",
    "\n",
    "def find_similar_words(word, model, vocab, idx_to_word, top_k=5):\n",
    "    if word not in vocab:\n",
    "        return []\n",
    "    \n",
    "    word_idx = vocab[word]\n",
    "    word_vector = model.get_word_embedding(word_idx)\n",
    "    \n",
    "    similarities = []\n",
    "    for idx in range(len(vocab)):\n",
    "        if idx != word_idx:\n",
    "            vector = model.get_word_embedding(idx)\n",
    "            similarity = np.dot(word_vector, vector) / (np.linalg.norm(word_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((idx_to_word[idx], similarity))\n",
    "    \n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_k]\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    #text = \"The wide road shimmered in the hot sun. The algorithm was developed to produce software.\"\n",
    "    model, vocab, idx_to_word = train_skip_gram_with_negative_sampling(\n",
    "        data, embedding_dim=50, epochs=10\n",
    "    )\n",
    "    \n",
    "    similar_words = find_similar_words(\"the\", model, vocab, idx_to_word)\n",
    "    print(f\"Words similar to 'the': {similar_words}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

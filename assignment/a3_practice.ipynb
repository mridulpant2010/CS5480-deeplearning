{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250801\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "vocab = Counter(data.lower().split())\n",
    "word_to_index = {word: idx for idx, (word, _) in enumerate(vocab.most_common())}\n",
    "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
    "\n",
    "\n",
    "total_count = sum(vocab.values())\n",
    "word_freq = {word: count / total_count for word, count in vocab.items()}\n",
    "subsample_prob = {word: 1 - np.sqrt(1e-5 / word_freq[word]) for word in vocab}\n",
    "tokens = [word for word in vocab if np.random.rand() > subsample_prob[word]]\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68020822"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "len(skip_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, context_pairs, vocab_size, num_negative_samples=5):\n",
    "        self.context_pairs = context_pairs\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_negative_samples = num_negative_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.context_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target, context = self.context_pairs[idx]\n",
    "        negative_samples = np.random.choice(\n",
    "            self.vocab_size, self.num_negative_samples, replace=False\n",
    "        )\n",
    "        return target, context, torch.LongTensor(negative_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    }
   ],
   "source": [
    "def data_preprocessing(file_path):\n",
    "    with open(file_path,'r') as file:\n",
    "        data=file.read()\n",
    "        print(\"read data\")\n",
    "    return data\n",
    "\n",
    "file_path = \"C:/users/pantm/Downloads/text8/text8.txt\"\n",
    "data = data_preprocessing(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n",
      "253855\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import random\n",
    "import string\n",
    "import torch\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def replace_punctuation_with_tokens(corpus):\n",
    "    token_map = {\n",
    "        r'\\.': ' <PERIOD> ',\n",
    "        r',': ' <COMMA> ',\n",
    "        r'\\?': ' <QUESTION> ',\n",
    "        r'!': ' <EXCLAMATION> ',\n",
    "        r':': ' <COLON> ',\n",
    "        r';': ' <SEMICOLON> '\n",
    "    }\n",
    "    for pattern, token in token_map.items():\n",
    "        corpus = re.sub(pattern, token, corpus)\n",
    "    return corpus\n",
    "\n",
    "def preprocess_text(corpus, min_count=5):\n",
    "    \"\"\"Preprocess text corpus into vocabulary and word indices.\"\"\"\n",
    "    # Lowercase and replace punctuation with tokens\n",
    "    corpus = replace_punctuation_with_tokens(corpus.lower())\n",
    "    # Split words and remove remaining punctuation\n",
    "    words = [word.strip(string.punctuation) for word in corpus.split() if word.strip(string.punctuation)]\n",
    "    # Count word frequencies and filter based on min_count\n",
    "    word_counts = Counter(words)\n",
    "    sorted_vocab = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    vocab = {word: i for i, (word, count) in enumerate(sorted_vocab) if count >= min_count}\n",
    "    \n",
    "    # Add UNK token for unknown words\n",
    "    vocab['UNK'] = len(vocab)\n",
    "    \n",
    "    idx_to_word = {i: word for word, i in vocab.items()}\n",
    "    \n",
    "    # Convert words to indices, replacing out-of-vocabulary words with 'UNK'\n",
    "    word_indices = [vocab.get(word, vocab['UNK']) for word in words]\n",
    "    \n",
    "    return vocab, idx_to_word, word_indices\n",
    "\n",
    "# Define the data variable before using it\n",
    "\n",
    "vocab, idx_to_word, word_indices = preprocess_text(data, min_count=1)  # Using min_count=1 for small example\n",
    "print(len(vocab))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsampling\n",
    "- how does subsampling helps ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17005207\n",
      "4980457\n",
      "0.2928783519071541\n"
     ]
    }
   ],
   "source": [
    "def subsample_words(int_words, threshold = 1e-5):\n",
    "  word_counts = Counter(int_words)\n",
    "  total_n_words = len(int_words)\n",
    "\n",
    "  freq_ratios = {word: count/total_n_words for word, count in word_counts.items()}\n",
    "  p_drop = {word: 1 - np.sqrt(threshold/freq_ratios[word]) for word in word_counts}\n",
    "\n",
    "  return [word for word in int_words if random.random() < (1 - p_drop[word])]\n",
    "\n",
    "\n",
    "train_words = subsample_words(word_indices)\n",
    "print(len(word_indices))\n",
    "print(len(train_words))\n",
    "print(len(train_words)/len(word_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit as sigmoid\n",
    "from collections import Counter\n",
    "\n",
    "class SkipGram:\n",
    "    def __init__(self, input_dim, hidden_dim, vocab_size, neg_sample_size=5, lr=0.01):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.neg_sample_size = neg_sample_size  # Number of negative samples\n",
    "        self.lr = lr  # Learning rate\n",
    "\n",
    "        # Initialize weights\n",
    "        self.W1 = np.random.randn(input_dim, hidden_dim) * 0.01\n",
    "        self.W2 = np.random.randn(hidden_dim, vocab_size) * 0.01\n",
    "        self.b1 = np.zeros(hidden_dim)\n",
    "        self.b2 = np.zeros(vocab_size)\n",
    "\n",
    "        # Adam parameters\n",
    "        self.m_W1, self.v_W1 = np.zeros_like(self.W1), np.zeros_like(self.W1)\n",
    "        self.m_W2, self.v_W2 = np.zeros_like(self.W2), np.zeros_like(self.W2)\n",
    "        self.m_b1, self.v_b1 = np.zeros_like(self.b1), np.zeros_like(self.b1)\n",
    "        self.m_b2, self.v_b2 = np.zeros_like(self.b2), np.zeros_like(self.b2)\n",
    "        self.beta1, self.beta2, self.eps = 0.9, 0.999, 1e-8\n",
    "        self.t = 0  # Time step for Adam\n",
    "    \n",
    "    def forward(self, input_vec):\n",
    "        \"\"\"Compute hidden activations and output scores.\"\"\"\n",
    "        f = np.tanh(np.dot(input_vec, self.W1) + self.b1)\n",
    "        output = np.dot(f, self.W2) + self.b2\n",
    "        return f, output\n",
    "    \n",
    "    def sample_negatives(self, vocab_size, exclude_indices, n_samples):\n",
    "        \"\"\"Sample negative words while avoiding positives.\"\"\"\n",
    "        neg_samples = []\n",
    "        while len(neg_samples) < n_samples:\n",
    "            sample = np.random.randint(0, vocab_size)\n",
    "            if sample not in exclude_indices:\n",
    "                neg_samples.append(sample)\n",
    "        return np.array(neg_samples)\n",
    "    \n",
    "    def update_adam(self, grad, m, v, param):\n",
    "        \"\"\"Apply Adam optimization.\"\"\"\n",
    "        self.t += 1\n",
    "        m = self.beta1 * m + (1 - self.beta1) * grad\n",
    "        v = self.beta2 * v + (1 - self.beta2) * (grad ** 2)\n",
    "        m_hat = m / (1 - self.beta1 ** self.t)\n",
    "        v_hat = v / (1 - self.beta2 ** self.t)\n",
    "        param -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n",
    "        return param, m, v\n",
    "\n",
    "    def train(self, input_tensor, target_indices, vocab_size, epochs=25):\n",
    "        \"\"\"Train the model using Adam optimizer.\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0.0\n",
    "            for i, input_idx in enumerate(input_tensor):\n",
    "                input_vec = np.zeros(self.input_dim)\n",
    "                input_vec[input_idx] = 1.0  # One-hot encoding\n",
    "\n",
    "                # Forward pass\n",
    "                f, output = self.forward(input_vec)\n",
    "\n",
    "                # Negative sampling\n",
    "                neg_sample_indices = self.sample_negatives(vocab_size, [target_indices[i]], self.neg_sample_size)\n",
    "\n",
    "                # Compute loss\n",
    "                pos_score = sigmoid(output[target_indices[i]])\n",
    "                neg_scores = sigmoid(output[neg_sample_indices])\n",
    "                loss = -np.log(pos_score + 1e-9) - np.sum(np.log(1 - neg_scores + 1e-9))\n",
    "                total_loss += loss\n",
    "\n",
    "                # Compute gradients\n",
    "                grad_W2 = np.zeros_like(self.W2)\n",
    "                grad_b2 = np.zeros_like(self.b2)\n",
    "                \n",
    "                grad_W2[:, target_indices[i]] = (pos_score - 1) * f\n",
    "                grad_b2[target_indices[i]] = (pos_score - 1)\n",
    "                \n",
    "                for j, neg_idx in enumerate(neg_sample_indices):\n",
    "                    grad_W2[:, neg_idx] += neg_scores[j] * f\n",
    "                    grad_b2[neg_idx] += neg_scores[j]\n",
    "                \n",
    "                grad_f = (pos_score - 1) * self.W2[:, target_indices[i]] + np.sum(neg_scores * self.W2[:, neg_sample_indices], axis=1)\n",
    "                grad_W1 = np.outer(input_vec, (1 - f ** 2) * grad_f)\n",
    "                grad_b1 = (1 - f ** 2) * grad_f\n",
    "                \n",
    "                # Apply Adam updates\n",
    "                self.W1, self.m_W1, self.v_W1 = self.update_adam(grad_W1, self.m_W1, self.v_W1, self.W1)\n",
    "                self.W2, self.m_W2, self.v_W2 = self.update_adam(grad_W2, self.m_W2, self.v_W2, self.W2)\n",
    "                self.b1, self.m_b1, self.v_b1 = self.update_adam(grad_b1, self.m_b1, self.v_b1, self.b1)\n",
    "                self.b2, self.m_b2, self.v_b2 = self.update_adam(grad_b2, self.m_b2, self.v_b2, self.b2)\n",
    "            \n",
    "            print(f'Epoch {epoch + 1}, Loss: {total_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"The quick brown fox jumps over the lazy dog\"\n",
    "vocab, idx_to_word, word_indices = preprocess_text(data, min_count=5)\n",
    "skip_grams = generate_skip_grams(word_indices, window_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the SkipGram model\n",
    "model = SkipGram(input_dim=1000, hidden_dim=50, vocab_size=len(vocab))\n",
    "model.train(input_tensor, target_indices, vocab_size=len(vocab))\n",
    "\n",
    "\n",
    "def preprocess_text(corpus, min_count=5):\n",
    "    \"\"\"Preprocess text corpus into vocabulary and word indices.\"\"\"\n",
    "    words = corpus.lower().split()\n",
    "    word_counts = Counter(words)\n",
    "    vocab = {word: i for i, (word, count) in enumerate(word_counts.items()) if count >= min_count}\n",
    "    vocab['UNK'] = len(vocab)\n",
    "    idx_to_word = {i: word for word, i in vocab.items()}\n",
    "    word_indices = [vocab.get(word, vocab['UNK']) for word in words]\n",
    "    return vocab, idx_to_word, word_indices\n",
    "\n",
    "def generate_skip_grams(word_indices, window_size=5):\n",
    "    \"\"\"Generate skip-gram pairs with context window.\"\"\"\n",
    "    skip_grams = []\n",
    "    for i in range(len(word_indices)):\n",
    "        target_word = word_indices[i]\n",
    "        context_range = range(max(0, i - window_size), min(len(word_indices), i + window_size + 1))\n",
    "        for j in context_range:\n",
    "            if i != j:\n",
    "                skip_grams.append((target_word, word_indices[j]))\n",
    "    return skip_grams\n",
    "\n",
    "\n",
    "data = \"hellow wrold ahowa reah and what is happenign and what you are trying to do m implementing skip gram is pain in the ass.\"\n",
    "# Generate random data\n",
    "vocab_size = len(data)  # Vocabulary size\n",
    "hidden_dim = 50  # Hidden layer size\n",
    "skip_gram_pairs = 500  # Number of skip-gram pairs\n",
    "\n",
    "# Generate random word indices for input and targets\n",
    "input_tensor = np.random.randint(0, vocab_size, size=skip_gram_pairs)\n",
    "target_indices = np.random.randint(0, vocab_size, size=skip_gram_pairs)\n",
    "\n",
    "# Initialize and train the SkipGram model\n",
    "model = SkipGram(input_dim=vocab_size, hidden_dim=hidden_dim, vocab_size=vocab_size)\n",
    "model.train(input_tensor, target_indices, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_skip_grams(word_indices, window_size=5):\n",
    "    \"\"\"Generate skip-gram pairs with context window\"\"\"\n",
    "    skip_grams = []\n",
    "    for i in range(len(word_indices)):\n",
    "        target_word = word_indices[i]\n",
    "        # Get context words within window\n",
    "        context_range = range(max(0, i - window_size), min(len(word_indices), i + window_size + 1))\n",
    "        for j in context_range:\n",
    "            if i != j:  # Skip the target word itself\n",
    "                skip_grams.append((target_word, word_indices[j]))\n",
    "    return skip_grams\n",
    "\n",
    "skip_grams = generate_skip_grams(word_indices, window_size=2)\n",
    "\n",
    "def generate_negative_samples(target_word, context_word, vocab_size, num_samples):\n",
    "    \"\"\"Generate negative samples that are different from the target-context pair\"\"\"\n",
    "    negative_samples = []\n",
    "    while len(negative_samples) < num_samples:\n",
    "        # Generate a random context word\n",
    "        neg_context = np.random.randint(0, vocab_size)\n",
    "        # Make sure it's not the same as the positive context\n",
    "        if neg_context != context_word and (target_word, neg_context) not in negative_samples:\n",
    "            negative_samples.append((target_word, neg_context))\n",
    "    return negative_samples\n",
    "\n",
    "# Generate negative samples\n",
    "num_negative_samples = 5\n",
    "negTrainSet = []\n",
    "\n",
    "for target_word, context_word in skip_grams:\n",
    "    # Generate negative samples for this target word\n",
    "    neg_samples = generate_negative_samples(target_word, context_word, len(vocab), num_negative_samples)\n",
    "    negTrainSet.extend(neg_samples)\n",
    "\n",
    "print(f\"Number of positive examples: {len(skip_grams)}\")\n",
    "print(f\"Number of negative examples: {len(negTrainSet)}\")\n",
    "# # Convert to tensors for model input\n",
    "# input_tensor = torch.tensor([pair[0] for pair in skip_grams], dtype=torch.long)\n",
    "# target_indices = torch.tensor([pair[1] for pair in skip_grams], dtype=torch.long)\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class SkipGram:\n",
    "    def __init__(self, vocab_size, embedding_dim, num_negative_samples=5):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_negative_samples = num_negative_samples\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        self.W1 = torch.randn(vocab_size, embedding_dim) * 0.1\n",
    "        self.W2 = torch.randn(vocab_size, embedding_dim) * 0.1\n",
    "        \n",
    "        self.eta = 0.01\n",
    "        \n",
    "    def forward(self, target_word, context_word, negative_samples):\n",
    "        # Get embeddings\n",
    "        target_emb = self.W1[target_word]  # [batch_size, embedding_dim]\n",
    "        context_emb = self.W2[context_word]  # [batch_size, embedding_dim]\n",
    "        neg_emb = self.W2[negative_samples]  # [batch_size, num_neg_samples, embedding_dim]\n",
    "        \n",
    "        # Positive score\n",
    "        pos_score = torch.sum(target_emb * context_emb, dim=1)\n",
    "        pos_loss = -F.logsigmoid(pos_score)\n",
    "        \n",
    "        # Negative score\n",
    "        neg_score = torch.bmm(neg_emb, target_emb.unsqueeze(2)).squeeze()\n",
    "        neg_loss = -torch.sum(F.logsigmoid(-neg_score), dim=1)\n",
    "        \n",
    "        return pos_loss + neg_loss, target_emb, context_emb, neg_emb\n",
    "\n",
    "    def gradient_descent(self, dataloader, n_epoch=10):\n",
    "        for epoch in range(n_epoch):\n",
    "            total_loss = 0\n",
    "            for target_word, context_word, negative_samples in dataloader:\n",
    "                loss, target_emb, context_emb, neg_emb = self.forward(target_word, context_word, negative_samples)\n",
    "                total_loss += loss.mean().item()\n",
    "\n",
    "                # Gradients for positive samples\n",
    "                d_pos = -torch.sigmoid(-torch.sum(target_emb * context_emb, dim=1)).unsqueeze(1) * context_emb\n",
    "                d_context = -torch.sigmoid(-torch.sum(target_emb * context_emb, dim=1)).unsqueeze(1) * target_emb\n",
    "\n",
    "                # Gradients for negative samples\n",
    "                d_neg = torch.sigmoid(torch.bmm(neg_emb, target_emb.unsqueeze(2)).squeeze()).unsqueeze(2) * neg_emb\n",
    "                d_target_neg = torch.sum(torch.sigmoid(torch.bmm(neg_emb, target_emb.unsqueeze(2)).squeeze()).unsqueeze(2) * neg_emb, dim=1)\n",
    "\n",
    "                # Update embeddings\n",
    "                self.W1[target_word] -= self.eta * (d_pos + d_target_neg)\n",
    "                self.W2[context_word] -= self.eta * d_context\n",
    "                self.W2[negative_samples] -= self.eta * d_neg\n",
    "\n",
    "            print(f'Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.4f}')\n",
    "\n",
    "    def get_word_embedding(self, word_idx):\n",
    "        return self.W1[word_idx].detach().numpy()\n",
    "\n",
    "class SkipGramDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text, window_size=2, num_negative_samples=5):\n",
    "        words = text.lower().split()\n",
    "        word_counts = Counter(words)\n",
    "        self.vocab = {word: idx for idx, (word, _) in enumerate(word_counts.most_common())}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.idx_to_word = {idx: word for word, idx in self.vocab.items()}\n",
    "        \n",
    "        word_indices = [self.vocab[word] for word in words if word in self.vocab]\n",
    "        \n",
    "        self.skip_grams = []\n",
    "        for i in range(len(word_indices)):\n",
    "            for j in range(max(0, i - window_size), min(len(word_indices), i + window_size + 1)):\n",
    "                if i != j:\n",
    "                    self.skip_grams.append((word_indices[i], word_indices[j]))\n",
    "        \n",
    "        self.num_negative_samples = num_negative_samples\n",
    "        \n",
    "        word_freqs = np.array([count for _, count in word_counts.most_common()])\n",
    "        word_freqs = word_freqs ** 0.75\n",
    "        self.sampling_weights = word_freqs / np.sum(word_freqs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.skip_grams)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target_word, context_word = self.skip_grams[idx]\n",
    "        negative_samples = np.random.choice(\n",
    "            self.vocab_size, \n",
    "            size=self.num_negative_samples, \n",
    "            p=self.sampling_weights, \n",
    "            replace=False\n",
    "        )\n",
    "        return torch.tensor(target_word), torch.tensor(context_word), torch.tensor(negative_samples)\n",
    "\n",
    "def train_skip_gram_with_negative_sampling(text, embedding_dim=100, window_size=2, \n",
    "                                          num_negative_samples=5, batch_size=32, epochs=5):\n",
    "    dataset = SkipGramDataset(text, window_size, num_negative_samples)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model = SkipGram(dataset.vocab_size, embedding_dim, num_negative_samples)\n",
    "    model.gradient_descent(dataloader, n_epoch=epochs)\n",
    "    \n",
    "    return model, dataset.vocab, dataset.idx_to_word\n",
    "\n",
    "def find_similar_words(word, model, vocab, idx_to_word, top_k=5):\n",
    "    if word not in vocab:\n",
    "        return []\n",
    "    \n",
    "    word_idx = vocab[word]\n",
    "    word_vector = model.get_word_embedding(word_idx)\n",
    "    \n",
    "    similarities = []\n",
    "    for idx in range(len(vocab)):\n",
    "        if idx != word_idx:\n",
    "            vector = model.get_word_embedding(idx)\n",
    "            similarity = np.dot(word_vector, vector) / (np.linalg.norm(word_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((idx_to_word[idx], similarity))\n",
    "    \n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_k]\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    #text = \"The wide road shimmered in the hot sun. The algorithm was developed to produce software.\"\n",
    "    model, vocab, idx_to_word = train_skip_gram_with_negative_sampling(\n",
    "        data, embedding_dim=50, epochs=10\n",
    "    )\n",
    "    \n",
    "    similar_words = find_similar_words(\"the\", model, vocab, idx_to_word)\n",
    "    print(f\"Words similar to 'the': {similar_words}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
